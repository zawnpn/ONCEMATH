<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Welcome to zhangwp's blog."><link href=https://www.zhangwp.com/notes/reinforcement-learning/notes/RLAI_3/ rel=canonical><meta name=author content=zawnpn><link rel="shortcut icon" href=../../../../assets/images/favicon.svg><meta name=generator content="mkdocs-1.1.2, mkdocs-material-5.4.0"><title>Chapter 3 - ZHANGWP</title><link rel=stylesheet href=../../../../assets/stylesheets/main.fe0cca5b.min.css><link rel=stylesheet href=../../../../assets/stylesheets/palette.a46bcfb3.min.css><meta name=theme-color content=#546e7a></head> <body dir=ltr data-md-color-scheme data-md-color-primary=blue-grey data-md-color-accent> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#- class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid" aria-label=Header> <a href=https://www.zhangwp.com title=ZHANGWP class="md-header-nav__button md-logo" aria-label=ZHANGWP> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><path d="M257.981 272.971L63.638 467.314c-9.373 9.373-24.569 9.373-33.941 0L7.029 444.647c-9.357-9.357-9.375-24.522-.04-33.901L161.011 256 6.99 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L257.981 239.03c9.373 9.372 9.373 24.568 0 33.941zM640 456v-32c0-13.255-10.745-24-24-24H312c-13.255 0-24 10.745-24 24v32c0 13.255 10.745 24 24 24h304c13.255 0 24-10.745 24-24z"/></svg> </a> <label class="md-header-nav__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header-nav__title data-md-component=header-title> <div class=md-header-nav__ellipsis> <span class="md-header-nav__topic md-ellipsis"> ZHANGWP </span> <span class="md-header-nav__topic md-ellipsis"> Chapter 3 </span> </div> </div> <label class="md-header-nav__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query data-md-state=active> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <button type=reset class="md-search__icon md-icon" aria-label=Clear data-md-component=search-reset tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header-nav__source> <a href=https://github.com/zawnpn/ZHANGWP/ title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class="md-tabs md-tabs--active" aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../ class="md-tabs__link md-tabs__link--active"> Notes </a> </li> <li class=md-tabs__item> <a href=../../../../tips/ class=md-tabs__link> Tips </a> </li> <li class=md-tabs__item> <a href=../../../../share/ class=md-tabs__link> Share </a> </li> <li class=md-tabs__item> <a href=../../../../statements/ class=md-tabs__link> Statements </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=https://www.zhangwp.com title=ZHANGWP class="md-nav__button md-logo" aria-label=ZHANGWP> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><path d="M257.981 272.971L63.638 467.314c-9.373 9.373-24.569 9.373-33.941 0L7.029 444.647c-9.357-9.357-9.375-24.522-.04-33.901L161.011 256 6.99 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L257.981 239.03c9.373 9.372 9.373 24.568 0 33.941zM640 456v-32c0-13.255-10.745-24-24-24H312c-13.255 0-24 10.745-24 24v32c0 13.255 10.745 24 24 24h304c13.255 0 24-10.745 24-24z"/></svg> </a> ZHANGWP </label> <div class=md-nav__source> <a href=https://github.com/zawnpn/ZHANGWP/ title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1 type=checkbox id=nav-1> <label class=md-nav__link for=nav-1> Home <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Home data-md-level=1> <label class=md-nav__title for=nav-1> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../.. title=Home class=md-nav__link> Home </a> </li> <li class=md-nav__item> <a href=../../../../links/ title=Links class=md-nav__link> Links </a> </li> <li class=md-nav__item> <a href=../../../../donates/ title=Donate class=md-nav__link> Donate </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2 type=checkbox id=nav-2 checked> <label class=md-nav__link for=nav-2> Notes <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Notes data-md-level=1> <label class=md-nav__title for=nav-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Notes </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../ title=Index class=md-nav__link> Index </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2-2 type=checkbox id=nav-2-2 checked> <label class=md-nav__link for=nav-2-2> Reinforcement <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Reinforcement data-md-level=2> <label class=md-nav__title for=nav-2-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Reinforcement </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2-2-1 type=checkbox id=nav-2-2-1 checked> <label class=md-nav__link for=nav-2-2-1> Reinforcement Learning An Introduction <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Reinforcement Learning An Introduction" data-md-level=3> <label class=md-nav__title for=nav-2-2-1> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Reinforcement Learning An Introduction </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../RLAI_2/ title="Chapter 2" class=md-nav__link> Chapter 2 </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Chapter 3 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 9h14V7H3v2m0 4h14v-2H3v2m0 4h14v-2H3v2m16 0h2v-2h-2v2m0-10v2h2V7h-2m0 6h2v-2h-2v2z"/></svg> </span> </label> <a href=./ title="Chapter 3" class="md-nav__link md-nav__link--active"> Chapter 3 </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#31-the-agentenvironment-interface class=md-nav__link> 3.1 The Agent–Environment Interface </a> <nav class=md-nav aria-label="3.1 The Agent–Environment Interface"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example-recycling-robot class=md-nav__link> Example: Recycling Robot </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#32-goals-and-rewards class=md-nav__link> 3.2 Goals and Rewards </a> </li> <li class=md-nav__item> <a href=#33-returns class=md-nav__link> 3.3 Returns </a> <nav class=md-nav aria-label="3.3 Returns"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#episodic-tasks class=md-nav__link> Episodic Tasks </a> </li> <li class=md-nav__item> <a href=#continuing-tasks class=md-nav__link> Continuing Tasks </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#34-unified-notation-for-episodic-and-continuing-tasks class=md-nav__link> 3.4 Unified Notation for Episodic and Continuing Tasks </a> </li> <li class=md-nav__item> <a href=#35-the-markov-property class=md-nav__link> 3.5 The Markov Property </a> <nav class=md-nav aria-label="3.5 The Markov Property"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#markov-property class=md-nav__link> Markov Property </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#36-markov-decision-processes class=md-nav__link> 3.6 Markov Decision Processes </a> <nav class=md-nav aria-label="3.6 Markov Decision Processes"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example-recycling-robot-mdp class=md-nav__link> Example: Recycling Robot MDP </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#37-value-functions class=md-nav__link> 3.7 Value Functions </a> <nav class=md-nav aria-label="3.7 Value Functions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#monte-carlo-methods class=md-nav__link> Monte Carlo Methods </a> </li> <li class=md-nav__item> <a href=#bellman-equation class=md-nav__link> Bellman Equation </a> </li> <li class=md-nav__item> <a href=#example-gridworld class=md-nav__link> Example: Gridworld </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#38-optimal-value-functions class=md-nav__link> 3.8 Optimal Value Functions </a> <nav class=md-nav aria-label="3.8 Optimal Value Functions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#bellman-optimality-equation class=md-nav__link> Bellman Optimality Equation </a> </li> <li class=md-nav__item> <a href=#example-solving-the-gridworld class=md-nav__link> Example: Solving the Gridworld </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#39-optimality-and-approximation class=md-nav__link> 3.9 Optimality and Approximation </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../RLAI_4/ title="Chapter 4" class=md-nav__link> Chapter 4 </a> </li> <li class=md-nav__item> <a href=../RLAI_5/ title="Chapter 5" class=md-nav__link> Chapter 5 </a> </li> <li class=md-nav__item> <a href=../RLAI_6/ title="Chapter 6" class=md-nav__link> Chapter 6 </a> </li> <li class=md-nav__item> <a href=../RLAI_7/ title="Chapter 7" class=md-nav__link> Chapter 7 </a> </li> <li class=md-nav__item> <a href=../RLAI_8/ title="Chapter 8" class=md-nav__link> Chapter 8 </a> </li> <li class=md-nav__item> <a href=../RLAI_9/ title="Chapter 9" class=md-nav__link> Chapter 9 </a> </li> <li class=md-nav__item> <a href=../RLAI_10/ title="Chapter 10" class=md-nav__link> Chapter 10 </a> </li> <li class=md-nav__item> <a href=../RLAI_11/ title="Chapter 11" class=md-nav__link> Chapter 11 </a> </li> <li class=md-nav__item> <a href=../RLAI_12/ title="Chapter 12" class=md-nav__link> Chapter 12 </a> </li> <li class=md-nav__item> <a href=../RLAI_13/ title="Chapter 13" class=md-nav__link> Chapter 13 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2-2-2 type=checkbox id=nav-2-2-2> <label class=md-nav__link for=nav-2-2-2> Some Introduction <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Some Introduction" data-md-level=3> <label class=md-nav__title for=nav-2-2-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Some Introduction </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../MCTS_introduction/ title=MCTS class=md-nav__link> MCTS </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> Tips <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Tips data-md-level=1> <label class=md-nav__title for=nav-3> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Tips </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../tips/ title=Tips class=md-nav__link> Tips </a> </li> <li class=md-nav__item> <a href=../../../../tips/to-do/ title="To Do" class=md-nav__link> To Do </a> </li> <li class=md-nav__item> <a href=../../../../tips/python/ title=Python class=md-nav__link> Python </a> </li> <li class=md-nav__item> <a href=../../../../tips/data-processing/ title="Data Processing" class=md-nav__link> Data Processing </a> </li> <li class=md-nav__item> <a href=../../../../tips/git/ title=Git class=md-nav__link> Git </a> </li> <li class=md-nav__item> <a href=../../../../tips/linux/ title=Linux class=md-nav__link> Linux </a> </li> <li class=md-nav__item> <a href=../../../../tips/win/ title=Windows class=md-nav__link> Windows </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4 type=checkbox id=nav-4> <label class=md-nav__link for=nav-4> Share <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Share data-md-level=1> <label class=md-nav__title for=nav-4> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Share </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/ title=Index class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../../share/blog-history/ title=博客历史 class=md-nav__link> 博客历史 </a> </li> <li class=md-nav__item> <a href=../../../../share/game-log/ title=Game-Log class=md-nav__link> Game-Log </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-4 type=checkbox id=nav-4-4> <label class=md-nav__link for=nav-4-4> NKU-Toolkit <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=NKU-Toolkit data-md-level=2> <label class=md-nav__title for=nav-4-4> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> NKU-Toolkit </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/nku-eamis/ title=NKU-EAMIS工具 class=md-nav__link> NKU-EAMIS工具 </a> </li> <li class=md-nav__item> <a href=../../../../share/nku-sms-rss/ title=NKU-SMS-RSS class=md-nav__link> NKU-SMS-RSS </a> </li> <li class=md-nav__item> <a href=../../../../share/eamis-miniapp/ title=NKU-EAMIS_MiniApp(南开大学教务助手小程序) class=md-nav__link> NKU-EAMIS_MiniApp(南开大学教务助手小程序) </a> </li> <li class=md-nav__item> <a href=../../../../share/eamis-workflow/ title="NKU-EAMIS for iOS(Workflow)" class=md-nav__link> NKU-EAMIS for iOS(Workflow) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-5 type=checkbox id=nav-4-5> <label class=md-nav__link for=nav-4-5> Steam-Toolkit <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Steam-Toolkit data-md-level=2> <label class=md-nav__title for=nav-4-5> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Steam-Toolkit </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/steam-market-price-bot/ title=Steam市场比价爬虫 class=md-nav__link> Steam市场比价爬虫 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-6 type=checkbox id=nav-4-6> <label class=md-nav__link for=nav-4-6> 数学建模 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=数学建模 data-md-level=2> <label class=md-nav__title for=nav-4-6> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 数学建模 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/2017-mcm-icm/ title="2017美赛参赛整理(Problem D)" class=md-nav__link> 2017美赛参赛整理(Problem D) </a> </li> <li class=md-nav__item> <a href=../../../../share/2016-guosai/ title=2016数学建模国赛 class=md-nav__link> 2016数学建模国赛 </a> </li> <li class=md-nav__item> <a href=../../../../share/math-model-szb/ title=数学建模之2016深圳杯——初次尝试 class=md-nav__link> 数学建模之2016深圳杯——初次尝试 </a> </li> <li class=md-nav__item> <a href=../../../../share/polygon-to-ellipse/ title=随机多边形转化为椭圆的过程研究 class=md-nav__link> 随机多边形转化为椭圆的过程研究 </a> </li> <li class=md-nav__item> <a href=../../../../share/FFT-GPU-Accel/ title=FFT-GPU-Accel class=md-nav__link> FFT-GPU-Accel </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7 type=checkbox id=nav-4-7> <label class=md-nav__link for=nav-4-7> NKU 数院试题整理 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="NKU 数院试题整理" data-md-level=2> <label class=md-nav__title for=nav-4-7> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> NKU 数院试题整理 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/nku-sms-exams/ title=汇总 class=md-nav__link> 汇总 </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7-2 type=checkbox id=nav-4-7-2> <label class=md-nav__link for=nav-4-7-2> 分析 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=分析 data-md-level=3> <label class=md-nav__title for=nav-4-7-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 分析 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/functional-analysis-final/ title=2017-2018第一学期泛函分析期末考试 class=md-nav__link> 2017-2018第一学期泛函分析期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/real-variable-function/ title=2016-2017第二学期实变函数期末考试 class=md-nav__link> 2016-2017第二学期实变函数期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-3-final/ title=2016-2017第一学期数学分析3-3期末考试 class=md-nav__link> 2016-2017第一学期数学分析3-3期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/complex-analysis-final/ title=2016-2017第一学期复变函数期末考试 class=md-nav__link> 2016-2017第一学期复变函数期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-3-middle/ title=2016-2017第一学期数学分析3-3期中考试 class=md-nav__link> 2016-2017第一学期数学分析3-3期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-2-final/ title=2015-2016第二学期数学分析3-2期末考试（含解答） class=md-nav__link> 2015-2016第二学期数学分析3-2期末考试（含解答） </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-2-middle/ title=2015-2016第二学期数学分析3-2期中考试 class=md-nav__link> 2015-2016第二学期数学分析3-2期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-1-final/ title=2015-2016第一学期数学分析3-1期末考试 class=md-nav__link> 2015-2016第一学期数学分析3-1期末考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7-3 type=checkbox id=nav-4-7-3> <label class=md-nav__link for=nav-4-7-3> 代数 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=代数 data-md-level=3> <label class=md-nav__title for=nav-4-7-3> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 代数 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/abstract-algebra-final/ title=2016-2017第一学期抽象代数期末考试 class=md-nav__link> 2016-2017第一学期抽象代数期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/abstract-algebra-middle/ title=2016-2017第一学期抽象代数期中考试 class=md-nav__link> 2016-2017第一学期抽象代数期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/advanced-algebra-2-2-final/ title=2015-2016第二学期高等代数2-2期末考试 class=md-nav__link> 2015-2016第二学期高等代数2-2期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/advanced-algebra-2-2-middle/ title=2015-2016第二学期高等代数2-2期中考试 class=md-nav__link> 2015-2016第二学期高等代数2-2期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/advanced-algebra-2-1-final/ title=2015-2016第一学期高等代数2-1期末考试 class=md-nav__link> 2015-2016第一学期高等代数2-1期末考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7-4 type=checkbox id=nav-4-7-4> <label class=md-nav__link for=nav-4-7-4> 概率统计 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=概率统计 data-md-level=3> <label class=md-nav__title for=nav-4-7-4> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 概率统计 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/probability-final/ title=2016-2017第二学期概率论期末考试 class=md-nav__link> 2016-2017第二学期概率论期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/probability-middle/ title=2016-2017第二学期概率论期中考试 class=md-nav__link> 2016-2017第二学期概率论期中考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7-5 type=checkbox id=nav-4-7-5> <label class=md-nav__link for=nav-4-7-5> 微分方程 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=微分方程 data-md-level=3> <label class=md-nav__title for=nav-4-7-5> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 微分方程 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/PDE-final/ title=2017-2018第一学期数理方程期末考试 class=md-nav__link> 2017-2018第一学期数理方程期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/ODE-final/ title=2016-2017第一学期常微分方程期末考试 class=md-nav__link> 2016-2017第一学期常微分方程期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/ODE-middle/ title=2016-2017第一学期常微分方程期中考试 class=md-nav__link> 2016-2017第一学期常微分方程期中考试 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-8 type=checkbox id=nav-4-8> <label class=md-nav__link for=nav-4-8> Other <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Other data-md-level=2> <label class=md-nav__title for=nav-4-8> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Other </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/github-student-pack/ title="Student Developer Pack - GitHub Education" class=md-nav__link> Student Developer Pack - GitHub Education </a> </li> <li class=md-nav__item> <a href=../../../../share/my-postgraduate-share/ title="保研推免经验分享 - 数学系跨保 CS" class=md-nav__link> 保研推免经验分享 - 数学系跨保 CS </a> </li> <li class=md-nav__item> <a href=../../../../share/roc-fly/ title=鹏程万里 class=md-nav__link> 鹏程万里 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-5 type=checkbox id=nav-5> <label class=md-nav__link for=nav-5> Statements <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Statements data-md-level=1> <label class=md-nav__title for=nav-5> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Statements </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../statements/ title=Statements class=md-nav__link> Statements </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#31-the-agentenvironment-interface class=md-nav__link> 3.1 The Agent–Environment Interface </a> <nav class=md-nav aria-label="3.1 The Agent–Environment Interface"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example-recycling-robot class=md-nav__link> Example: Recycling Robot </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#32-goals-and-rewards class=md-nav__link> 3.2 Goals and Rewards </a> </li> <li class=md-nav__item> <a href=#33-returns class=md-nav__link> 3.3 Returns </a> <nav class=md-nav aria-label="3.3 Returns"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#episodic-tasks class=md-nav__link> Episodic Tasks </a> </li> <li class=md-nav__item> <a href=#continuing-tasks class=md-nav__link> Continuing Tasks </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#34-unified-notation-for-episodic-and-continuing-tasks class=md-nav__link> 3.4 Unified Notation for Episodic and Continuing Tasks </a> </li> <li class=md-nav__item> <a href=#35-the-markov-property class=md-nav__link> 3.5 The Markov Property </a> <nav class=md-nav aria-label="3.5 The Markov Property"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#markov-property class=md-nav__link> Markov Property </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#36-markov-decision-processes class=md-nav__link> 3.6 Markov Decision Processes </a> <nav class=md-nav aria-label="3.6 Markov Decision Processes"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example-recycling-robot-mdp class=md-nav__link> Example: Recycling Robot MDP </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#37-value-functions class=md-nav__link> 3.7 Value Functions </a> <nav class=md-nav aria-label="3.7 Value Functions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#monte-carlo-methods class=md-nav__link> Monte Carlo Methods </a> </li> <li class=md-nav__item> <a href=#bellman-equation class=md-nav__link> Bellman Equation </a> </li> <li class=md-nav__item> <a href=#example-gridworld class=md-nav__link> Example: Gridworld </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#38-optimal-value-functions class=md-nav__link> 3.8 Optimal Value Functions </a> <nav class=md-nav aria-label="3.8 Optimal Value Functions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#bellman-optimality-equation class=md-nav__link> Bellman Optimality Equation </a> </li> <li class=md-nav__item> <a href=#example-solving-the-gridworld class=md-nav__link> Example: Solving the Gridworld </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#39-optimality-and-approximation class=md-nav__link> 3.9 Optimality and Approximation </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <a href=https://github.com/zawnpn/ZHANGWP/edit/master/docs/notes/reinforcement-learning/notes/RLAI_3.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg> </a> <h1 id=->强化学习导论（三）- 有限马尔可夫决策过程<a class=headerlink href=#- title="Permanent link">&para;</a></h1> <p>这章主要讲有限马尔可夫决策过程（finite MDPs）。</p> <p>首先会涉及到上一章提到的评估反馈，但与之前不同的是，现在也会开始考虑问题与环境的联系，也就是在不同情景下去做不同的选择。MDPs 是决策序列的一种经典形式化模型，其中的行动不仅会影响当前的即时奖励值，也会通过未来的奖励值来影响后续的情况或状态。</p> <h2 id=31-the-agentenvironment-interface>3.1 The Agent–Environment Interface<a class=headerlink href=#31-the-agentenvironment-interface title="Permanent link">&para;</a></h2> <ul> <li><strong>agent</strong>: 学习者/决策者</li> <li><strong>environment</strong>: agent 与外界进行交互的物体的组合</li> </ul> <p>Agent 和 environment 在一系列的时间点上进行交互：当时间为 <span><span class=MathJax_Preview>t(t = 0, 1, 2, 3, \ldots)</span><script type=math/tex>t(t = 0, 1, 2, 3, \ldots)</script></span>，agent 接收到环境的状态表示：<span><span class=MathJax_Preview>S_t \in \mathcal{S}</span><script type=math/tex>S_t \in \mathcal{S}</script></span> ，在此基础上选择了一个行动 <span><span class=MathJax_Preview>A_t \in \mathcal{A}(s)</span><script type=math/tex>A_t \in \mathcal{A}(s)</script></span> ，之后 agent 会得到作为行动反馈的数值奖励 <span><span class=MathJax_Preview>R_{t+1}\in\mathcal{R}\subset\mathbb{R}</span><script type=math/tex>R_{t+1}\in\mathcal{R}\subset\mathbb{R}</script></span>，并进入新状态 <span><span class=MathJax_Preview>S_{t+1}</span><script type=math/tex>S_{t+1}</script></span>。下图描绘了这样一个往复交替的过程。</p> <p><img alt=agent-env src=../imgs/RLAI_3/agent-env.png></p> <p>关于如何划清 agent 和 environment 的界限，有一个通用的准则：<strong>不能被 agent 任意改变的事物，均认为其属于 environment</strong>。Agent 往往能够得知很多外界的信息，甚至能知道奖励值关于其行动和状态的具体函数，但我们还是得把奖励值的计算过程放在 agent 外界，而不能让他自己进行计算，这是因为，正是这个奖励机制定义了 agent 所要学习去处理的任务，所以这个任务必须是超出 agent 控制能力的，反之则本末倒置了，如果 agent 能自我计算/改变 reward ，那他会按照自己的一些「理解」去学习问题，而没有面对问题的「客观本质」去学习。</p> <p>所以，agent 与 environment 的界限，代表着 <strong>学习者/决策体 的绝对控制权，而不是其掌握的 知识/信息</strong>。而一旦确定好三个关键因素「<strong>状态</strong>、<strong>行动</strong> 和 <strong>奖励</strong>」，就意味着这个界限已被确定下来。</p> <p>任何<strong>对受目标引导的行为</strong>的学习问题，都可以简化为三个<strong>信号</strong>在 agent 和 environment 间前后传递的模型：</p> <ul> <li><strong>actions</strong>: agent 的决策行动</li> <li><strong>states</strong>: 进行行动选择的基准</li> <li><strong>rewards</strong>: agent 的目标</li> </ul> <h3 id=example-recycling-robot>Example: Recycling Robot<a class=headerlink href=#example-recycling-robot title="Permanent link">&para;</a></h3> <p>前面的讲述可能有些抽象，这里举个实际的例子（清洁机器人）来看看 actions、states、rewards 在具体问题中是什么。这个机器人的主要目标是要持续清洁垃圾，但还需注意电量不被用完，所以在一些情况下得注意要回去充电，这就是问题背景，后面还会反复提到这个例子。</p> <p><strong>Actions</strong>:</p> <ul> <li>活动起来去搜寻垃圾（耗电）</li> <li>保持在原地等人来捡垃圾（不耗电）</li> <li>回去充电</li> </ul> <p><strong>States</strong>:</p> <ul> <li>电池状态</li> </ul> <p><strong>Rewards</strong>:</p> <ul> <li>如果清洁了垃圾，反馈一个正值作为奖励</li> <li>如果耗尽电量而没能及时返回充电，反馈一个较大负值作为惩罚</li> <li>其余情况反馈 0 奖励值</li> </ul> <p>这只是简单举个例子，用来给大家看看实际情况下是如何去通过这三个信号来定义一个问题。</p> <h2 id=32-goals-and-rewards>3.2 Goals and Rewards<a class=headerlink href=#32-goals-and-rewards title="Permanent link">&para;</a></h2> <p>在强化学习中，agent 的目标通过 environment 传入的奖励信号来量化，具体而言，目标即为<strong>最大化奖励值的累积求和</strong>。换一种说法，不同的奖励值决定了不同的目标、不同的学习问题。</p> <p>这里举个简单的例子作为对比：</p> <ul> <li>让机器人学会走路：提供与行走距离成正比的奖励值作为机器人的奖励值。</li> <li>让机器人学会逃出迷宫：逃出迷宫前的每一步都提供 -1 作为「奖励值」。</li> </ul> <p>不难理解，如果提供正值作为奖励，则会不断「鼓励」机器人多走路，反之，则会「促使」机器人找到一条最短路径逃出迷宫。</p> <p>前面说到，agent 的目标很清晰，就是最大化累积奖励值，而他的行动也很大程度受到我们给定的 rewards 的影响，不恰当地设定 rewards 必然会影响到 agent 的学习效果。所以，需要强调一点，我们要提供的 rewards ，是要用于<strong>促使</strong> agent 来<strong>达成目标</strong>，而不是根据人的先验知识来<strong>告诉他该怎样去做</strong>。说直白点，就是不要去人为「干预」他怎样学，只管给他设定目标和一些基本规则，让他多去自由探索。</p> <p>举个例子，在下棋时，应该在真正赢下比赛时才给予适当奖励，而不是在吃掉对手某个棋子，亦或占据某个区域时就立即给他奖励。虽然按人的理解，吃掉棋子之类的行动一般都看似是个不错的选择，但在一些关键的步骤，说不定以退为进才是上策，盲目吃子反而陷入陷阱。所以如果按后者那样的策略去学习，agent 最终总会倾向于不顾输赢地去拿下这些眼前利益，而不去长远考虑，失去「大局观」。</p> <p>之前强调过，rewards 的计算要放在 agent 外界，现在不难明白，原因就在于 agent 的终极目标是要去<strong>掌握之前并不能完美掌握的事物</strong>，而这正是我们设计强化学习来解决难题的本质。</p> <h2 id=33-returns>3.3 Returns<a class=headerlink href=#33-returns title="Permanent link">&para;</a></h2> <p>目前已经明确了强化学习的目标——最大化累积奖励，那么如何用数学语言来表述呢？这里我们引入一个返回值（Return）的概念，定义为<strong>奖励值序列的特定函数</strong>。定义 <span><span class=MathJax_Preview>G_t</span><script type=math/tex>G_t</script></span> 来表示时间点 t 之后的期望返回值，一种最简单的期望返回值为对未来的奖励值序列进行求和：</p> <div> <div class=MathJax_Preview>G_t\doteq R_{t+1}+R_{t+2}+R_{t+3}+\cdots+R_T</div> <script type="math/tex; mode=display">G_t\doteq R_{t+1}+R_{t+2}+R_{t+3}+\cdots+R_T</script> </div> <h3 id=episodic-tasks>Episodic Tasks<a class=headerlink href=#episodic-tasks title="Permanent link">&para;</a></h3> <p>上述的返回值表示形式只在有限时间点条件（或者说有终止时间点）下适用，我们将这类任务定义为<strong>片段式任务</strong>：</p> <ul> <li>episodes: 决策体与环境的交互过程的子片段（任意重复性的交互过程），称为 episodes</li> <li>terminal state: 每个子片段都有一个特殊状态，其后续时间点被重置进入新片段，这样的特殊片段，称为 terminal state</li> </ul> <p>在片段式任务中时常需要区分终止态，所以符号上也要有所区分：</p> <ul> <li><span><span class=MathJax_Preview>\mathcal{S}</span><script type=math/tex>\mathcal{S}</script></span>: 非终止态的全体集合</li> <li><span><span class=MathJax_Preview>\mathcal{S}^+</span><script type=math/tex>\mathcal{S}^+</script></span>: <span><span class=MathJax_Preview>\mathcal{S} \bigcup</span><script type=math/tex>\mathcal{S} \bigcup</script></span> {终止态全体}</li> </ul> <h3 id=continuing-tasks>Continuing Tasks<a class=headerlink href=#continuing-tasks title="Permanent link">&para;</a></h3> <p>在很多时候，agent 与 environment 的交互过程并不能很自然地被分解为「片段」，而是无止限地持续下去，称之为连续式任务。此时上面的 <span><span class=MathJax_Preview>G_t</span><script type=math/tex>G_t</script></span> 会因 <span><span class=MathJax_Preview>T=\infty</span><script type=math/tex>T=\infty</script></span> 而必然趋于无穷，导致 agent 无法根据返回值来进行比较学习，此时需要加入「削减系数」<span><span class=MathJax_Preview>\gamma</span><script type=math/tex>\gamma</script></span> 。</p> <p><strong>削减返回值</strong>:</p> <div> <div class=MathJax_Preview>G_t\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\cdots = \sum_{k=0}^{\infty}\gamma^kR_{t+k+1}</div> <script type="math/tex; mode=display">G_t\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\cdots = \sum_{k=0}^{\infty}\gamma^kR_{t+k+1}</script> </div> <p>其中 <span><span class=MathJax_Preview>\gamma(0\leq \gamma \leq 1)</span><script type=math/tex>\gamma(0\leq \gamma \leq 1)</script></span> 作为削减率，决定了未来奖励值在当前的表现：</p> <ul> <li>若 <span><span class=MathJax_Preview>\gamma \rightarrow 0</span><script type=math/tex>\gamma \rightarrow 0</script></span>，agent 更着重于最大化即时奖励值，也就是说，此时他的目标为学习如何选择 <span><span class=MathJax_Preview>A_t</span><script type=math/tex>A_t</script></span> 来最大化 <span><span class=MathJax_Preview>R_{t+1}</span><script type=math/tex>R_{t+1}</script></span>.</li> <li>若 <span><span class=MathJax_Preview>\gamma \rightarrow 1</span><script type=math/tex>\gamma \rightarrow 1</script></span>，返回值会更加强烈地把未来的奖励情况考虑进来，使得 agent 变得更有「远见」。</li> </ul> <h2 id=34-unified-notation-for-episodic-and-continuing-tasks>3.4 Unified Notation for Episodic and Continuing Tasks<a class=headerlink href=#34-unified-notation-for-episodic-and-continuing-tasks title="Permanent link">&para;</a></h2> <p>在后面的讨论中，我们会对各种任务来统一地来讨论分析，所以需要统一符号。</p> <p>在片段式任务中，我们应该用 <span><span class=MathJax_Preview>S_{t,i}</span><script type=math/tex>S_{t,i}</script></span> 来表示第 i 段的第 t 步（ <span><span class=MathJax_Preview>A_{t,i}, R_{t,i}, \pi_{t,i}, T_i</span><script type=math/tex>A_{t,i}, R_{t,i}, \pi_{t,i}, T_i</script></span> 等同理），但是每一段其实本质上意义相近，对于其他段我们可以统一分析，故简写 <span><span class=MathJax_Preview>S_t</span><script type=math/tex>S_t</script></span> 来统一表示各个 <span><span class=MathJax_Preview>S_{t,i}</span><script type=math/tex>S_{t,i}</script></span>，其他几个符号也同理。</p> <p>此外，我们还需要统一片段式任务和连续式任务的形式，他们分布有着有限项的返回值公式和无限项的返回值公式，为了统一公式，我们针对片段式任务加入一种特殊的「吸收态」，其特点是状态的交互和转移过程都只在自身进行，且奖励值为 0 ，如下图所示：</p> <p><img alt="Absorbing State" src=../imgs/RLAI_3/unified.png></p> <p>这样显然可见，片段式任务也能表示为无穷项了，只不过原本的终止态之后 reward 为 0，不对公式造成影响。于是我们可以统一地定义：</p> <div> <div class=MathJax_Preview>G_t\doteq \sum_{k=t+1}^{T}\gamma^{k-t-1}R_k</div> <script type="math/tex; mode=display">G_t\doteq \sum_{k=t+1}^{T}\gamma^{k-t-1}R_k</script> </div> <h2 id=35-the-markov-property>3.5 The Markov Property<a class=headerlink href=#35-the-markov-property title="Permanent link">&para;</a></h2> <p>回想我们进行强化学习的主要目标在于，学习出一个能由任意状态信号决定行动的策略函数，而状态是以<strong>即时感知以及历史状态和信息</strong>为基础，逐渐<strong>构筑、维护</strong>下来的，他包含一定的历史信息，但并不意味着能从中得知环境中的一切信息。比如棋盘上的棋子，棋子在不同的位置即在不同的状态，从每个状态，我们能知道走下这步棋各种可能的组合，但并不能准确得知他是如何一步一步走到这一步的。</p> <p>正如上面的棋子的例子一样，我们称，如果一个状态继承并保留了所有相关信息，则具有<strong>马尔可夫性质</strong>。下面在用数学语言描述一下这一性质。</p> <h3 id=markov-property>Markov Property<a class=headerlink href=#markov-property title="Permanent link">&para;</a></h3> <p>首先定义基于全部历史信息的完全联合分布：</p> <div> <div class=MathJax_Preview>\mathrm{Pr}\{S_{t+1}=s',R_{t+1}=r|S_0,A_0,R_1,\ldots,S_{t-1},A_{t-1},R_t,S_t,A_t\}</div> <script type="math/tex; mode=display">\mathrm{Pr}\{S_{t+1}=s',R_{t+1}=r|S_0,A_0,R_1,\ldots,S_{t-1},A_{t-1},R_t,S_t,A_t\}</script> </div> <p>环境的动态分布定义为：</p> <div> <div class=MathJax_Preview>p(s',r|,s,a)\doteq \mathrm{Pr}\{S_{t+1}=s',R_{t+1}=r|S_t=s,A_t=a\}</div> <script type="math/tex; mode=display">p(s',r|,s,a)\doteq \mathrm{Pr}\{S_{t+1}=s',R_{t+1}=r|S_t=s,A_t=a\}</script> </div> <p>当且仅当上面两式相等，即</p> <div> <div class=MathJax_Preview>p(s',r|s,a) = \mathrm{Pr}\{S_{t+1}=s',R_{t+1}=r|S_0,A_0,R_1,\ldots,S_t,A_t\}</div> <script type="math/tex; mode=display">p(s',r|s,a) = \mathrm{Pr}\{S_{t+1}=s',R_{t+1}=r|S_0,A_0,R_1,\ldots,S_t,A_t\}</script> </div> <p>我们称这样的环境和任务具有<strong>马尔可夫性质</strong>。</p> <p>马尔科夫性质在强化学习中的重要之处体现在，如果问题具备这一条件，那么各种决策值将只与当前状态有关，这能极大程度地方便我们分析各种理论和模型。即使是非严格遵守马尔可夫性质的问题，也能有所应用。</p> <p><em>这本书之后的所有理论都将基于马尔可夫性质来讨论。</em></p> <h2 id=36-markov-decision-processes>3.6 Markov Decision Processes<a class=headerlink href=#36-markov-decision-processes title="Permanent link">&para;</a></h2> <p>如果一个强化学习任务满足马尔可夫性质，我们称之为<strong>马尔可夫决策过程（MDP）</strong>。如果状态空间和行动空间都是有限的，则称为<strong>有限马尔可夫过程（finite MDP）</strong>。</p> <p>在有限马尔可夫过程中，状态集、行动集、奖励集（<span><span class=MathJax_Preview>\mathcal{S}, \mathcal{A}, \mathcal{R}</span><script type=math/tex>\mathcal{S}, \mathcal{A}, \mathcal{R}</script></span>）内的元素均有上界，所以能定义</p> <div> <div class=MathJax_Preview>p(s',r|s,a)\doteq\mathrm{Pr}\left\{S_t=s',R_t=r|S_{t-1}=s,A_{t-1}=a\right\}</div> <script type="math/tex; mode=display">p(s',r|s,a)\doteq\mathrm{Pr}\left\{S_t=s',R_t=r|S_{t-1}=s,A_{t-1}=a\right\}</script> </div> <p>其中</p> <div> <div class=MathJax_Preview>\sum_{s'\in\mathcal{S}}\sum_{r\in\mathcal{R}}p(s',r|s,a)=1</div> <script type="math/tex; mode=display">\sum_{s'\in\mathcal{S}}\sum_{r\in\mathcal{R}}p(s',r|s,a)=1</script> </div> <p>有了 <span><span class=MathJax_Preview>p(s',r|s,a)</span><script type=math/tex>p(s',r|s,a)</script></span> ，我们可以进一步计算得到：</p> <ul> <li><strong>状态转移概率</strong>:</li> </ul> <div> <div class=MathJax_Preview>p(s'|s,a)\doteq\mathrm{Pr}\left\{S_t=s'|S_{t-1}=s,A_{t-1}=a\right\}=\sum_{r\in\mathcal{R}}p(s',r|s,a)</div> <script type="math/tex; mode=display">p(s'|s,a)\doteq\mathrm{Pr}\left\{S_t=s'|S_{t-1}=s,A_{t-1}=a\right\}=\sum_{r\in\mathcal{R}}p(s',r|s,a)</script> </div> <ul> <li><strong>给定状态、行动的期望奖励</strong>:</li> </ul> <div> <div class=MathJax_Preview>r(s,a)\doteq \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a\right]=\sum_{r\in\mathcal{R}}r\sum_{s'\in\mathcal{S}}p(s',r|s,a)</div> <script type="math/tex; mode=display">r(s,a)\doteq \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a\right]=\sum_{r\in\mathcal{R}}r\sum_{s'\in\mathcal{S}}p(s',r|s,a)</script> </div> <ul> <li><strong>给定状态、行动、后继状态的期望奖励</strong>:</li> </ul> <div> <div class=MathJax_Preview>r(s,a,s')\doteq \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a,S_t=s'\right]=\sum_{r\in\mathcal{R}}r\frac{p(s',r|s,a)}{p(s'|s,a)}</div> <script type="math/tex; mode=display">r(s,a,s')\doteq \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a,S_t=s'\right]=\sum_{r\in\mathcal{R}}r\frac{p(s',r|s,a)}{p(s'|s,a)}</script> </div> <p>得到这些动态性质后，有助于我们后面的具体分析。</p> <h3 id=example-recycling-robot-mdp>Example: Recycling Robot MDP<a class=headerlink href=#example-recycling-robot-mdp title="Permanent link">&para;</a></h3> <p>前面提到的清洁机器人，便能按照有限马尔科夫决策过程来分析。下面的图表详细地描述了怎样用 MDP 来分析问题。比如第三、四行，表示机器人在低电量下仍去搜索垃圾，有 <span><span class=MathJax_Preview>\beta</span><script type=math/tex>\beta</script></span> 的概率仍然是低电量，收获 <span><span class=MathJax_Preview>r_{search}</span><script type=math/tex>r_{search}</script></span> 的奖励值，有 <span><span class=MathJax_Preview>\beta​</span><script type=math/tex>\beta​</script></span> 的概率耗尽电量未能及时自行返回充电，被人工带回充电后进入高电量状态，同时给予 -3 的惩罚值。其他的过程也可类似分析，可以看出在分析接下来情景时，我们无需考虑机器人过去所有做过的事情，只需要从当前状态出发，分析进入下一个可能状态的过程即可。</p> <p><img alt=robot_tb src=../imgs/RLAI_3/robot_tb.png></p> <p><img alt=robot_diag src=../imgs/RLAI_3/robot_diag.png></p> <h2 id=37-value-functions>3.7 Value Functions<a class=headerlink href=#37-value-functions title="Permanent link">&para;</a></h2> <p>强化学习算法总是涉及到估计 value function，用于量化评估不同条件下行动的好坏程度。这里我们可以对返回值求期望来作为 value function 来进行评估，而显然 agent 的策略决定了如何来计算这一期望。</p> <p>回想我们反复说到的策略这一概念，简单来说，策略是由状态空间 <span><span class=MathJax_Preview>\mathcal{S}</span><script type=math/tex>\mathcal{S}</script></span>、行动空间 <span><span class=MathJax_Preview>\mathcal{A}</span><script type=math/tex>\mathcal{A}</script></span> 到概率空间的一个映射：<span><span class=MathJax_Preview>\pi:\mathcal{S}\times\mathcal{A}\to \mathcal{P}</span><script type=math/tex>\pi:\mathcal{S}\times\mathcal{A}\to \mathcal{P}</script></span> 。</p> <ul> <li><span><span class=MathJax_Preview>\pi(a|s)</span><script type=math/tex>\pi(a|s)</script></span>: 状态为 s 时选择行动 a 的概率</li> <li><span><span class=MathJax_Preview>v_{\pi}(s)</span><script type=math/tex>v_{\pi}(s)</script></span>: <strong>策略 <span><span class=MathJax_Preview>\pi</span><script type=math/tex>\pi</script></span> 下的状态值函数</strong>。表示状态 s 下，遵守策略 <span><span class=MathJax_Preview>\pi</span><script type=math/tex>\pi</script></span> 的期望返回值。对于 MDP，我们可以定义</li> </ul> <div> <div class=MathJax_Preview>v_{\pi}(s)\doteq\mathbb{E}_{\pi}\left[G_t|S_t=s\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}\mid S_t=s\right], \forall s \in \mathcal{S}</div> <script type="math/tex; mode=display">v_{\pi}(s)\doteq\mathbb{E}_{\pi}\left[G_t|S_t=s\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}\mid S_t=s\right], \forall s \in \mathcal{S}</script> </div> <ul> <li><span><span class=MathJax_Preview>q_{\pi}(s,a)</span><script type=math/tex>q_{\pi}(s,a)</script></span>: <strong>策略 <span><span class=MathJax_Preview>\pi</span><script type=math/tex>\pi</script></span> 下的行动值函数</strong>。表示状态 s 下，选择行动 a ，遵守策略 <span><span class=MathJax_Preview>\pi</span><script type=math/tex>\pi</script></span> 的期望返回值。类似地，可以定义</li> </ul> <div> <div class=MathJax_Preview>q_{\pi}(s,a)\doteq\mathbb{E}_{\pi}\left[G_t|S_t=s,A_t=a\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}\mid S_t=s,A_t=a\right]</div> <script type="math/tex; mode=display">q_{\pi}(s,a)\doteq\mathbb{E}_{\pi}\left[G_t|S_t=s,A_t=a\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}\mid S_t=s,A_t=a\right]</script> </div> <h3 id=monte-carlo-methods><strong>Monte Carlo Methods</strong><a class=headerlink href=#monte-carlo-methods title="Permanent link">&para;</a></h3> <p>上面的函数可以通过模拟方法来进行经验估计，称为蒙特卡洛方法。</p> <ul> <li>在实验中，若 agent 在遵守策略 <span><span class=MathJax_Preview>\pi</span><script type=math/tex>\pi</script></span> 的条件下，为每个出发状态 s 都保留了后面的真实返回值的均值，显然这个均值会随实验次数增加而最终收敛到 <span><span class=MathJax_Preview>v_{\pi}(s)</span><script type=math/tex>v_{\pi}(s)</script></span>.</li> <li>类似地，如果还细分到为每个出发状态 s、每个行动选择 a 都保留了真实返回值的均值，则便能最终收敛到 <span><span class=MathJax_Preview>q_{\pi}(s,a)</span><script type=math/tex>q_{\pi}(s,a)</script></span>.</li> </ul> <p>如果状态空间很大，显然可知，进行上述的大量样本实验来模拟估计很不现实。此时可以考虑将 <span><span class=MathJax_Preview>v_{\pi}, q_{\pi}</span><script type=math/tex>v_{\pi}, q_{\pi}</script></span> 看作参数化函数，通过调参来逼近真实值，一样能够较为精确地进行估计（第五章讲）。</p> <h3 id=bellman-equation>Bellman Equation<a class=headerlink href=#bellman-equation title="Permanent link">&para;</a></h3> <p>Value function 被用在强化学习和动态规划的一个基本性质在于其满足一种特殊递归关系：</p> <div> <div class=MathJax_Preview>\begin{aligned} v_\pi(s) &amp;\doteq \mathbb{E}_\pi[G_t\,|\,S_t=s] \\ &amp;=\mathbb{E}\pi \left[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\,|\,S_t=s \right] \\ &amp;= \mathbb{E}_\pi \left [R_{t+1}+\gamma\sum_{k=0}^\infty\gamma^kR_{t+k+2}\,|\,S_t=s \right] \\ &amp;=\sum_a\pi(a|s)\sum_{s'}\sum_rp(s',r|s,a)\left[r+\gamma \mathbb{E}_\pi \left[\sum_{k=0}^\infty\gamma^kR_{t+k+2}\,|\,S_{t+1}=s' \right] \right] \\ &amp;= \sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)\left[r+\gamma v_\pi(s') \right], \qquad \forall s \in \mathcal S \end{aligned}</div> <script type="math/tex; mode=display">\begin{aligned} v_\pi(s) &\doteq \mathbb{E}_\pi[G_t\,|\,S_t=s] \\ &=\mathbb{E}\pi \left[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\,|\,S_t=s \right] \\ &= \mathbb{E}_\pi \left [R_{t+1}+\gamma\sum_{k=0}^\infty\gamma^kR_{t+k+2}\,|\,S_t=s \right] \\ &=\sum_a\pi(a|s)\sum_{s'}\sum_rp(s',r|s,a)\left[r+\gamma \mathbb{E}_\pi \left[\sum_{k=0}^\infty\gamma^kR_{t+k+2}\,|\,S_{t+1}=s' \right] \right] \\ &= \sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)\left[r+\gamma v_\pi(s') \right], \qquad \forall s \in \mathcal S \end{aligned}</script> </div> <ul> <li><span><span class=MathJax_Preview>a</span><script type=math/tex>a</script></span>: 一个行动，取自行动集 <span><span class=MathJax_Preview>\mathcal{A}(s)</span><script type=math/tex>\mathcal{A}(s)</script></span></li> <li><span><span class=MathJax_Preview>s'</span><script type=math/tex>s'</script></span>: 后继状态，取自状态集 <span><span class=MathJax_Preview>\mathcal{S}</span><script type=math/tex>\mathcal{S}</script></span> （对于片段式任务，取自 <span><span class=MathJax_Preview>\mathcal{S}^+</span><script type=math/tex>\mathcal{S}^+</script></span> ）</li> <li><span><span class=MathJax_Preview>r</span><script type=math/tex>r</script></span>: 奖励值，取自奖励集 <span><span class=MathJax_Preview>\mathcal{R}</span><script type=math/tex>\mathcal{R}</script></span></li> </ul> <p>对于任意策略 <span><span class=MathJax_Preview>\pi</span><script type=math/tex>\pi</script></span> 和状态 s ，当前 value 和未来可能的 value 满足上面的递推关系，称为<strong>贝尔曼方程（Bellman Equation）</strong>。</p> <p><strong><span><span class=MathJax_Preview>v_\pi</span><script type=math/tex>v_\pi</script></span> 的贝尔曼方程：</strong></p> <blockquote> <div> <div class=MathJax_Preview>v_{\pi}(s) = \sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)\left[r+\gamma v_\pi(s') \right]</div> <script type="math/tex; mode=display">v_{\pi}(s) = \sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)\left[r+\gamma v_\pi(s') \right]</script> </div> </blockquote> <p>贝尔曼方程描述了<strong>状态值与所有后继状态值的关系</strong>。</p> <p><img alt=back-diag-v src=../imgs/RLAI_3/back-diag-v.png></p> <p>如上面这样的示意图，我们称为「<strong>backup diagrams</strong>」，它能描绘出问题的更新反馈机制，这也正是强化学习算法的一个核心之处。</p> <h3 id=example-gridworld>Example: Gridworld<a class=headerlink href=#example-gridworld title="Permanent link">&para;</a></h3> <p><img alt=grid-world src=../imgs/RLAI_3/grid-world.png></p> <p>在这个例子中，每一格上有四个等概率行动（往四个方向移动）。若从靠边的格子往界外移动，则会给予 -1 的惩罚，但位置保持不变，若移入特殊点 <span><span class=MathJax_Preview>A,B</span><script type=math/tex>A,B</script></span> ，则分别被移入 <span><span class=MathJax_Preview>A',B'</span><script type=math/tex>A',B'</script></span> 并给予 +10、+5 的特殊奖励，其余情况的奖励为 0 。</p> <p>右图则是通过解贝尔曼线性方程组求出了 <span><span class=MathJax_Preview>v_{\pi}</span><script type=math/tex>v_{\pi}</script></span> 的解并填写在格子上（每个格子分别对应一个状态 <span><span class=MathJax_Preview>s_i</span><script type=math/tex>s_i</script></span>，填写的值则为 <span><span class=MathJax_Preview>v_{\pi}(s_i)</span><script type=math/tex>v_{\pi}(s_i)</script></span> ）。本例中，<span><span class=MathJax_Preview>\gamma = 0.9</span><script type=math/tex>\gamma = 0.9</script></span>。</p> <h2 id=38-optimal-value-functions>3.8 Optimal Value Functions<a class=headerlink href=#38-optimal-value-functions title="Permanent link">&para;</a></h2> <p>解决强化学习问题就是在于找到一个最优的测量，使 agent 能够按照该策略行动并得到最好的累积奖励值，在 MDP 中，当且仅当一个策略 <span><span class=MathJax_Preview>\pi​</span><script type=math/tex>\pi​</script></span> 在任意状态下的期望返回值都大于等于策略 <span><span class=MathJax_Preview>\pi'​</span><script type=math/tex>\pi'​</script></span> 的期望返回值，称策略 <span><span class=MathJax_Preview>\pi​</span><script type=math/tex>\pi​</script></span> 优于策略 <span><span class=MathJax_Preview>\pi'​</span><script type=math/tex>\pi'​</script></span> ，即： <span><span class=MathJax_Preview>\pi \geq \pi'​</span><script type=math/tex>\pi \geq \pi'​</script></span> 当且仅当 <span><span class=MathJax_Preview>v_{\pi}(s)\geq v_{\pi'}(s), \forall s \in \mathcal{S}​</span><script type=math/tex>v_{\pi}(s)\geq v_{\pi'}(s), \forall s \in \mathcal{S}​</script></span>.</p> <ul> <li><strong>最优策略</strong>：至少存在一个策略，优于其他所有策略，称所有这样的策略为最优策略</li> <li><strong>最优状态值函数</strong>：<span><span class=MathJax_Preview>v_*(s)\doteq \max\limits_{\pi}v_{\pi}(s),\forall s \in \mathcal{S}</span><script type=math/tex>v_*(s)\doteq \max\limits_{\pi}v_{\pi}(s),\forall s \in \mathcal{S}</script></span></li> <li><strong>最优行动值函数</strong>：<span><span class=MathJax_Preview>q_*(s,a)=\max\limits_{\pi}q_{\pi}(s,a),\forall s \in \mathcal{S}</span><script type=math/tex>q_*(s,a)=\max\limits_{\pi}q_{\pi}(s,a),\forall s \in \mathcal{S}</script></span></li> </ul> <p>其中易知，<span><span class=MathJax_Preview>q_*(s,a)=\mathbb{E}\left[R_{t+1}+\gamma v_*(S_{t+1})\mid S_t=s,A_t=a\right]</span><script type=math/tex>q_*(s,a)=\mathbb{E}\left[R_{t+1}+\gamma v_*(S_{t+1})\mid S_t=s,A_t=a\right]</script></span></p> <h3 id=bellman-optimality-equation>Bellman Optimality Equation<a class=headerlink href=#bellman-optimality-equation title="Permanent link">&para;</a></h3> <p><span><span class=MathJax_Preview>v_*</span><script type=math/tex>v_*</script></span> 作为 value function，也有贝尔曼方程，此时称该方程为贝尔曼最优方程。</p> <p>贝尔曼最优方程表述了<strong>最优策略下一个状态的状态值，必然等于该状态下最优行动的期望返回值</strong>：</p> <div> <div class=MathJax_Preview>\begin{aligned}v_*(s) &amp;= \max_{a\in \mathcal A(s)}q_{\pi_*}(s,a) \\ &amp;=\max_a \mathbb{E}_{\pi_*} \left[G_t \mid S_t=s, A_t=a \right] \\ &amp;= \max_a \mathbb{E}_{\pi_*} \left[\sum_{k=0}^\infty \gamma^kR_{t+k+1} \mid S_t=s,A_t=a \right] \\ &amp;= \max_a \mathbb{E}_{\pi_*} \left[R_{t+1}+\gamma \sum_{k=0}^\infty\gamma^kR_{t+k+2}\mid S_t=s,A_t=a \right] \\ &amp;= \max_a \mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1}) \mid S_t=s,A_t=a] \\ &amp;=\max_{a}\sum_{s',r}p(s',r \mid s,a)[r+\gamma v_*(s')]\end{aligned}</div> <script type="math/tex; mode=display">\begin{aligned}v_*(s) &= \max_{a\in \mathcal A(s)}q_{\pi_*}(s,a) \\ &=\max_a \mathbb{E}_{\pi_*} \left[G_t \mid S_t=s, A_t=a \right] \\ &= \max_a \mathbb{E}_{\pi_*} \left[\sum_{k=0}^\infty \gamma^kR_{t+k+1} \mid S_t=s,A_t=a \right] \\ &= \max_a \mathbb{E}_{\pi_*} \left[R_{t+1}+\gamma \sum_{k=0}^\infty\gamma^kR_{t+k+2}\mid S_t=s,A_t=a \right] \\ &= \max_a \mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1}) \mid S_t=s,A_t=a] \\ &=\max_{a}\sum_{s',r}p(s',r \mid s,a)[r+\gamma v_*(s')]\end{aligned}</script> </div> <p><strong>贝尔曼最优方程</strong>:</p> <ul> <li><span><span class=MathJax_Preview>v_*</span><script type=math/tex>v_*</script></span>:</li> </ul> <blockquote> <div> <div class=MathJax_Preview>\begin{aligned} v_*(s)&amp;= \max_a \mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1}) \mid S_t=s,A_t=a]\\&amp;=\max_{a}\sum_{s',r}p(s',r \mid s,a)[r+\gamma v_*(s')]\end{aligned}</div> <script type="math/tex; mode=display">\begin{aligned} v_*(s)&= \max_a \mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1}) \mid S_t=s,A_t=a]\\&=\max_{a}\sum_{s',r}p(s',r \mid s,a)[r+\gamma v_*(s')]\end{aligned}</script> </div> </blockquote> <ul> <li><span><span class=MathJax_Preview>q_*</span><script type=math/tex>q_*</script></span>:</li> </ul> <blockquote> <div> <div class=MathJax_Preview>\begin{aligned}q_*(s,a) &amp;= \mathbb{E} \left[R_{t+1}+\gamma \max_{a'}q_*(S_{t+1},a') \mid S_t=s,A_t=a \right] \\ &amp;= \sum_{s',r}p(s',r \mid s,a)\left[r+\gamma \max_{a'}q_*(s',a') \right]\end{aligned}</div> <script type="math/tex; mode=display">\begin{aligned}q_*(s,a) &= \mathbb{E} \left[R_{t+1}+\gamma \max_{a'}q_*(S_{t+1},a') \mid S_t=s,A_t=a \right] \\ &= \sum_{s',r}p(s',r \mid s,a)\left[r+\gamma \max_{a'}q_*(s',a') \right]\end{aligned}</script> </div> </blockquote> <p><img alt=opt-diag src=../imgs/RLAI_3/opt-diag.png></p> <p>在上面的 backup diagrams 中，我们用一个圆弧来表示在不同的选项中去最大值。</p> <p>对于有限 MDP ，贝尔曼最优方程一定有独立于策略的唯一解。事实上，设有 n 个状态，每个状态对应一个方程，这样总共由 n 个不同的方程形成 n 元方程组，如果环境的动态 <span><span class=MathJax_Preview>p(s',r|s,a)</span><script type=math/tex>p(s',r|s,a)</script></span> 给定，由数学知识可知，该方程组必然能得到唯一解 <span><span class=MathJax_Preview>v_*</span><script type=math/tex>v_*</script></span> ，进而又可利用其计算得到 <span><span class=MathJax_Preview>q_*</span><script type=math/tex>q_*</script></span> 。</p> <p>一旦求得这一解，便能确定最优策略：</p> <blockquote> <p><strong>最优策略</strong>：显然至少存在一个行动能使行动值取到 <span><span class=MathJax_Preview>v_*</span><script type=math/tex>v_*</script></span> ，如果一个策略只将非 0 概率分配给这样的行动，称这个策略是最优策略。</p> </blockquote> <p>如果每一步都采取<strong>贪心策略</strong>，即只根据 <span><span class=MathJax_Preview>v_*</span><script type=math/tex>v_*</script></span> 来确定下一步的行动，这样的行动却恰好是最优行动，<span><span class=MathJax_Preview>v_*</span><script type=math/tex>v_*</script></span> 的优美之处便体现于此。之所以能达到这一效果，是因为 <span><span class=MathJax_Preview>v_*</span><script type=math/tex>v_*</script></span> 已经考虑到了未来所有可能性，于是，看似贪心的「一步搜索」却能生成出全局最优行动。</p> <p>如果我们进一步解得了 <span><span class=MathJax_Preview>q_*</span><script type=math/tex>q_*</script></span> ，agent 甚至都无需来做「一步搜索」：对于任意状态 s ，只需找到 <span><span class=MathJax_Preview>a_0</span><script type=math/tex>a_0</script></span> 使得 <span><span class=MathJax_Preview>q_*(s,a_0)=\max\limits_{a}q_*(s,a)</span><script type=math/tex>q_*(s,a_0)=\max\limits_{a}q_*(s,a)</script></span> 即可。这是因为我们已经在之前的工作中多做了一些准备，将进一步的搜索信息缓存在了各个 <span><span class=MathJax_Preview>q_*</span><script type=math/tex>q_*</script></span> 中，使得它的信息量比 <span><span class=MathJax_Preview>v_*</span><script type=math/tex>v_*</script></span> 更大。</p> <h3 id=example-solving-the-gridworld>Example: Solving the Gridworld<a class=headerlink href=#example-solving-the-gridworld title="Permanent link">&para;</a></h3> <p><img alt=grid-world src=../imgs/RLAI_3/grid-world_solve.png></p> <ul> <li>中图：最优状态值函数</li> <li>右图：最优策略</li> </ul> <p>显式求解贝尔曼最优方程虽然能直接确定最优策略，但是并不太实用，因为这一方法涉及到了「穷举」，依赖严苛的计算资源和内存条件。此外，这个方法还依赖于三个假设，使得问题本身就很难利用上这一方法：</p> <ol> <li>精确地知道环境的动态性质 <span><span class=MathJax_Preview>p</span><script type=math/tex>p</script></span></li> <li>有足够的计算资源</li> <li>满足马尔可夫性质</li> </ol> <p>在后面的章节中会介绍一些算法，如「启发式搜索」、「动态规划」等，可以看作是对「解贝尔曼方程」的近似求解，能够弥补上述的一些不足之处。</p> <h2 id=39-optimality-and-approximation>3.9 Optimality and Approximation<a class=headerlink href=#39-optimality-and-approximation title="Permanent link">&para;</a></h2> <p>前面已经讲过，最优策略是通过耗费极端的计算资源求得的，这使得我们不得不考虑一些方法来近似估计前面的一些函数。</p> <p>在近似求解最优行为时，我们不难想象，会有很多状态其实只会以极低概率出现，我们若仍对其求解最优行动则意义不大，这时候如果选取一个局部最优行动来代替最优行动，显然，从整体期望意义来看，对总体奖励值造成的影响其实并不大，但却能节省很多的计算资源。与之对应的，当遇到那些经常出现的状态，我们则务必求出最优解。这是在解决 MDP 时强化学习方法区别于其他近似方法的一个重要性质。</p> <hr> <div class=md-source-date> <small> Last update: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">August 1, 2019</span> </small> </div> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid" aria-label=Footer> <a href=../RLAI_2/ title="Chapter 2" class="md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Previous </span> Chapter 2 </div> </div> </a> <a href=../RLAI_4/ title="Chapter 4" class="md-footer-nav__link md-footer-nav__link--next" rel=next> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Next </span> Chapter 4 </div> </div> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2016-2020 ZHANGWP </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-footer-social> <a href=https://github.com/zawnpn target=_blank rel=noopener title=github.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://twitter.com/zawnpn target=_blank rel=noopener title=twitter.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> </a> <a href=https://psnprofiles.com/zawnpn target=_blank rel=noopener title=psnprofiles.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><path d="M570.9 372.3c-11.3 14.2-38.8 24.3-38.8 24.3L327 470.2v-54.3l150.9-53.8c17.1-6.1 19.8-14.8 5.8-19.4-13.9-4.6-39.1-3.3-56.2 2.9L327 381.1v-56.4c23.2-7.8 47.1-13.6 75.7-16.8 40.9-4.5 90.9.6 130.2 15.5 44.2 14 49.2 34.7 38 48.9zm-224.4-92.5v-139c0-16.3-3-31.3-18.3-35.6-11.7-3.8-19 7.1-19 23.4v347.9l-93.8-29.8V32c39.9 7.4 98 24.9 129.2 35.4C424.1 94.7 451 128.7 451 205.2c0 74.5-46 102.8-104.5 74.6zM43.2 410.2c-45.4-12.8-53-39.5-32.3-54.8 19.1-14.2 51.7-24.9 51.7-24.9l134.5-47.8v54.5l-96.8 34.6c-17.1 6.1-19.7 14.8-5.8 19.4 13.9 4.6 39.1 3.3 56.2-2.9l46.4-16.9v48.8c-51.6 9.3-101.4 7.3-153.9-10z"/></svg> </a> <a href=https://steamcommunity.com/id/zawnpn/ target=_blank rel=noopener title=steamcommunity.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M496 256c0 137-111.2 248-248.4 248-113.8 0-209.6-76.3-239-180.4l95.2 39.3c6.4 32.1 34.9 56.4 68.9 56.4 39.2 0 71.9-32.4 70.2-73.5l84.5-60.2c52.1 1.3 95.8-40.9 95.8-93.5 0-51.6-42-93.5-93.7-93.5s-93.7 42-93.7 93.5v1.2L176.6 279c-15.5-.9-30.7 3.4-43.5 12.1L0 236.1C10.2 108.4 117.1 8 247.6 8 384.8 8 496 119 496 256zM155.7 384.3l-30.5-12.6a52.79 52.79 0 0027.2 25.8c26.9 11.2 57.8-1.6 69-28.4 5.4-13 5.5-27.3.1-40.3-5.4-13-15.5-23.2-28.5-28.6-12.9-5.4-26.7-5.2-38.9-.6l31.5 13c19.8 8.2 29.2 30.9 20.9 50.7-8.3 19.9-31 29.2-50.8 21zm173.8-129.9c-34.4 0-62.4-28-62.4-62.3s28-62.3 62.4-62.3 62.4 28 62.4 62.3-27.9 62.3-62.4 62.3zm.1-15.6c25.9 0 46.9-21 46.9-46.8 0-25.9-21-46.8-46.9-46.8s-46.9 21-46.9 46.8c.1 25.8 21.1 46.8 46.9 46.8z"/></svg> </a> <a href=https://www.zhihu.com/people/zhangwanpeng target=_blank rel=noopener title=www.zhihu.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13H170.54zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82v170.31zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62v.01zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2l19.23 14.43zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78l.03-.01z"/></svg> </a> </div> </div> </div> </footer> </div> <script src=../../../../assets/javascripts/vendor.d710d30a.min.js></script> <script src=../../../../assets/javascripts/bundle.b39636ac.min.js></script><script id=__lang type=application/json>{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script> <script>
        app = initialize({
          base: "../../../..",
          features: ["tabs"],
          search: Object.assign({
            worker: "../../../../assets/javascripts/worker/search.a68abb33.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script> <script src="//cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-MML-AM_SVG"></script> </body> </html>