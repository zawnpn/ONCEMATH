



<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="Welcome to zhangwp's blog.">
      
      
        <link rel="canonical" href="https://www.zhangwp.com/notes/reinforcement-learning/notes/RLAI_3/">
      
      
        <meta name="author" content="zawnpn">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="jp">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.0.4">
    
    
      
        <title>Chapter 3 - ZHANGWP</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/application.451f80e5.css">
      
        <link rel="stylesheet" href="../../../../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
      <script src="../../../../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      
    
    <link rel="stylesheet" href="../../../../assets/fonts/material-icons.css">
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="white" data-md-color-accent="light-blue">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="../../../../#-" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://www.zhangwp.com" title="ZHANGWP" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                ZHANGWP
              </span>
              <span class="md-header-nav__topic">
                Chapter 3
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/zawnpn/ZHANGWP/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      GitHub
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../.." title="Home" class="md-tabs__link">
          Home
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../" title="Notes" class="md-tabs__link md-tabs__link--active">
          Notes
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../../tips/" title="Tips" class="md-tabs__link">
          Tips
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../../share/" title="Share" class="md-tabs__link">
          Share
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../../statements/" title="Statements" class="md-tabs__link">
          Statements
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://www.zhangwp.com" title="ZHANGWP" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    ZHANGWP
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/zawnpn/ZHANGWP/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      GitHub
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1" type="checkbox" id="nav-1">
    
    <label class="md-nav__link" for="nav-1">
      Home
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-1">
        Home
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../.." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../links/" title="Links" class="md-nav__link">
      Links
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../donates/" title="Donate" class="md-nav__link">
      Donate
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      Notes
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Notes
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../" title="Index" class="md-nav__link">
      Index
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-2" type="checkbox" id="nav-2-2" checked>
    
    <label class="md-nav__link" for="nav-2-2">
      Reinforcement
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-2-2">
        Reinforcement
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-2-1" type="checkbox" id="nav-2-2-1" checked>
    
    <label class="md-nav__link" for="nav-2-2-1">
      Reinforcement Learning An Introduction
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-2-2-1">
        Reinforcement Learning An Introduction
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_2/" title="Chapter 2" class="md-nav__link">
      Chapter 2
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Chapter 3
      </label>
    
    <a href="./" title="Chapter 3" class="md-nav__link md-nav__link--active">
      Chapter 3
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#31-the-agentenvironment-interface" title="3.1 The Agent–Environment Interface" class="md-nav__link">
    3.1 The Agent–Environment Interface
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-recycling-robot" title="Example: Recycling Robot" class="md-nav__link">
    Example: Recycling Robot
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#32-goals-and-rewards" title="3.2 Goals and Rewards" class="md-nav__link">
    3.2 Goals and Rewards
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#33-returns" title="3.3 Returns" class="md-nav__link">
    3.3 Returns
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#episodic-tasks" title="Episodic Tasks" class="md-nav__link">
    Episodic Tasks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#continuing-tasks" title="Continuing Tasks" class="md-nav__link">
    Continuing Tasks
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#34-unified-notation-for-episodic-and-continuing-tasks" title="3.4 Unified Notation for Episodic and Continuing Tasks" class="md-nav__link">
    3.4 Unified Notation for Episodic and Continuing Tasks
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#35-the-markov-property" title="3.5 The Markov Property" class="md-nav__link">
    3.5 The Markov Property
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#markov-property" title="Markov Property" class="md-nav__link">
    Markov Property
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#36-markov-decision-processes" title="3.6 Markov Decision Processes" class="md-nav__link">
    3.6 Markov Decision Processes
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-recycling-robot-mdp" title="Example: Recycling Robot MDP" class="md-nav__link">
    Example: Recycling Robot MDP
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#37-value-functions" title="3.7 Value Functions" class="md-nav__link">
    3.7 Value Functions
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#monte-carlo-methods" title="Monte Carlo Methods" class="md-nav__link">
    Monte Carlo Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bellman-equation" title="Bellman Equation" class="md-nav__link">
    Bellman Equation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-gridworld" title="Example: Gridworld" class="md-nav__link">
    Example: Gridworld
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#38-optimal-value-functions" title="3.8 Optimal Value Functions" class="md-nav__link">
    3.8 Optimal Value Functions
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bellman-optimality-equation" title="Bellman Optimality Equation" class="md-nav__link">
    Bellman Optimality Equation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-solving-the-gridworld" title="Example: Solving the Gridworld" class="md-nav__link">
    Example: Solving the Gridworld
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#39-optimality-and-approximation" title="3.9 Optimality and Approximation" class="md-nav__link">
    3.9 Optimality and Approximation
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_4/" title="Chapter 4" class="md-nav__link">
      Chapter 4
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_5/" title="Chapter 5" class="md-nav__link">
      Chapter 5
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_6/" title="Chapter 6" class="md-nav__link">
      Chapter 6
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_7/" title="Chapter 7" class="md-nav__link">
      Chapter 7
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_8/" title="Chapter 8" class="md-nav__link">
      Chapter 8
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_9/" title="Chapter 9" class="md-nav__link">
      Chapter 9
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_10/" title="Chapter 10" class="md-nav__link">
      Chapter 10
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_11/" title="Chapter 11" class="md-nav__link">
      Chapter 11
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_12/" title="Chapter 12" class="md-nav__link">
      Chapter 12
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_13/" title="Chapter 13" class="md-nav__link">
      Chapter 13
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-2-2" type="checkbox" id="nav-2-2-2">
    
    <label class="md-nav__link" for="nav-2-2-2">
      Some Introduction
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-2-2-2">
        Some Introduction
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../MCTS_introduction/" title="MCTS" class="md-nav__link">
      MCTS
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Tips
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Tips
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../tips/" title="Tips" class="md-nav__link">
      Tips
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../tips/to-do/" title="To Do" class="md-nav__link">
      To Do
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../tips/python/" title="Python" class="md-nav__link">
      Python
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../tips/data-processing/" title="Data Processing" class="md-nav__link">
      Data Processing
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../tips/git/" title="Git" class="md-nav__link">
      Git
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../tips/linux/" title="Linux" class="md-nav__link">
      Linux
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../tips/win/" title="Windows" class="md-nav__link">
      Windows
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      Share
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        Share
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/" title="Index" class="md-nav__link">
      Index
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/roc-fly/" title="鹏程万里" class="md-nav__link">
      鹏程万里
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/blog-history/" title="博客历史" class="md-nav__link">
      博客历史
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/my-postgraduate-share/" title="保研推免经验分享 - 数学系跨保 CS" class="md-nav__link">
      保研推免经验分享 - 数学系跨保 CS
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/game-play/" title="Game-Experience" class="md-nav__link">
      Game-Experience
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-6" type="checkbox" id="nav-4-6">
    
    <label class="md-nav__link" for="nav-4-6">
      NKU-Toolkit
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-4-6">
        NKU-Toolkit
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/nku-eamis/" title="NKU-EAMIS工具" class="md-nav__link">
      NKU-EAMIS工具
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/nku-sms-rss/" title="NKU-SMS-RSS" class="md-nav__link">
      NKU-SMS-RSS
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/eamis-miniapp/" title="NKU-EAMIS_MiniApp(南开大学教务助手小程序)" class="md-nav__link">
      NKU-EAMIS_MiniApp(南开大学教务助手小程序)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/eamis-workflow/" title="NKU-EAMIS for iOS(Workflow)" class="md-nav__link">
      NKU-EAMIS for iOS(Workflow)
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-7" type="checkbox" id="nav-4-7">
    
    <label class="md-nav__link" for="nav-4-7">
      Steam-Toolkit
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-4-7">
        Steam-Toolkit
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/steam-market-price-bot/" title="Steam市场比价爬虫" class="md-nav__link">
      Steam市场比价爬虫
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-8" type="checkbox" id="nav-4-8">
    
    <label class="md-nav__link" for="nav-4-8">
      数学建模
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-4-8">
        数学建模
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/2017-mcm-icm/" title="2017美赛参赛整理(Problem D)" class="md-nav__link">
      2017美赛参赛整理(Problem D)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/2016-guosai/" title="2016数学建模国赛" class="md-nav__link">
      2016数学建模国赛
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/math-model-szb/" title="数学建模之2016深圳杯——初次尝试" class="md-nav__link">
      数学建模之2016深圳杯——初次尝试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/polygon-to-ellipse/" title="随机多边形转化为椭圆的过程研究" class="md-nav__link">
      随机多边形转化为椭圆的过程研究
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/FFT-GPU-Accel/" title="FFT-GPU-Accel" class="md-nav__link">
      FFT-GPU-Accel
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-9" type="checkbox" id="nav-4-9">
    
    <label class="md-nav__link" for="nav-4-9">
      NKU 数院试题整理
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-4-9">
        NKU 数院试题整理
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/nku-sms-exams/" title="汇总" class="md-nav__link">
      汇总
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-9-2" type="checkbox" id="nav-4-9-2">
    
    <label class="md-nav__link" for="nav-4-9-2">
      分析
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-4-9-2">
        分析
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/functional-analysis-final/" title="2017-2018第一学期泛函分析期末考试" class="md-nav__link">
      2017-2018第一学期泛函分析期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/real-variable-function/" title="2016-2017第二学期实变函数期末考试" class="md-nav__link">
      2016-2017第二学期实变函数期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/mathematical-analysis-3-3-final/" title="2016-2017第一学期数学分析3-3期末考试" class="md-nav__link">
      2016-2017第一学期数学分析3-3期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/complex-analysis-final/" title="2016-2017第一学期复变函数期末考试" class="md-nav__link">
      2016-2017第一学期复变函数期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/mathematical-analysis-3-3-middle/" title="2016-2017第一学期数学分析3-3期中考试" class="md-nav__link">
      2016-2017第一学期数学分析3-3期中考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/mathematical-analysis-3-2-final/" title="2015-2016第二学期数学分析3-2期末考试（含解答）" class="md-nav__link">
      2015-2016第二学期数学分析3-2期末考试（含解答）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/mathematical-analysis-3-2-middle/" title="2015-2016第二学期数学分析3-2期中考试" class="md-nav__link">
      2015-2016第二学期数学分析3-2期中考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/mathematical-analysis-3-1-final/" title="2015-2016第一学期数学分析3-1期末考试" class="md-nav__link">
      2015-2016第一学期数学分析3-1期末考试
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-9-3" type="checkbox" id="nav-4-9-3">
    
    <label class="md-nav__link" for="nav-4-9-3">
      代数
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-4-9-3">
        代数
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/abstract-algebra-final/" title="2016-2017第一学期抽象代数期末考试" class="md-nav__link">
      2016-2017第一学期抽象代数期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/abstract-algebra-middle/" title="2016-2017第一学期抽象代数期中考试" class="md-nav__link">
      2016-2017第一学期抽象代数期中考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/advanced-algebra-2-2-final/" title="2015-2016第二学期高等代数2-2期末考试" class="md-nav__link">
      2015-2016第二学期高等代数2-2期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/advanced-algebra-2-2-middle/" title="2015-2016第二学期高等代数2-2期中考试" class="md-nav__link">
      2015-2016第二学期高等代数2-2期中考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/advanced-algebra-2-1-final/" title="2015-2016第一学期高等代数2-1期末考试" class="md-nav__link">
      2015-2016第一学期高等代数2-1期末考试
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-9-4" type="checkbox" id="nav-4-9-4">
    
    <label class="md-nav__link" for="nav-4-9-4">
      概率统计
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-4-9-4">
        概率统计
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/probability-final/" title="2016-2017第二学期概率论期末考试" class="md-nav__link">
      2016-2017第二学期概率论期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/probability-middle/" title="2016-2017第二学期概率论期中考试" class="md-nav__link">
      2016-2017第二学期概率论期中考试
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-9-5" type="checkbox" id="nav-4-9-5">
    
    <label class="md-nav__link" for="nav-4-9-5">
      微分方程
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-4-9-5">
        微分方程
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/PDE-final/" title="2017-2018第一学期数理方程期末考试" class="md-nav__link">
      2017-2018第一学期数理方程期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/ODE-final/" title="2016-2017第一学期常微分方程期末考试" class="md-nav__link">
      2016-2017第一学期常微分方程期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/ODE-middle/" title="2016-2017第一学期常微分方程期中考试" class="md-nav__link">
      2016-2017第一学期常微分方程期中考试
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-10" type="checkbox" id="nav-4-10">
    
    <label class="md-nav__link" for="nav-4-10">
      Other
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-4-10">
        Other
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/github-student-pack/" title="Student Developer Pack - GitHub Education" class="md-nav__link">
      Student Developer Pack - GitHub Education
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      Statements
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        Statements
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../statements/" title="Statements" class="md-nav__link">
      Statements
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#31-the-agentenvironment-interface" title="3.1 The Agent–Environment Interface" class="md-nav__link">
    3.1 The Agent–Environment Interface
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-recycling-robot" title="Example: Recycling Robot" class="md-nav__link">
    Example: Recycling Robot
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#32-goals-and-rewards" title="3.2 Goals and Rewards" class="md-nav__link">
    3.2 Goals and Rewards
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#33-returns" title="3.3 Returns" class="md-nav__link">
    3.3 Returns
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#episodic-tasks" title="Episodic Tasks" class="md-nav__link">
    Episodic Tasks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#continuing-tasks" title="Continuing Tasks" class="md-nav__link">
    Continuing Tasks
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#34-unified-notation-for-episodic-and-continuing-tasks" title="3.4 Unified Notation for Episodic and Continuing Tasks" class="md-nav__link">
    3.4 Unified Notation for Episodic and Continuing Tasks
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#35-the-markov-property" title="3.5 The Markov Property" class="md-nav__link">
    3.5 The Markov Property
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#markov-property" title="Markov Property" class="md-nav__link">
    Markov Property
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#36-markov-decision-processes" title="3.6 Markov Decision Processes" class="md-nav__link">
    3.6 Markov Decision Processes
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-recycling-robot-mdp" title="Example: Recycling Robot MDP" class="md-nav__link">
    Example: Recycling Robot MDP
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#37-value-functions" title="3.7 Value Functions" class="md-nav__link">
    3.7 Value Functions
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#monte-carlo-methods" title="Monte Carlo Methods" class="md-nav__link">
    Monte Carlo Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bellman-equation" title="Bellman Equation" class="md-nav__link">
    Bellman Equation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-gridworld" title="Example: Gridworld" class="md-nav__link">
    Example: Gridworld
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#38-optimal-value-functions" title="3.8 Optimal Value Functions" class="md-nav__link">
    3.8 Optimal Value Functions
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bellman-optimality-equation" title="Bellman Optimality Equation" class="md-nav__link">
    Bellman Optimality Equation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-solving-the-gridworld" title="Example: Solving the Gridworld" class="md-nav__link">
    Example: Solving the Gridworld
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#39-optimality-and-approximation" title="3.9 Optimality and Approximation" class="md-nav__link">
    3.9 Optimality and Approximation
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/zawnpn/ZHANGWP/edit/master/docs/notes/reinforcement-learning/notes/RLAI_3.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="-">强化学习导论（三）- 有限马尔可夫决策过程<a class="headerlink" href="#-" title="Permanent link">&para;</a></h1>
<p>这章主要讲有限马尔可夫决策过程（finite MDPs）。</p>
<p>首先会涉及到上一章提到的评估反馈，但与之前不同的是，现在也会开始考虑问题与环境的联系，也就是在不同情景下去做不同的选择。MDPs 是决策序列的一种经典形式化模型，其中的行动不仅会影响当前的即时奖励值，也会通过未来的奖励值来影响后续的情况或状态。</p>
<h2 id="31-the-agentenvironment-interface">3.1 The Agent–Environment Interface<a class="headerlink" href="#31-the-agentenvironment-interface" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>agent</strong>: 学习者/决策者</li>
<li><strong>environment</strong>: agent 与外界进行交互的物体的组合</li>
</ul>
<p>Agent 和 environment 在一系列的时间点上进行交互：当时间为 <span><span class="MathJax_Preview">t(t = 0, 1, 2, 3, \ldots)</span><script type="math/tex">t(t = 0, 1, 2, 3, \ldots)</script></span>，agent 接收到环境的状态表示：<span><span class="MathJax_Preview">S_t \in \mathcal{S}</span><script type="math/tex">S_t \in \mathcal{S}</script></span> ，在此基础上选择了一个行动 <span><span class="MathJax_Preview">A_t \in \mathcal{A}(s)</span><script type="math/tex">A_t \in \mathcal{A}(s)</script></span> ，之后 agent 会得到作为行动反馈的数值奖励 <span><span class="MathJax_Preview">R_{t+1}\in\mathcal{R}\subset\mathbb{R}</span><script type="math/tex">R_{t+1}\in\mathcal{R}\subset\mathbb{R}</script></span>，并进入新状态 <span><span class="MathJax_Preview">S_{t+1}</span><script type="math/tex">S_{t+1}</script></span>。下图描绘了这样一个往复交替的过程。</p>
<p><img alt="agent-env" src="../imgs/RLAI_3/agent-env.png" /></p>
<p>关于如何划清 agent 和 environment 的界限，有一个通用的准则：<strong>不能被 agent 任意改变的事物，均认为其属于 environment</strong>。Agent 往往能够得知很多外界的信息，甚至能知道奖励值关于其行动和状态的具体函数，但我们还是得把奖励值的计算过程放在 agent 外界，而不能让他自己进行计算，这是因为，正是这个奖励机制定义了 agent 所要学习去处理的任务，所以这个任务必须是超出 agent 控制能力的，反之则本末倒置了，如果 agent 能自我计算/改变 reward ，那他会按照自己的一些「理解」去学习问题，而没有面对问题的「客观本质」去学习。</p>
<p>所以，agent 与 environment 的界限，代表着 <strong>学习者/决策体 的绝对控制权，而不是其掌握的 知识/信息</strong>。而一旦确定好三个关键因素「<strong>状态</strong>、<strong>行动</strong> 和 <strong>奖励</strong>」，就意味着这个界限已被确定下来。</p>
<p>任何<strong>对受目标引导的行为</strong>的学习问题，都可以简化为三个<strong>信号</strong>在 agent 和 environment 间前后传递的模型：</p>
<ul>
<li><strong>actions</strong>: agent 的决策行动</li>
<li><strong>states</strong>: 进行行动选择的基准</li>
<li><strong>rewards</strong>: agent 的目标</li>
</ul>
<h3 id="example-recycling-robot">Example: Recycling Robot<a class="headerlink" href="#example-recycling-robot" title="Permanent link">&para;</a></h3>
<p>前面的讲述可能有些抽象，这里举个实际的例子（清洁机器人）来看看 actions、states、rewards 在具体问题中是什么。这个机器人的主要目标是要持续清洁垃圾，但还需注意电量不被用完，所以在一些情况下得注意要回去充电，这就是问题背景，后面还会反复提到这个例子。</p>
<p><strong>Actions</strong>:</p>
<ul>
<li>活动起来去搜寻垃圾（耗电）</li>
<li>保持在原地等人来捡垃圾（不耗电）</li>
<li>回去充电</li>
</ul>
<p><strong>States</strong>:</p>
<ul>
<li>电池状态</li>
</ul>
<p><strong>Rewards</strong>:</p>
<ul>
<li>如果清洁了垃圾，反馈一个正值作为奖励</li>
<li>如果耗尽电量而没能及时返回充电，反馈一个较大负值作为惩罚</li>
<li>其余情况反馈 0 奖励值</li>
</ul>
<p>这只是简单举个例子，用来给大家看看实际情况下是如何去通过这三个信号来定义一个问题。</p>
<h2 id="32-goals-and-rewards">3.2 Goals and Rewards<a class="headerlink" href="#32-goals-and-rewards" title="Permanent link">&para;</a></h2>
<p>在强化学习中，agent 的目标通过 environment 传入的奖励信号来量化，具体而言，目标即为<strong>最大化奖励值的累积求和</strong>。换一种说法，不同的奖励值决定了不同的目标、不同的学习问题。</p>
<p>这里举个简单的例子作为对比：</p>
<ul>
<li>让机器人学会走路：提供与行走距离成正比的奖励值作为机器人的奖励值。</li>
<li>让机器人学会逃出迷宫：逃出迷宫前的每一步都提供 -1 作为「奖励值」。</li>
</ul>
<p>不难理解，如果提供正值作为奖励，则会不断「鼓励」机器人多走路，反之，则会「促使」机器人找到一条最短路径逃出迷宫。</p>
<p>前面说到，agent 的目标很清晰，就是最大化累积奖励值，而他的行动也很大程度受到我们给定的 rewards 的影响，不恰当地设定 rewards 必然会影响到 agent 的学习效果。所以，需要强调一点，我们要提供的 rewards ，是要用于<strong>促使</strong> agent 来<strong>达成目标</strong>，而不是根据人的先验知识来<strong>告诉他该怎样去做</strong>。说直白点，就是不要去人为「干预」他怎样学，只管给他设定目标和一些基本规则，让他多去自由探索。</p>
<p>举个例子，在下棋时，应该在真正赢下比赛时才给予适当奖励，而不是在吃掉对手某个棋子，亦或占据某个区域时就立即给他奖励。虽然按人的理解，吃掉棋子之类的行动一般都看似是个不错的选择，但在一些关键的步骤，说不定以退为进才是上策，盲目吃子反而陷入陷阱。所以如果按后者那样的策略去学习，agent 最终总会倾向于不顾输赢地去拿下这些眼前利益，而不去长远考虑，失去「大局观」。</p>
<p>之前强调过，rewards 的计算要放在 agent 外界，现在不难明白，原因就在于 agent 的终极目标是要去<strong>掌握之前并不能完美掌握的事物</strong>，而这正是我们设计强化学习来解决难题的本质。</p>
<h2 id="33-returns">3.3 Returns<a class="headerlink" href="#33-returns" title="Permanent link">&para;</a></h2>
<p>目前已经明确了强化学习的目标——最大化累积奖励，那么如何用数学语言来表述呢？这里我们引入一个返回值（Return）的概念，定义为<strong>奖励值序列的特定函数</strong>。定义 <span><span class="MathJax_Preview">G_t</span><script type="math/tex">G_t</script></span> 来表示时间点 t 之后的期望返回值，一种最简单的期望返回值为对未来的奖励值序列进行求和：</p>
<div>
<div class="MathJax_Preview">G_t\doteq R_{t+1}+R_{t+2}+R_{t+3}+\cdots+R_T</div>
<script type="math/tex; mode=display">G_t\doteq R_{t+1}+R_{t+2}+R_{t+3}+\cdots+R_T</script>
</div>
<h3 id="episodic-tasks">Episodic Tasks<a class="headerlink" href="#episodic-tasks" title="Permanent link">&para;</a></h3>
<p>上述的返回值表示形式只在有限时间点条件（或者说有终止时间点）下适用，我们将这类任务定义为<strong>片段式任务</strong>：</p>
<ul>
<li>episodes: 决策体与环境的交互过程的子片段（任意重复性的交互过程），称为 episodes</li>
<li>terminal state: 每个子片段都有一个特殊状态，其后续时间点被重置进入新片段，这样的特殊片段，称为 terminal state</li>
</ul>
<p>在片段式任务中时常需要区分终止态，所以符号上也要有所区分：</p>
<ul>
<li><span><span class="MathJax_Preview">\mathcal{S}</span><script type="math/tex">\mathcal{S}</script></span>: 非终止态的全体集合</li>
<li><span><span class="MathJax_Preview">\mathcal{S}^+</span><script type="math/tex">\mathcal{S}^+</script></span>: <span><span class="MathJax_Preview">\mathcal{S} \bigcup</span><script type="math/tex">\mathcal{S} \bigcup</script></span> {终止态全体}</li>
</ul>
<h3 id="continuing-tasks">Continuing Tasks<a class="headerlink" href="#continuing-tasks" title="Permanent link">&para;</a></h3>
<p>在很多时候，agent 与 environment 的交互过程并不能很自然地被分解为「片段」，而是无止限地持续下去，称之为连续式任务。此时上面的 <span><span class="MathJax_Preview">G_t</span><script type="math/tex">G_t</script></span> 会因 <span><span class="MathJax_Preview">T=\infty</span><script type="math/tex">T=\infty</script></span> 而必然趋于无穷，导致 agent 无法根据返回值来进行比较学习，此时需要加入「削减系数」<span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> 。</p>
<p><strong>削减返回值</strong>:</p>
<div>
<div class="MathJax_Preview">G_t\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\cdots = \sum_{k=0}^{\infty}\gamma^kR_{t+k+1}</div>
<script type="math/tex; mode=display">G_t\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\cdots = \sum_{k=0}^{\infty}\gamma^kR_{t+k+1}</script>
</div>
<p>其中 <span><span class="MathJax_Preview">\gamma(0\leq \gamma \leq 1)</span><script type="math/tex">\gamma(0\leq \gamma \leq 1)</script></span> 作为削减率，决定了未来奖励值在当前的表现：</p>
<ul>
<li>若 <span><span class="MathJax_Preview">\gamma \rightarrow 0</span><script type="math/tex">\gamma \rightarrow 0</script></span>，agent 更着重于最大化即时奖励值，也就是说，此时他的目标为学习如何选择 <span><span class="MathJax_Preview">A_t</span><script type="math/tex">A_t</script></span> 来最大化 <span><span class="MathJax_Preview">R_{t+1}</span><script type="math/tex">R_{t+1}</script></span>.</li>
<li>若 <span><span class="MathJax_Preview">\gamma \rightarrow 1</span><script type="math/tex">\gamma \rightarrow 1</script></span>，返回值会更加强烈地把未来的奖励情况考虑进来，使得 agent 变得更有「远见」。</li>
</ul>
<h2 id="34-unified-notation-for-episodic-and-continuing-tasks">3.4 Unified Notation for Episodic and Continuing Tasks<a class="headerlink" href="#34-unified-notation-for-episodic-and-continuing-tasks" title="Permanent link">&para;</a></h2>
<p>在后面的讨论中，我们会对各种任务来统一地来讨论分析，所以需要统一符号。</p>
<p>在片段式任务中，我们应该用 <span><span class="MathJax_Preview">S_{t,i}</span><script type="math/tex">S_{t,i}</script></span> 来表示第 i 段的第 t 步（ <span><span class="MathJax_Preview">A_{t,i}, R_{t,i}, \pi_{t,i}, T_i</span><script type="math/tex">A_{t,i}, R_{t,i}, \pi_{t,i}, T_i</script></span> 等同理），但是每一段其实本质上意义相近，对于其他段我们可以统一分析，故简写 <span><span class="MathJax_Preview">S_t</span><script type="math/tex">S_t</script></span> 来统一表示各个 <span><span class="MathJax_Preview">S_{t,i}</span><script type="math/tex">S_{t,i}</script></span>，其他几个符号也同理。</p>
<p>此外，我们还需要统一片段式任务和连续式任务的形式，他们分布有着有限项的返回值公式和无限项的返回值公式，为了统一公式，我们针对片段式任务加入一种特殊的「吸收态」，其特点是状态的交互和转移过程都只在自身进行，且奖励值为 0 ，如下图所示：</p>
<p><img alt="Absorbing State" src="../imgs/RLAI_3/unified.png" /></p>
<p>这样显然可见，片段式任务也能表示为无穷项了，只不过原本的终止态之后 reward 为 0，不对公式造成影响。于是我们可以统一地定义：</p>
<div>
<div class="MathJax_Preview">G_t\doteq \sum_{k=t+1}^{T}\gamma^{k-t-1}R_k</div>
<script type="math/tex; mode=display">G_t\doteq \sum_{k=t+1}^{T}\gamma^{k-t-1}R_k</script>
</div>
<h2 id="35-the-markov-property">3.5 The Markov Property<a class="headerlink" href="#35-the-markov-property" title="Permanent link">&para;</a></h2>
<p>回想我们进行强化学习的主要目标在于，学习出一个能由任意状态信号决定行动的策略函数，而状态是以<strong>即时感知以及历史状态和信息</strong>为基础，逐渐<strong>构筑、维护</strong>下来的，他包含一定的历史信息，但并不意味着能从中得知环境中的一切信息。比如棋盘上的棋子，棋子在不同的位置即在不同的状态，从每个状态，我们能知道走下这步棋各种可能的组合，但并不能准确得知他是如何一步一步走到这一步的。</p>
<p>正如上面的棋子的例子一样，我们称，如果一个状态继承并保留了所有相关信息，则具有<strong>马尔可夫性质</strong>。下面在用数学语言描述一下这一性质。</p>
<h3 id="markov-property">Markov Property<a class="headerlink" href="#markov-property" title="Permanent link">&para;</a></h3>
<p>首先定义基于全部历史信息的完全联合分布：</p>
<div>
<div class="MathJax_Preview">\mathrm{Pr}\{S_{t+1}=s',R_{t+1}=r|S_0,A_0,R_1,\ldots,S_{t-1},A_{t-1},R_t,S_t,A_t\}</div>
<script type="math/tex; mode=display">\mathrm{Pr}\{S_{t+1}=s',R_{t+1}=r|S_0,A_0,R_1,\ldots,S_{t-1},A_{t-1},R_t,S_t,A_t\}</script>
</div>
<p>环境的动态分布定义为：</p>
<div>
<div class="MathJax_Preview">p(s',r|,s,a)\doteq \mathrm{Pr}\{S_{t+1}=s',R_{t+1}=r|S_t=s,A_t=a\}</div>
<script type="math/tex; mode=display">p(s',r|,s,a)\doteq \mathrm{Pr}\{S_{t+1}=s',R_{t+1}=r|S_t=s,A_t=a\}</script>
</div>
<p>当且仅当上面两式相等，即</p>
<div>
<div class="MathJax_Preview">p(s',r|s,a) = \mathrm{Pr}\{S_{t+1}=s',R_{t+1}=r|S_0,A_0,R_1,\ldots,S_t,A_t\}</div>
<script type="math/tex; mode=display">p(s',r|s,a) = \mathrm{Pr}\{S_{t+1}=s',R_{t+1}=r|S_0,A_0,R_1,\ldots,S_t,A_t\}</script>
</div>
<p>我们称这样的环境和任务具有<strong>马尔可夫性质</strong>。</p>
<p>马尔科夫性质在强化学习中的重要之处体现在，如果问题具备这一条件，那么各种决策值将只与当前状态有关，这能极大程度地方便我们分析各种理论和模型。即使是非严格遵守马尔可夫性质的问题，也能有所应用。</p>
<p><em>这本书之后的所有理论都将基于马尔可夫性质来讨论。</em></p>
<h2 id="36-markov-decision-processes">3.6 Markov Decision Processes<a class="headerlink" href="#36-markov-decision-processes" title="Permanent link">&para;</a></h2>
<p>如果一个强化学习任务满足马尔可夫性质，我们称之为<strong>马尔可夫决策过程（MDP）</strong>。如果状态空间和行动空间都是有限的，则称为<strong>有限马尔可夫过程（finite MDP）</strong>。</p>
<p>在有限马尔可夫过程中，状态集、行动集、奖励集（<span><span class="MathJax_Preview">\mathcal{S}, \mathcal{A}, \mathcal{R}</span><script type="math/tex">\mathcal{S}, \mathcal{A}, \mathcal{R}</script></span>）内的元素均有上界，所以能定义</p>
<div>
<div class="MathJax_Preview">p(s',r|s,a)\doteq\mathrm{Pr}\left\{S_t=s',R_t=r|S_{t-1}=s,A_{t-1}=a\right\}</div>
<script type="math/tex; mode=display">p(s',r|s,a)\doteq\mathrm{Pr}\left\{S_t=s',R_t=r|S_{t-1}=s,A_{t-1}=a\right\}</script>
</div>
<p>其中</p>
<div>
<div class="MathJax_Preview">\sum_{s'\in\mathcal{S}}\sum_{r\in\mathcal{R}}p(s',r|s,a)=1</div>
<script type="math/tex; mode=display">\sum_{s'\in\mathcal{S}}\sum_{r\in\mathcal{R}}p(s',r|s,a)=1</script>
</div>
<p>有了 <span><span class="MathJax_Preview">p(s',r|s,a)</span><script type="math/tex">p(s',r|s,a)</script></span> ，我们可以进一步计算得到：</p>
<ul>
<li><strong>状态转移概率</strong>:</li>
</ul>
<div>
<div class="MathJax_Preview">p(s'|s,a)\doteq\mathrm{Pr}\left\{S_t=s'|S_{t-1}=s,A_{t-1}=a\right\}=\sum_{r\in\mathcal{R}}p(s',r|s,a)</div>
<script type="math/tex; mode=display">p(s'|s,a)\doteq\mathrm{Pr}\left\{S_t=s'|S_{t-1}=s,A_{t-1}=a\right\}=\sum_{r\in\mathcal{R}}p(s',r|s,a)</script>
</div>
<ul>
<li><strong>给定状态、行动的期望奖励</strong>:</li>
</ul>
<div>
<div class="MathJax_Preview">r(s,a)\doteq \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a\right]=\sum_{r\in\mathcal{R}}r\sum_{s'\in\mathcal{S}}p(s',r|s,a)</div>
<script type="math/tex; mode=display">r(s,a)\doteq \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a\right]=\sum_{r\in\mathcal{R}}r\sum_{s'\in\mathcal{S}}p(s',r|s,a)</script>
</div>
<ul>
<li><strong>给定状态、行动、后继状态的期望奖励</strong>:</li>
</ul>
<div>
<div class="MathJax_Preview">r(s,a,s')\doteq \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a,S_t=s'\right]=\sum_{r\in\mathcal{R}}r\frac{p(s',r|s,a)}{p(s'|s,a)}</div>
<script type="math/tex; mode=display">r(s,a,s')\doteq \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a,S_t=s'\right]=\sum_{r\in\mathcal{R}}r\frac{p(s',r|s,a)}{p(s'|s,a)}</script>
</div>
<p>得到这些动态性质后，有助于我们后面的具体分析。</p>
<h3 id="example-recycling-robot-mdp">Example: Recycling Robot MDP<a class="headerlink" href="#example-recycling-robot-mdp" title="Permanent link">&para;</a></h3>
<p>前面提到的清洁机器人，便能按照有限马尔科夫决策过程来分析。下面的图表详细地描述了怎样用 MDP 来分析问题。比如第三、四行，表示机器人在低电量下仍去搜索垃圾，有 <span><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> 的概率仍然是低电量，收获 <span><span class="MathJax_Preview">r_{search}</span><script type="math/tex">r_{search}</script></span> 的奖励值，有 <span><span class="MathJax_Preview">\beta​</span><script type="math/tex">\beta​</script></span> 的概率耗尽电量未能及时自行返回充电，被人工带回充电后进入高电量状态，同时给予 -3 的惩罚值。其他的过程也可类似分析，可以看出在分析接下来情景时，我们无需考虑机器人过去所有做过的事情，只需要从当前状态出发，分析进入下一个可能状态的过程即可。</p>
<p><img alt="robot_tb" src="../imgs/RLAI_3/robot_tb.png" /></p>
<p><img alt="robot_diag" src="../imgs/RLAI_3/robot_diag.png" /></p>
<h2 id="37-value-functions">3.7 Value Functions<a class="headerlink" href="#37-value-functions" title="Permanent link">&para;</a></h2>
<p>强化学习算法总是涉及到估计 value function，用于量化评估不同条件下行动的好坏程度。这里我们可以对返回值求期望来作为 value function 来进行评估，而显然 agent 的策略决定了如何来计算这一期望。</p>
<p>回想我们反复说到的策略这一概念，简单来说，策略是由状态空间 <span><span class="MathJax_Preview">\mathcal{S}</span><script type="math/tex">\mathcal{S}</script></span>、行动空间 <span><span class="MathJax_Preview">\mathcal{A}</span><script type="math/tex">\mathcal{A}</script></span> 到概率空间的一个映射：<span><span class="MathJax_Preview">\pi:\mathcal{S}\times\mathcal{A}\to \mathcal{P}</span><script type="math/tex">\pi:\mathcal{S}\times\mathcal{A}\to \mathcal{P}</script></span> 。</p>
<ul>
<li><span><span class="MathJax_Preview">\pi(a|s)</span><script type="math/tex">\pi(a|s)</script></span>: 状态为 s 时选择行动 a 的概率</li>
<li><span><span class="MathJax_Preview">v_{\pi}(s)</span><script type="math/tex">v_{\pi}(s)</script></span>: <strong>策略 <span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span> 下的状态值函数</strong>。表示状态 s 下，遵守策略 <span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span> 的期望返回值。对于 MDP，我们可以定义</li>
</ul>
<div>
<div class="MathJax_Preview">v_{\pi}(s)\doteq\mathbb{E}_{\pi}\left[G_t|S_t=s\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}\mid S_t=s\right], \forall s \in \mathcal{S}</div>
<script type="math/tex; mode=display">v_{\pi}(s)\doteq\mathbb{E}_{\pi}\left[G_t|S_t=s\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}\mid S_t=s\right], \forall s \in \mathcal{S}</script>
</div>
<ul>
<li><span><span class="MathJax_Preview">q_{\pi}(s,a)</span><script type="math/tex">q_{\pi}(s,a)</script></span>: <strong>策略 <span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span> 下的行动值函数</strong>。表示状态 s 下，选择行动 a ，遵守策略 <span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span> 的期望返回值。类似地，可以定义</li>
</ul>
<div>
<div class="MathJax_Preview">q_{\pi}(s,a)\doteq\mathbb{E}_{\pi}\left[G_t|S_t=s,A_t=a\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}\mid S_t=s,A_t=a\right]</div>
<script type="math/tex; mode=display">q_{\pi}(s,a)\doteq\mathbb{E}_{\pi}\left[G_t|S_t=s,A_t=a\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}\mid S_t=s,A_t=a\right]</script>
</div>
<h3 id="monte-carlo-methods"><strong>Monte Carlo Methods</strong><a class="headerlink" href="#monte-carlo-methods" title="Permanent link">&para;</a></h3>
<p>上面的函数可以通过模拟方法来进行经验估计，称为蒙特卡洛方法。</p>
<ul>
<li>在实验中，若 agent 在遵守策略 <span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span> 的条件下，为每个出发状态 s 都保留了后面的真实返回值的均值，显然这个均值会随实验次数增加而最终收敛到 <span><span class="MathJax_Preview">v_{\pi}(s)</span><script type="math/tex">v_{\pi}(s)</script></span>.</li>
<li>类似地，如果还细分到为每个出发状态 s、每个行动选择 a 都保留了真实返回值的均值，则便能最终收敛到 <span><span class="MathJax_Preview">q_{\pi}(s,a)</span><script type="math/tex">q_{\pi}(s,a)</script></span>.</li>
</ul>
<p>如果状态空间很大，显然可知，进行上述的大量样本实验来模拟估计很不现实。此时可以考虑将 <span><span class="MathJax_Preview">v_{\pi}, q_{\pi}</span><script type="math/tex">v_{\pi}, q_{\pi}</script></span> 看作参数化函数，通过调参来逼近真实值，一样能够较为精确地进行估计（第五章讲）。</p>
<h3 id="bellman-equation">Bellman Equation<a class="headerlink" href="#bellman-equation" title="Permanent link">&para;</a></h3>
<p>Value function 被用在强化学习和动态规划的一个基本性质在于其满足一种特殊递归关系：</p>
<div>
<div class="MathJax_Preview">\begin{aligned} v_\pi(s) &amp;\doteq \mathbb{E}_\pi[G_t\,|\,S_t=s] \\ &amp;=\mathbb{E}\pi \left[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\,|\,S_t=s \right] \\ &amp;= \mathbb{E}_\pi \left [R_{t+1}+\gamma\sum_{k=0}^\infty\gamma^kR_{t+k+2}\,|\,S_t=s \right] \\ &amp;=\sum_a\pi(a|s)\sum_{s'}\sum_rp(s',r|s,a)\left[r+\gamma \mathbb{E}_\pi \left[\sum_{k=0}^\infty\gamma^kR_{t+k+2}\,|\,S_{t+1}=s' \right] \right] \\ &amp;= \sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)\left[r+\gamma v_\pi(s') \right], \qquad \forall s \in \mathcal S \end{aligned}</div>
<script type="math/tex; mode=display">\begin{aligned} v_\pi(s) &\doteq \mathbb{E}_\pi[G_t\,|\,S_t=s] \\ &=\mathbb{E}\pi \left[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\,|\,S_t=s \right] \\ &= \mathbb{E}_\pi \left [R_{t+1}+\gamma\sum_{k=0}^\infty\gamma^kR_{t+k+2}\,|\,S_t=s \right] \\ &=\sum_a\pi(a|s)\sum_{s'}\sum_rp(s',r|s,a)\left[r+\gamma \mathbb{E}_\pi \left[\sum_{k=0}^\infty\gamma^kR_{t+k+2}\,|\,S_{t+1}=s' \right] \right] \\ &= \sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)\left[r+\gamma v_\pi(s') \right], \qquad \forall s \in \mathcal S \end{aligned}</script>
</div>
<ul>
<li><span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span>: 一个行动，取自行动集 <span><span class="MathJax_Preview">\mathcal{A}(s)</span><script type="math/tex">\mathcal{A}(s)</script></span></li>
<li><span><span class="MathJax_Preview">s'</span><script type="math/tex">s'</script></span>: 后继状态，取自状态集 <span><span class="MathJax_Preview">\mathcal{S}</span><script type="math/tex">\mathcal{S}</script></span> （对于片段式任务，取自 <span><span class="MathJax_Preview">\mathcal{S}^+</span><script type="math/tex">\mathcal{S}^+</script></span> ）</li>
<li><span><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span>: 奖励值，取自奖励集 <span><span class="MathJax_Preview">\mathcal{R}</span><script type="math/tex">\mathcal{R}</script></span></li>
</ul>
<p>对于任意策略 <span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span> 和状态 s ，当前 value 和未来可能的 value 满足上面的递推关系，称为<strong>贝尔曼方程（Bellman Equation）</strong>。</p>
<p><strong><span><span class="MathJax_Preview">v_\pi</span><script type="math/tex">v_\pi</script></span> 的贝尔曼方程：</strong></p>
<blockquote>
<div>
<div class="MathJax_Preview">v_{\pi}(s) = \sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)\left[r+\gamma v_\pi(s') \right]</div>
<script type="math/tex; mode=display">v_{\pi}(s) = \sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)\left[r+\gamma v_\pi(s') \right]</script>
</div>
</blockquote>
<p>贝尔曼方程描述了<strong>状态值与所有后继状态值的关系</strong>。</p>
<p><img alt="back-diag-v" src="../imgs/RLAI_3/back-diag-v.png" /></p>
<p>如上面这样的示意图，我们称为「<strong>backup diagrams</strong>」，它能描绘出问题的更新反馈机制，这也正是强化学习算法的一个核心之处。</p>
<h3 id="example-gridworld">Example: Gridworld<a class="headerlink" href="#example-gridworld" title="Permanent link">&para;</a></h3>
<p><img alt="grid-world" src="../imgs/RLAI_3/grid-world.png" /></p>
<p>在这个例子中，每一格上有四个等概率行动（往四个方向移动）。若从靠边的格子往界外移动，则会给予 -1 的惩罚，但位置保持不变，若移入特殊点 <span><span class="MathJax_Preview">A,B</span><script type="math/tex">A,B</script></span> ，则分别被移入 <span><span class="MathJax_Preview">A',B'</span><script type="math/tex">A',B'</script></span> 并给予 +10、+5 的特殊奖励，其余情况的奖励为 0 。</p>
<p>右图则是通过解贝尔曼线性方程组求出了 <span><span class="MathJax_Preview">v_{\pi}</span><script type="math/tex">v_{\pi}</script></span> 的解并填写在格子上（每个格子分别对应一个状态 <span><span class="MathJax_Preview">s_i</span><script type="math/tex">s_i</script></span>，填写的值则为 <span><span class="MathJax_Preview">v_{\pi}(s_i)</span><script type="math/tex">v_{\pi}(s_i)</script></span> ）。本例中，<span><span class="MathJax_Preview">\gamma = 0.9</span><script type="math/tex">\gamma = 0.9</script></span>。</p>
<h2 id="38-optimal-value-functions">3.8 Optimal Value Functions<a class="headerlink" href="#38-optimal-value-functions" title="Permanent link">&para;</a></h2>
<p>解决强化学习问题就是在于找到一个最优的测量，使 agent 能够按照该策略行动并得到最好的累积奖励值，在 MDP 中，当且仅当一个策略 <span><span class="MathJax_Preview">\pi​</span><script type="math/tex">\pi​</script></span> 在任意状态下的期望返回值都大于等于策略 <span><span class="MathJax_Preview">\pi'​</span><script type="math/tex">\pi'​</script></span> 的期望返回值，称策略 <span><span class="MathJax_Preview">\pi​</span><script type="math/tex">\pi​</script></span> 优于策略 <span><span class="MathJax_Preview">\pi'​</span><script type="math/tex">\pi'​</script></span> ，即： <span><span class="MathJax_Preview">\pi \geq \pi'​</span><script type="math/tex">\pi \geq \pi'​</script></span> 当且仅当 <span><span class="MathJax_Preview">v_{\pi}(s)\geq v_{\pi'}(s), \forall s \in \mathcal{S}​</span><script type="math/tex">v_{\pi}(s)\geq v_{\pi'}(s), \forall s \in \mathcal{S}​</script></span>.</p>
<ul>
<li><strong>最优策略</strong>：至少存在一个策略，优于其他所有策略，称所有这样的策略为最优策略</li>
<li><strong>最优状态值函数</strong>：<span><span class="MathJax_Preview">v_*(s)\doteq \max\limits_{\pi}v_{\pi}(s),\forall s \in \mathcal{S}</span><script type="math/tex">v_*(s)\doteq \max\limits_{\pi}v_{\pi}(s),\forall s \in \mathcal{S}</script></span></li>
<li><strong>最优行动值函数</strong>：<span><span class="MathJax_Preview">q_*(s,a)=\max\limits_{\pi}q_{\pi}(s,a),\forall s \in \mathcal{S}</span><script type="math/tex">q_*(s,a)=\max\limits_{\pi}q_{\pi}(s,a),\forall s \in \mathcal{S}</script></span></li>
</ul>
<p>其中易知，<span><span class="MathJax_Preview">q_*(s,a)=\mathbb{E}\left[R_{t+1}+\gamma v_*(S_{t+1})\mid S_t=s,A_t=a\right]</span><script type="math/tex">q_*(s,a)=\mathbb{E}\left[R_{t+1}+\gamma v_*(S_{t+1})\mid S_t=s,A_t=a\right]</script></span></p>
<h3 id="bellman-optimality-equation">Bellman Optimality Equation<a class="headerlink" href="#bellman-optimality-equation" title="Permanent link">&para;</a></h3>
<p><span><span class="MathJax_Preview">v_*</span><script type="math/tex">v_*</script></span> 作为 value function，也有贝尔曼方程，此时称该方程为贝尔曼最优方程。</p>
<p>贝尔曼最优方程表述了<strong>最优策略下一个状态的状态值，必然等于该状态下最优行动的期望返回值</strong>：</p>
<div>
<div class="MathJax_Preview">\begin{aligned}v_*(s) &amp;= \max_{a\in \mathcal A(s)}q_{\pi_*}(s,a) \\ &amp;=\max_a \mathbb{E}_{\pi_*} \left[G_t \mid S_t=s, A_t=a \right] \\ &amp;= \max_a \mathbb{E}_{\pi_*} \left[\sum_{k=0}^\infty \gamma^kR_{t+k+1} \mid S_t=s,A_t=a \right] \\ &amp;= \max_a \mathbb{E}_{\pi_*} \left[R_{t+1}+\gamma \sum_{k=0}^\infty\gamma^kR_{t+k+2}\mid S_t=s,A_t=a \right] \\ &amp;= \max_a \mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1}) \mid S_t=s,A_t=a] \\ &amp;=\max_{a}\sum_{s',r}p(s',r \mid s,a)[r+\gamma v_*(s')]\end{aligned}</div>
<script type="math/tex; mode=display">\begin{aligned}v_*(s) &= \max_{a\in \mathcal A(s)}q_{\pi_*}(s,a) \\ &=\max_a \mathbb{E}_{\pi_*} \left[G_t \mid S_t=s, A_t=a \right] \\ &= \max_a \mathbb{E}_{\pi_*} \left[\sum_{k=0}^\infty \gamma^kR_{t+k+1} \mid S_t=s,A_t=a \right] \\ &= \max_a \mathbb{E}_{\pi_*} \left[R_{t+1}+\gamma \sum_{k=0}^\infty\gamma^kR_{t+k+2}\mid S_t=s,A_t=a \right] \\ &= \max_a \mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1}) \mid S_t=s,A_t=a] \\ &=\max_{a}\sum_{s',r}p(s',r \mid s,a)[r+\gamma v_*(s')]\end{aligned}</script>
</div>
<p><strong>贝尔曼最优方程</strong>:</p>
<ul>
<li><span><span class="MathJax_Preview">v_*</span><script type="math/tex">v_*</script></span>:</li>
</ul>
<blockquote>
<div>
<div class="MathJax_Preview">\begin{aligned} v_*(s)&amp;= \max_a \mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1}) \mid S_t=s,A_t=a]\\&amp;=\max_{a}\sum_{s',r}p(s',r \mid s,a)[r+\gamma v_*(s')]\end{aligned}</div>
<script type="math/tex; mode=display">\begin{aligned} v_*(s)&= \max_a \mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1}) \mid S_t=s,A_t=a]\\&=\max_{a}\sum_{s',r}p(s',r \mid s,a)[r+\gamma v_*(s')]\end{aligned}</script>
</div>
</blockquote>
<ul>
<li><span><span class="MathJax_Preview">q_*</span><script type="math/tex">q_*</script></span>:</li>
</ul>
<blockquote>
<div>
<div class="MathJax_Preview">\begin{aligned}q_*(s,a) &amp;= \mathbb{E} \left[R_{t+1}+\gamma \max_{a'}q_*(S_{t+1},a') \mid S_t=s,A_t=a \right] \\ &amp;= \sum_{s',r}p(s',r \mid s,a)\left[r+\gamma \max_{a'}q_*(s',a') \right]\end{aligned}</div>
<script type="math/tex; mode=display">\begin{aligned}q_*(s,a) &= \mathbb{E} \left[R_{t+1}+\gamma \max_{a'}q_*(S_{t+1},a') \mid S_t=s,A_t=a \right] \\ &= \sum_{s',r}p(s',r \mid s,a)\left[r+\gamma \max_{a'}q_*(s',a') \right]\end{aligned}</script>
</div>
</blockquote>
<p><img alt="opt-diag" src="../imgs/RLAI_3/opt-diag.png" /></p>
<p>在上面的 backup diagrams 中，我们用一个圆弧来表示在不同的选项中去最大值。</p>
<p>对于有限 MDP ，贝尔曼最优方程一定有独立于策略的唯一解。事实上，设有 n 个状态，每个状态对应一个方程，这样总共由 n 个不同的方程形成 n 元方程组，如果环境的动态 <span><span class="MathJax_Preview">p(s',r|s,a)</span><script type="math/tex">p(s',r|s,a)</script></span> 给定，由数学知识可知，该方程组必然能得到唯一解 <span><span class="MathJax_Preview">v_*</span><script type="math/tex">v_*</script></span> ，进而又可利用其计算得到 <span><span class="MathJax_Preview">q_*</span><script type="math/tex">q_*</script></span> 。</p>
<p>一旦求得这一解，便能确定最优策略：</p>
<blockquote>
<p><strong>最优策略</strong>：显然至少存在一个行动能使行动值取到 <span><span class="MathJax_Preview">v_*</span><script type="math/tex">v_*</script></span> ，如果一个策略只将非 0 概率分配给这样的行动，称这个策略是最优策略。</p>
</blockquote>
<p>如果每一步都采取<strong>贪心策略</strong>，即只根据 <span><span class="MathJax_Preview">v_*</span><script type="math/tex">v_*</script></span> 来确定下一步的行动，这样的行动却恰好是最优行动，<span><span class="MathJax_Preview">v_*</span><script type="math/tex">v_*</script></span> 的优美之处便体现于此。之所以能达到这一效果，是因为 <span><span class="MathJax_Preview">v_*</span><script type="math/tex">v_*</script></span> 已经考虑到了未来所有可能性，于是，看似贪心的「一步搜索」却能生成出全局最优行动。</p>
<p>如果我们进一步解得了 <span><span class="MathJax_Preview">q_*</span><script type="math/tex">q_*</script></span> ，agent 甚至都无需来做「一步搜索」：对于任意状态 s ，只需找到 <span><span class="MathJax_Preview">a_0</span><script type="math/tex">a_0</script></span> 使得 <span><span class="MathJax_Preview">q_*(s,a_0)=\max\limits_{a}q_*(s,a)</span><script type="math/tex">q_*(s,a_0)=\max\limits_{a}q_*(s,a)</script></span> 即可。这是因为我们已经在之前的工作中多做了一些准备，将进一步的搜索信息缓存在了各个 <span><span class="MathJax_Preview">q_*</span><script type="math/tex">q_*</script></span> 中，使得它的信息量比 <span><span class="MathJax_Preview">v_*</span><script type="math/tex">v_*</script></span> 更大。</p>
<h3 id="example-solving-the-gridworld">Example: Solving the Gridworld<a class="headerlink" href="#example-solving-the-gridworld" title="Permanent link">&para;</a></h3>
<p><img alt="grid-world" src="../imgs/RLAI_3/grid-world_solve.png" /></p>
<ul>
<li>中图：最优状态值函数</li>
<li>右图：最优策略</li>
</ul>
<p>显式求解贝尔曼最优方程虽然能直接确定最优策略，但是并不太实用，因为这一方法涉及到了「穷举」，依赖严苛的计算资源和内存条件。此外，这个方法还依赖于三个假设，使得问题本身就很难利用上这一方法：</p>
<ol>
<li>精确地知道环境的动态性质 <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span></li>
<li>有足够的计算资源</li>
<li>满足马尔可夫性质</li>
</ol>
<p>在后面的章节中会介绍一些算法，如「启发式搜索」、「动态规划」等，可以看作是对「解贝尔曼方程」的近似求解，能够弥补上述的一些不足之处。</p>
<h2 id="39-optimality-and-approximation">3.9 Optimality and Approximation<a class="headerlink" href="#39-optimality-and-approximation" title="Permanent link">&para;</a></h2>
<p>前面已经讲过，最优策略是通过耗费极端的计算资源求得的，这使得我们不得不考虑一些方法来近似估计前面的一些函数。</p>
<p>在近似求解最优行为时，我们不难想象，会有很多状态其实只会以极低概率出现，我们若仍对其求解最优行动则意义不大，这时候如果选取一个局部最优行动来代替最优行动，显然，从整体期望意义来看，对总体奖励值造成的影响其实并不大，但却能节省很多的计算资源。与之对应的，当遇到那些经常出现的状态，我们则务必求出最优解。这是在解决 MDP 时强化学习方法区别于其他近似方法的一个重要性质。</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../RLAI_2/" title="Chapter 2" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Chapter 2
              </span>
            </div>
          </a>
        
        
          <a href="../RLAI_4/" title="Chapter 4" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Chapter 4
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2016-2020 ZHANGWP
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
  <div class="md-footer-social">
    <link rel="stylesheet" href="../../../../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/zawnpn" class="md-footer-social__link fa fa-github"></a>
    
      <a href="https://twitter.com/zawnpn" class="md-footer-social__link fa fa-twitter"></a>
    
      <a href="https://psnprofiles.com/zawnpn" class="md-footer-social__link fa fa-trophy"></a>
    
      <a href="https://steamcommunity.com/id/zawnpn/" class="md-footer-social__link fa fa-steam"></a>
    
      <a href="https://www.zhihu.com/people/zhangwanpeng" class="md-footer-social__link fa fa-globe"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../../assets/javascripts/application.583bbe55.js"></script>
      
        
        
          
          <script src="../../../../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
                <script src="../../../../assets/javascripts/lunr/tinyseg.js"></script>
              
              
                <script src="../../../../assets/javascripts/lunr/lunr.jp.js"></script>
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:"../../../.."}})</script>
      
        <script src="../../../../assets/extra.js"></script>
      
        <script src="//cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_SVG"></script>
      
    
    
      
    
  </body>
</html>