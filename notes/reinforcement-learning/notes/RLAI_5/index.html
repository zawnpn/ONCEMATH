<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Welcome to zhangwp's blog."><link href=https://www.zhangwp.com/notes/reinforcement-learning/notes/RLAI_5/ rel=canonical><meta name=author content=zawnpn><link rel="shortcut icon" href=../../../../assets/images/favicon.svg><meta name=generator content="mkdocs-1.1.2, mkdocs-material-5.4.0"><title>Chapter 5 - ZHANGWP</title><link rel=stylesheet href=../../../../assets/stylesheets/main.fe0cca5b.min.css><link rel=stylesheet href=../../../../assets/stylesheets/palette.a46bcfb3.min.css><meta name=theme-color content=#546e7a></head> <body dir=ltr data-md-color-scheme data-md-color-primary=blue-grey data-md-color-accent> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#- class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid" aria-label=Header> <a href=https://www.zhangwp.com title=ZHANGWP class="md-header-nav__button md-logo" aria-label=ZHANGWP> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><path d="M257.981 272.971L63.638 467.314c-9.373 9.373-24.569 9.373-33.941 0L7.029 444.647c-9.357-9.357-9.375-24.522-.04-33.901L161.011 256 6.99 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L257.981 239.03c9.373 9.372 9.373 24.568 0 33.941zM640 456v-32c0-13.255-10.745-24-24-24H312c-13.255 0-24 10.745-24 24v32c0 13.255 10.745 24 24 24h304c13.255 0 24-10.745 24-24z"/></svg> </a> <label class="md-header-nav__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header-nav__title data-md-component=header-title> <div class=md-header-nav__ellipsis> <span class="md-header-nav__topic md-ellipsis"> ZHANGWP </span> <span class="md-header-nav__topic md-ellipsis"> Chapter 5 </span> </div> </div> <label class="md-header-nav__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query data-md-state=active> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <button type=reset class="md-search__icon md-icon" aria-label=Clear data-md-component=search-reset tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header-nav__source> <a href=https://github.com/zawnpn/ZHANGWP/ title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class="md-tabs md-tabs--active" aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../ class="md-tabs__link md-tabs__link--active"> Notes </a> </li> <li class=md-tabs__item> <a href=../../../../tips/ class=md-tabs__link> Tips </a> </li> <li class=md-tabs__item> <a href=../../../../share/ class=md-tabs__link> Share </a> </li> <li class=md-tabs__item> <a href=../../../../statements/ class=md-tabs__link> Statements </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=https://www.zhangwp.com title=ZHANGWP class="md-nav__button md-logo" aria-label=ZHANGWP> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><path d="M257.981 272.971L63.638 467.314c-9.373 9.373-24.569 9.373-33.941 0L7.029 444.647c-9.357-9.357-9.375-24.522-.04-33.901L161.011 256 6.99 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L257.981 239.03c9.373 9.372 9.373 24.568 0 33.941zM640 456v-32c0-13.255-10.745-24-24-24H312c-13.255 0-24 10.745-24 24v32c0 13.255 10.745 24 24 24h304c13.255 0 24-10.745 24-24z"/></svg> </a> ZHANGWP </label> <div class=md-nav__source> <a href=https://github.com/zawnpn/ZHANGWP/ title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1 type=checkbox id=nav-1> <label class=md-nav__link for=nav-1> Home <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Home data-md-level=1> <label class=md-nav__title for=nav-1> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../.. title=Home class=md-nav__link> Home </a> </li> <li class=md-nav__item> <a href=../../../../links/ title=Links class=md-nav__link> Links </a> </li> <li class=md-nav__item> <a href=../../../../donates/ title=Donate class=md-nav__link> Donate </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2 type=checkbox id=nav-2 checked> <label class=md-nav__link for=nav-2> Notes <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Notes data-md-level=1> <label class=md-nav__title for=nav-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Notes </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../ title=Index class=md-nav__link> Index </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2-2 type=checkbox id=nav-2-2 checked> <label class=md-nav__link for=nav-2-2> Reinforcement <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Reinforcement data-md-level=2> <label class=md-nav__title for=nav-2-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Reinforcement </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2-2-1 type=checkbox id=nav-2-2-1 checked> <label class=md-nav__link for=nav-2-2-1> Reinforcement Learning An Introduction <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Reinforcement Learning An Introduction" data-md-level=3> <label class=md-nav__title for=nav-2-2-1> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Reinforcement Learning An Introduction </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../RLAI_2/ title="Chapter 2" class=md-nav__link> Chapter 2 </a> </li> <li class=md-nav__item> <a href=../RLAI_3/ title="Chapter 3" class=md-nav__link> Chapter 3 </a> </li> <li class=md-nav__item> <a href=../RLAI_4/ title="Chapter 4" class=md-nav__link> Chapter 4 </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Chapter 5 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 9h14V7H3v2m0 4h14v-2H3v2m0 4h14v-2H3v2m16 0h2v-2h-2v2m0-10v2h2V7h-2m0 6h2v-2h-2v2z"/></svg> </span> </label> <a href=./ title="Chapter 5" class="md-nav__link md-nav__link--active"> Chapter 5 </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#51-monte-carlo-prediction class=md-nav__link> 5.1 Monte Carlo Prediction </a> </li> <li class=md-nav__item> <a href=#52-monte-carlo-estimation-of-action-values class=md-nav__link> 5.2 Monte Carlo Estimation of Action Values </a> <nav class=md-nav aria-label="5.2 Monte Carlo Estimation of Action Values"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_1 class=md-nav__link> 问题 </a> </li> <li class=md-nav__item> <a href=#_2 class=md-nav__link> 思路 </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#53-monte-carlo-control class=md-nav__link> 5.3 Monte Carlo Control </a> <nav class=md-nav aria-label="5.3 Monte Carlo Control"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_3 class=md-nav__link> 目标 </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> 方法 </a> </li> <li class=md-nav__item> <a href=#monte-carlo-es-monte-carlo-with-exploring-starts class=md-nav__link> Monte Carlo ES (Monte Carlo with Exploring Starts) </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#54-monte-carlo-control-without-exploring-starts class=md-nav__link> 5.4 Monte Carlo Control without Exploring Starts </a> <nav class=md-nav aria-label="5.4 Monte Carlo Control without Exploring Starts"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_5 class=md-nav__link> 目标 </a> </li> <li class=md-nav__item> <a href=#_6 class=md-nav__link> 方法 </a> </li> <li class=md-nav__item> <a href=#on-policy-first-visit-mc-control class=md-nav__link> On-policy first-visit MC control </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#55-off-policy-prediction-via-importance-sampling class=md-nav__link> 5.5 Off-policy Prediction via Importance Sampling </a> <nav class=md-nav aria-label="5.5 Off-policy Prediction via Importance Sampling"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_7 class=md-nav__link> 目标 </a> </li> <li class=md-nav__item> <a href=#_8 class=md-nav__link> 原理 </a> </li> <li class=md-nav__item> <a href=#_9 class=md-nav__link> 两种估计方法 </a> <nav class=md-nav aria-label=两种估计方法> <ul class=md-nav__list> <li class=md-nav__item> <a href=#ordinary-importance-sampling class=md-nav__link> 简单平均（Ordinary Importance Sampling） </a> </li> <li class=md-nav__item> <a href=#weighted-importance-sampling class=md-nav__link> 加权平均（Weighted Importance Sampling） </a> </li> <li class=md-nav__item> <a href=#_10 class=md-nav__link> 举例：简单平均有可能导致无穷大方差 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#56-incremental-implementation class=md-nav__link> 5.6 Incremental Implementation </a> </li> <li class=md-nav__item> <a href=#57-off-policy-monte-carlo-control class=md-nav__link> 5.7 Off-policy Monte Carlo Control </a> </li> <li class=md-nav__item> <a href=#others class=md-nav__link> Others </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../RLAI_6/ title="Chapter 6" class=md-nav__link> Chapter 6 </a> </li> <li class=md-nav__item> <a href=../RLAI_7/ title="Chapter 7" class=md-nav__link> Chapter 7 </a> </li> <li class=md-nav__item> <a href=../RLAI_8/ title="Chapter 8" class=md-nav__link> Chapter 8 </a> </li> <li class=md-nav__item> <a href=../RLAI_9/ title="Chapter 9" class=md-nav__link> Chapter 9 </a> </li> <li class=md-nav__item> <a href=../RLAI_10/ title="Chapter 10" class=md-nav__link> Chapter 10 </a> </li> <li class=md-nav__item> <a href=../RLAI_11/ title="Chapter 11" class=md-nav__link> Chapter 11 </a> </li> <li class=md-nav__item> <a href=../RLAI_12/ title="Chapter 12" class=md-nav__link> Chapter 12 </a> </li> <li class=md-nav__item> <a href=../RLAI_13/ title="Chapter 13" class=md-nav__link> Chapter 13 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2-2-2 type=checkbox id=nav-2-2-2> <label class=md-nav__link for=nav-2-2-2> Some Introduction <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Some Introduction" data-md-level=3> <label class=md-nav__title for=nav-2-2-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Some Introduction </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../MCTS_introduction/ title=MCTS class=md-nav__link> MCTS </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> Tips <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Tips data-md-level=1> <label class=md-nav__title for=nav-3> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Tips </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../tips/ title=Tips class=md-nav__link> Tips </a> </li> <li class=md-nav__item> <a href=../../../../tips/to-do/ title="To Do" class=md-nav__link> To Do </a> </li> <li class=md-nav__item> <a href=../../../../tips/python/ title=Python class=md-nav__link> Python </a> </li> <li class=md-nav__item> <a href=../../../../tips/data-processing/ title="Data Processing" class=md-nav__link> Data Processing </a> </li> <li class=md-nav__item> <a href=../../../../tips/git/ title=Git class=md-nav__link> Git </a> </li> <li class=md-nav__item> <a href=../../../../tips/linux/ title=Linux class=md-nav__link> Linux </a> </li> <li class=md-nav__item> <a href=../../../../tips/win/ title=Windows class=md-nav__link> Windows </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4 type=checkbox id=nav-4> <label class=md-nav__link for=nav-4> Share <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Share data-md-level=1> <label class=md-nav__title for=nav-4> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Share </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/ title=Index class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../../share/blog-history/ title=博客历史 class=md-nav__link> 博客历史 </a> </li> <li class=md-nav__item> <a href=../../../../share/game-log/ title=Game-Log class=md-nav__link> Game-Log </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-4 type=checkbox id=nav-4-4> <label class=md-nav__link for=nav-4-4> NKU-Toolkit <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=NKU-Toolkit data-md-level=2> <label class=md-nav__title for=nav-4-4> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> NKU-Toolkit </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/nku-eamis/ title=NKU-EAMIS工具 class=md-nav__link> NKU-EAMIS工具 </a> </li> <li class=md-nav__item> <a href=../../../../share/nku-sms-rss/ title=NKU-SMS-RSS class=md-nav__link> NKU-SMS-RSS </a> </li> <li class=md-nav__item> <a href=../../../../share/eamis-miniapp/ title=NKU-EAMIS_MiniApp(南开大学教务助手小程序) class=md-nav__link> NKU-EAMIS_MiniApp(南开大学教务助手小程序) </a> </li> <li class=md-nav__item> <a href=../../../../share/eamis-workflow/ title="NKU-EAMIS for iOS(Workflow)" class=md-nav__link> NKU-EAMIS for iOS(Workflow) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-5 type=checkbox id=nav-4-5> <label class=md-nav__link for=nav-4-5> Steam-Toolkit <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Steam-Toolkit data-md-level=2> <label class=md-nav__title for=nav-4-5> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Steam-Toolkit </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/steam-market-price-bot/ title=Steam市场比价爬虫 class=md-nav__link> Steam市场比价爬虫 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-6 type=checkbox id=nav-4-6> <label class=md-nav__link for=nav-4-6> 数学建模 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=数学建模 data-md-level=2> <label class=md-nav__title for=nav-4-6> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 数学建模 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/2017-mcm-icm/ title="2017美赛参赛整理(Problem D)" class=md-nav__link> 2017美赛参赛整理(Problem D) </a> </li> <li class=md-nav__item> <a href=../../../../share/2016-guosai/ title=2016数学建模国赛 class=md-nav__link> 2016数学建模国赛 </a> </li> <li class=md-nav__item> <a href=../../../../share/math-model-szb/ title=数学建模之2016深圳杯——初次尝试 class=md-nav__link> 数学建模之2016深圳杯——初次尝试 </a> </li> <li class=md-nav__item> <a href=../../../../share/polygon-to-ellipse/ title=随机多边形转化为椭圆的过程研究 class=md-nav__link> 随机多边形转化为椭圆的过程研究 </a> </li> <li class=md-nav__item> <a href=../../../../share/FFT-GPU-Accel/ title=FFT-GPU-Accel class=md-nav__link> FFT-GPU-Accel </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7 type=checkbox id=nav-4-7> <label class=md-nav__link for=nav-4-7> NKU 数院试题整理 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="NKU 数院试题整理" data-md-level=2> <label class=md-nav__title for=nav-4-7> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> NKU 数院试题整理 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/nku-sms-exams/ title=汇总 class=md-nav__link> 汇总 </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7-2 type=checkbox id=nav-4-7-2> <label class=md-nav__link for=nav-4-7-2> 分析 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=分析 data-md-level=3> <label class=md-nav__title for=nav-4-7-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 分析 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/functional-analysis-final/ title=2017-2018第一学期泛函分析期末考试 class=md-nav__link> 2017-2018第一学期泛函分析期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/real-variable-function/ title=2016-2017第二学期实变函数期末考试 class=md-nav__link> 2016-2017第二学期实变函数期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-3-final/ title=2016-2017第一学期数学分析3-3期末考试 class=md-nav__link> 2016-2017第一学期数学分析3-3期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/complex-analysis-final/ title=2016-2017第一学期复变函数期末考试 class=md-nav__link> 2016-2017第一学期复变函数期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-3-middle/ title=2016-2017第一学期数学分析3-3期中考试 class=md-nav__link> 2016-2017第一学期数学分析3-3期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-2-final/ title=2015-2016第二学期数学分析3-2期末考试（含解答） class=md-nav__link> 2015-2016第二学期数学分析3-2期末考试（含解答） </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-2-middle/ title=2015-2016第二学期数学分析3-2期中考试 class=md-nav__link> 2015-2016第二学期数学分析3-2期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-1-final/ title=2015-2016第一学期数学分析3-1期末考试 class=md-nav__link> 2015-2016第一学期数学分析3-1期末考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7-3 type=checkbox id=nav-4-7-3> <label class=md-nav__link for=nav-4-7-3> 代数 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=代数 data-md-level=3> <label class=md-nav__title for=nav-4-7-3> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 代数 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/abstract-algebra-final/ title=2016-2017第一学期抽象代数期末考试 class=md-nav__link> 2016-2017第一学期抽象代数期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/abstract-algebra-middle/ title=2016-2017第一学期抽象代数期中考试 class=md-nav__link> 2016-2017第一学期抽象代数期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/advanced-algebra-2-2-final/ title=2015-2016第二学期高等代数2-2期末考试 class=md-nav__link> 2015-2016第二学期高等代数2-2期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/advanced-algebra-2-2-middle/ title=2015-2016第二学期高等代数2-2期中考试 class=md-nav__link> 2015-2016第二学期高等代数2-2期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/advanced-algebra-2-1-final/ title=2015-2016第一学期高等代数2-1期末考试 class=md-nav__link> 2015-2016第一学期高等代数2-1期末考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7-4 type=checkbox id=nav-4-7-4> <label class=md-nav__link for=nav-4-7-4> 概率统计 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=概率统计 data-md-level=3> <label class=md-nav__title for=nav-4-7-4> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 概率统计 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/probability-final/ title=2016-2017第二学期概率论期末考试 class=md-nav__link> 2016-2017第二学期概率论期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/probability-middle/ title=2016-2017第二学期概率论期中考试 class=md-nav__link> 2016-2017第二学期概率论期中考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7-5 type=checkbox id=nav-4-7-5> <label class=md-nav__link for=nav-4-7-5> 微分方程 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=微分方程 data-md-level=3> <label class=md-nav__title for=nav-4-7-5> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 微分方程 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/PDE-final/ title=2017-2018第一学期数理方程期末考试 class=md-nav__link> 2017-2018第一学期数理方程期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/ODE-final/ title=2016-2017第一学期常微分方程期末考试 class=md-nav__link> 2016-2017第一学期常微分方程期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/ODE-middle/ title=2016-2017第一学期常微分方程期中考试 class=md-nav__link> 2016-2017第一学期常微分方程期中考试 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-8 type=checkbox id=nav-4-8> <label class=md-nav__link for=nav-4-8> Other <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Other data-md-level=2> <label class=md-nav__title for=nav-4-8> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Other </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/github-student-pack/ title="Student Developer Pack - GitHub Education" class=md-nav__link> Student Developer Pack - GitHub Education </a> </li> <li class=md-nav__item> <a href=../../../../share/my-postgraduate-share/ title="保研推免经验分享 - 数学系跨保 CS" class=md-nav__link> 保研推免经验分享 - 数学系跨保 CS </a> </li> <li class=md-nav__item> <a href=../../../../share/roc-fly/ title=鹏程万里 class=md-nav__link> 鹏程万里 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-5 type=checkbox id=nav-5> <label class=md-nav__link for=nav-5> Statements <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Statements data-md-level=1> <label class=md-nav__title for=nav-5> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Statements </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../statements/ title=Statements class=md-nav__link> Statements </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#51-monte-carlo-prediction class=md-nav__link> 5.1 Monte Carlo Prediction </a> </li> <li class=md-nav__item> <a href=#52-monte-carlo-estimation-of-action-values class=md-nav__link> 5.2 Monte Carlo Estimation of Action Values </a> <nav class=md-nav aria-label="5.2 Monte Carlo Estimation of Action Values"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_1 class=md-nav__link> 问题 </a> </li> <li class=md-nav__item> <a href=#_2 class=md-nav__link> 思路 </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#53-monte-carlo-control class=md-nav__link> 5.3 Monte Carlo Control </a> <nav class=md-nav aria-label="5.3 Monte Carlo Control"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_3 class=md-nav__link> 目标 </a> </li> <li class=md-nav__item> <a href=#_4 class=md-nav__link> 方法 </a> </li> <li class=md-nav__item> <a href=#monte-carlo-es-monte-carlo-with-exploring-starts class=md-nav__link> Monte Carlo ES (Monte Carlo with Exploring Starts) </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#54-monte-carlo-control-without-exploring-starts class=md-nav__link> 5.4 Monte Carlo Control without Exploring Starts </a> <nav class=md-nav aria-label="5.4 Monte Carlo Control without Exploring Starts"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_5 class=md-nav__link> 目标 </a> </li> <li class=md-nav__item> <a href=#_6 class=md-nav__link> 方法 </a> </li> <li class=md-nav__item> <a href=#on-policy-first-visit-mc-control class=md-nav__link> On-policy first-visit MC control </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#55-off-policy-prediction-via-importance-sampling class=md-nav__link> 5.5 Off-policy Prediction via Importance Sampling </a> <nav class=md-nav aria-label="5.5 Off-policy Prediction via Importance Sampling"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_7 class=md-nav__link> 目标 </a> </li> <li class=md-nav__item> <a href=#_8 class=md-nav__link> 原理 </a> </li> <li class=md-nav__item> <a href=#_9 class=md-nav__link> 两种估计方法 </a> <nav class=md-nav aria-label=两种估计方法> <ul class=md-nav__list> <li class=md-nav__item> <a href=#ordinary-importance-sampling class=md-nav__link> 简单平均（Ordinary Importance Sampling） </a> </li> <li class=md-nav__item> <a href=#weighted-importance-sampling class=md-nav__link> 加权平均（Weighted Importance Sampling） </a> </li> <li class=md-nav__item> <a href=#_10 class=md-nav__link> 举例：简单平均有可能导致无穷大方差 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#56-incremental-implementation class=md-nav__link> 5.6 Incremental Implementation </a> </li> <li class=md-nav__item> <a href=#57-off-policy-monte-carlo-control class=md-nav__link> 5.7 Off-policy Monte Carlo Control </a> </li> <li class=md-nav__item> <a href=#others class=md-nav__link> Others </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <a href=https://github.com/zawnpn/ZHANGWP/edit/master/docs/notes/reinforcement-learning/notes/RLAI_5.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg> </a> <h1 id=->强化学习导论（五）- 蒙特卡罗方法<a class=headerlink href=#- title="Permanent link">&para;</a></h1> <p>本章主要讲强化学习中的蒙特卡罗方法。</p> <p><strong>Monte Carlo Methods(<a href=https://zh.wikipedia.org/wiki/%E8%92%99%E5%9C%B0%E5%8D%A1%E7%BE%85%E6%96%B9%E6%B3%95>Wiki</a>)</strong>:</p> <p>蒙特卡罗方法，也称统计模拟方法，是1940年代中期由于科学技术的发展和电子计算机的发明，而提出的一种以概率统计理论为指导的数值计算方法。是指使用随机数（或更常见的伪随机数）来解决很多计算问题的方法。</p> <p><strong>Monte Carlo Methods</strong>:</p> <p>在本书中，蒙特卡罗方法具体指<strong>基于样本返回值的均值，用于解决强化学习问题的方法</strong>。</p> <p>由于蒙特卡罗方法基于样本返回值来解决问题，所以需要得到明确的反馈值，因此本章主要基于片段式任务（episode tasks）进行探讨（并且 <span><span class=MathJax_Preview>\gamma=1</span><script type=math/tex>\gamma=1</script></span>），以 episode 为单位，当一个片段结束，才去通过整个片段的反馈来进行相应调整。</p> <h2 id=51-monte-carlo-prediction>5.1 Monte Carlo Prediction<a class=headerlink href=#51-monte-carlo-prediction title="Permanent link">&para;</a></h2> <p>在我们使用蒙特卡罗方法估计 return 时，一般有两种主要的统计方法：</p> <ul> <li><strong>first-visit MC method</strong>：</li> </ul> <p>在<strong>一个 episode 中</strong>，对于每个状态 s （或「状态-行动组合」 s-a），只考虑第一次进入 s （或 s-a）之后的 return 来对 <span><span class=MathJax_Preview>v_\pi(s)</span><script type=math/tex>v_\pi(s)</script></span> (or <span><span class=MathJax_Preview>q_\pi(s,a)</span><script type=math/tex>q_\pi(s,a)</script></span>) 进行估计，往后再遇到 s（或 s-a）则不再统计。</p> <p><img alt src=../imgs/RLAI_5/fvmc.png></p> <ul> <li><strong>every-visit MC method</strong>：</li> </ul> <p>every-visit 总体与 first-visit 相似，唯一的差别在于，在一个 episode 中，所有以 s （或 s-a） 为出发点的 episode 都会对其 return 进行统计，进而用于估计 <span><span class=MathJax_Preview>v_\pi, q_\pi</span><script type=math/tex>v_\pi, q_\pi</script></span> 。</p> <p>根据大数定律，容易分析知，他们均能收敛到 <span><span class=MathJax_Preview>v_\pi(s)</span><script type=math/tex>v_\pi(s)</script></span>。</p> <p>在一些牌类游戏中，原则上我们是可以将其视作有限 MDP 问题，比如我们将每局牌视作一个 episode ，然后根据「赢 / 输 / 平」给定奖励值 +1 / -1 / 0 ，然后就按我们之前讲的方法来做。</p> <p>然而实际操作中，我们很难求出这个问题背景下的「状态转移概率」，简单而言，即使我们完全清楚环境的变化机制 <span><span class=MathJax_Preview>p(s',r|s,a)</span><script type=math/tex>p(s',r|s,a)</script></span> ，也很难把问题背景理解「透彻」，我们很难一一分析列举出所有可能的情况，以及他们之间的关系（虽然理论上是肯定可以穷举出来的），所以需要用到蒙特卡罗方法直接根据大量「经验」来暴力估计出我们想要的东西。通俗地讲，就是：「我们虽然没有去具体分析环境的变化机制，但是不用想那么多，照着以前的经验做就是了」。</p> <h2 id=52-monte-carlo-estimation-of-action-values>5.2 Monte Carlo Estimation of Action Values<a class=headerlink href=#52-monte-carlo-estimation-of-action-values title="Permanent link">&para;</a></h2> <h3 id=_1>问题<a class=headerlink href=#_1 title="Permanent link">&para;</a></h3> <p>如果给定一个策略，在这个策略下去模拟，可能会有不少「状态-行动组合（state-action pair）」从来没尝试过，也就是某些状态的样本量可能为 0 ，对于随机模拟方法而言，这样估计出来的东西显然会有偏差，就像第二章讨论的那样，得多做一些「探索（exploration）」。</p> <h3 id=_2>思路<a class=headerlink href=#_2 title="Permanent link">&para;</a></h3> <p><strong>exploration start</strong>:</p> <p>既然策略 <span><span class=MathJax_Preview>\pi</span><script type=math/tex>\pi</script></span> 是给定的，在模拟过程中，基于这个策略一步一步行动下去，我们很难改变什么，唯一能任意指定的，就是初始状态，所以在生成模拟片段时，可以考虑随机指定任意状态为初始状态，这样只要我们生成的模拟片段足够多，一样能确保每个状态都能被我们访问足够多次。我们称这样指定的初始状态为 <strong>exploration start</strong> 。</p> <p>不过，这个方法有相当大的局限性：</p> <ul> <li>在一些特殊情况下，我们必须得跟环境交互才能学习策略，这时候便不能指定初始状态了</li> <li>操作起来麻烦，而且依然考虑得不够全面</li> </ul> <h2 id=53-monte-carlo-control>5.3 Monte Carlo Control<a class=headerlink href=#53-monte-carlo-control title="Permanent link">&para;</a></h2> <h3 id=_3>目标<a class=headerlink href=#_3 title="Permanent link">&para;</a></h3> <p>通过蒙特卡罗方法来估计最优策略。</p> <h3 id=_4>方法<a class=headerlink href=#_4 title="Permanent link">&para;</a></h3> <p>上一章讲过，GPI 模型非常通用，能够描述绝大多数强化学习方法，而这一章我们依然是基于 GPI 模型，通过「值的估计」和「策略改进」这两个环节交替作用，最终得到最优策略。</p> <div> <div class=MathJax_Preview> \pi_0 \xrightarrow{E} q_{\pi_0} \xrightarrow{I}\pi_1 \xrightarrow{E} q_{\pi_1} \xrightarrow{I} \pi_2 \xrightarrow{E} \cdots \xrightarrow{I} \pi_* \xrightarrow{E} q_* </div> <script type="math/tex; mode=display">
\pi_0 \xrightarrow{E} q_{\pi_0} \xrightarrow{I}\pi_1 \xrightarrow{E} q_{\pi_1} \xrightarrow{I} \pi_2 \xrightarrow{E} \cdots \xrightarrow{I} \pi_* \xrightarrow{E} q_*
</script> </div> <ul> <li> <p>Policy evaluation: 使用蒙特卡罗方法来根据经验模拟估计，而不是像上一章的方法直接计算。这一小节的算法需要有 <strong>exploring start</strong> 。</p> </li> <li> <p>Policy improvement: 跟上一章一样，采用贪心策略来改进当前策略 <span><span class=MathJax_Preview>\pi(s)\doteq \max\limits_aq(s,a)</span><script type=math/tex>\pi(s)\doteq \max\limits_aq(s,a)</script></span> 。</p> </li> </ul> <h3 id=monte-carlo-es-monte-carlo-with-exploring-starts>Monte Carlo ES (Monte Carlo with Exploring Starts)<a class=headerlink href=#monte-carlo-es-monte-carlo-with-exploring-starts title="Permanent link">&para;</a></h3> <p>下面是带有「探索初始态」的蒙特卡罗方法的伪代码：</p> <p><img alt src=../imgs/RLAI_5/mces.png></p> <h2 id=54-monte-carlo-control-without-exploring-starts>5.4 Monte Carlo Control without Exploring Starts<a class=headerlink href=#54-monte-carlo-control-without-exploring-starts title="Permanent link">&para;</a></h2> <h3 id=_5>目标<a class=headerlink href=#_5 title="Permanent link">&para;</a></h3> <p>之前提到，exploration start 这样的条件，仍然有不少缺点，缺乏泛用性，我们需要其他的办法。</p> <h3 id=_6>方法<a class=headerlink href=#_6 title="Permanent link">&para;</a></h3> <p>我们先提两个概念：</p> <ul> <li> <p>On-policy: 直接对我们的决策策略进行估值和改进。</p> </li> <li> <p>Off-policy: 结合一个其他的策略，来对我们的决策策略进行估值和改进。</p> </li> </ul> <p>这一小节我们先讲 On-policy 。</p> <p>在 On-Policy 方法中，我们的策略一般得是「<strong>soft</strong>」的： <span><span class=MathJax_Preview>\pi(a|s)&gt;0, \forall s \in \mathcal S, a\in\mathcal A(s)</span><script type=math/tex>\pi(a|s)>0, \forall s \in \mathcal S, a\in\mathcal A(s)</script></span>。直白点讲，就是所有的情况都要被考虑到，即使某个 action 并不优秀，也不能直接将其选取概率设为 0 ，这样的策略显得不那么绝对，体现出一种「趋势性」：好的 action 更容易被选上，不好的 action 也有一定的机会。</p> <p><strong><span><span class=MathJax_Preview>\varepsilon</span><script type=math/tex>\varepsilon</script></span>-soft policy</strong>:</p> <ul> <li> <p>若 a 为非贪心策略（exploration），则 <span><span class=MathJax_Preview>\pi(a|s)=\frac{\varepsilon}{|\mathcal A(s)|}</span><script type=math/tex>\pi(a|s)=\frac{\varepsilon}{|\mathcal A(s)|}</script></span></p> </li> <li> <p>若 a 为贪心策略（exploitation），则 <span><span class=MathJax_Preview>\pi(a|s)=1-\varepsilon + \frac{\varepsilon}{|\mathcal A(s)|}</span><script type=math/tex>\pi(a|s)=1-\varepsilon + \frac{\varepsilon}{|\mathcal A(s)|}</script></span></p> </li> </ul> <h3 id=on-policy-first-visit-mc-control>On-policy first-visit MC control<a class=headerlink href=#on-policy-first-visit-mc-control title="Permanent link">&para;</a></h3> <p>基于 On-policy 方法，我们可以给出下面的算法来估计最优策略：</p> <p><img alt src=../imgs/RLAI_5/opfvmc.png></p> <p><strong>policy improvement theorem</strong>:</p> <p>GPI 模型并不要求我们的策略全程都是贪心策略，只需要「渐渐变得贪心」就可以了，在我们上面的算法中，我们的 <span><span class=MathJax_Preview>\varepsilon</span><script type=math/tex>\varepsilon</script></span>-soft 策略会渐渐变成一个 <span><span class=MathJax_Preview>\varepsilon</span><script type=math/tex>\varepsilon</script></span>-greedy 策略，并且可以证明，通过贪心方法确实能够改进 <span><span class=MathJax_Preview>\varepsilon</span><script type=math/tex>\varepsilon</script></span>-soft 策略，证明如下：</p> <p>设 <span><span class=MathJax_Preview>\pi'</span><script type=math/tex>\pi'</script></span> 为贪心改进后的策略，则有</p> <div> <div class=MathJax_Preview> \pi'(a|s)= \begin{cases} \frac{\varepsilon}{|\mathcal A(s)|}&amp;, \mathrm{non-greedy}\\ 1-\varepsilon-\frac{\varepsilon}{|\mathcal A(s)|}&amp;, \mathrm{greedy} \end{cases} </div> <script type="math/tex; mode=display">
\pi'(a|s)=
\begin{cases}
\frac{\varepsilon}{|\mathcal A(s)|}&, \mathrm{non-greedy}\\
1-\varepsilon-\frac{\varepsilon}{|\mathcal A(s)|}&, \mathrm{greedy}
\end{cases}
</script> </div> <p>而原本的 <span><span class=MathJax_Preview>\varepsilon</span><script type=math/tex>\varepsilon</script></span>-soft 策略 <span><span class=MathJax_Preview>\pi</span><script type=math/tex>\pi</script></span>，我们记其概率分布为</p> <div> <div class=MathJax_Preview> \pi(a_i|s)= \begin{cases} \frac{\varepsilon}{|\mathcal A(s)|}+\delta_i&amp;,a_i\neq a_*\\ 1-\frac{(|\mathcal A(s)|-1)\varepsilon}{|\mathcal A(s)|}-\sum_{a_i\neq a_*}\delta_i&amp;,a_i=a_* \end{cases} </div> <script type="math/tex; mode=display">
\pi(a_i|s)=
\begin{cases}
\frac{\varepsilon}{|\mathcal A(s)|}+\delta_i&,a_i\neq a_*\\
1-\frac{(|\mathcal A(s)|-1)\varepsilon}{|\mathcal A(s)|}-\sum_{a_i\neq a_*}\delta_i&,a_i=a_*
\end{cases}
</script> </div> <p>接下来，可得</p> <div> <div class=MathJax_Preview> \begin{aligned} v_{\pi'}(s)&amp;=\sum_a \pi'(a|s)q_\pi(s,a)\\ &amp;=\frac{\varepsilon}{|\mathcal A(s)|}\sum_aq_\pi(s,a)+(1-\varepsilon)\max\limits_aq_\pi(s,a) \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
v_{\pi'}(s)&=\sum_a
\pi'(a|s)q_\pi(s,a)\\
&=\frac{\varepsilon}{|\mathcal A(s)|}\sum_aq_\pi(s,a)+(1-\varepsilon)\max\limits_aq_\pi(s,a)
\end{aligned}
</script> </div> <p>记 <span><span class=MathJax_Preview>M=\max\limits_aq_\pi(s,a)</span><script type=math/tex>M=\max\limits_aq_\pi(s,a)</script></span>，我们先证明不等式 <span><span class=MathJax_Preview>\displaystyle \max\limits_aq_\pi(s,a)\geq\sum_a\frac{\pi(a|s)-\frac{\varepsilon}{|\mathcal A(s)|}}{1-\varepsilon}q_\pi(s,a)</span><script type=math/tex>\displaystyle \max\limits_aq_\pi(s,a)\geq\sum_a\frac{\pi(a|s)-\frac{\varepsilon}{|\mathcal A(s)|}}{1-\varepsilon}q_\pi(s,a)</script></span>：</p> <div> <div class=MathJax_Preview> \begin{aligned} &amp;\sum_a\frac{\pi(a|s)-\frac{\varepsilon}{|\mathcal A(s)|}}{1-\varepsilon}q_\pi(s,a)\\ &amp;=\frac{\pi(a_*|s)-\frac{\varepsilon}{|\mathcal A(s)|}}{1-\varepsilon}M+\sum_{a_i\neq a_*}\frac{\pi(a_i|s)-\frac{\varepsilon}{|\mathcal A(s)|}}{1-\varepsilon}q_\pi(s,a_i)\\ &amp;= \frac{1-\frac{(|\mathcal A(s)|-1)\varepsilon}{|\mathcal A(s)|}-\sum_{a_i\neq a_*}\delta_i-\frac{\varepsilon}{|\mathcal A(s)|}}{1-\varepsilon}M+\sum_{a_i\neq a_*}\frac{\delta_i}{1-\varepsilon}q_\pi(s,a_i)\\ &amp;=\frac{1-\varepsilon}{1-\varepsilon}M-\frac{1}{1-\varepsilon}\sum_{a_i\neq a_*}\delta_iM+\frac{1}{1-\varepsilon}\sum_{a_i\neq a_*}\delta_iq_\pi(s,a_i)\\ &amp;=M-\frac{\sum_{a_i\neq a_*}\delta_i}{1-\varepsilon}(M-q_\pi(s,a_i))\\ &amp;\leq M=\max\limits_aq_\pi(s,a) \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
&\sum_a\frac{\pi(a|s)-\frac{\varepsilon}{|\mathcal A(s)|}}{1-\varepsilon}q_\pi(s,a)\\
&=\frac{\pi(a_*|s)-\frac{\varepsilon}{|\mathcal A(s)|}}{1-\varepsilon}M+\sum_{a_i\neq a_*}\frac{\pi(a_i|s)-\frac{\varepsilon}{|\mathcal A(s)|}}{1-\varepsilon}q_\pi(s,a_i)\\
&= \frac{1-\frac{(|\mathcal A(s)|-1)\varepsilon}{|\mathcal A(s)|}-\sum_{a_i\neq a_*}\delta_i-\frac{\varepsilon}{|\mathcal A(s)|}}{1-\varepsilon}M+\sum_{a_i\neq a_*}\frac{\delta_i}{1-\varepsilon}q_\pi(s,a_i)\\
&=\frac{1-\varepsilon}{1-\varepsilon}M-\frac{1}{1-\varepsilon}\sum_{a_i\neq a_*}\delta_iM+\frac{1}{1-\varepsilon}\sum_{a_i\neq a_*}\delta_iq_\pi(s,a_i)\\
&=M-\frac{\sum_{a_i\neq a_*}\delta_i}{1-\varepsilon}(M-q_\pi(s,a_i))\\
&\leq M=\max\limits_aq_\pi(s,a)
\end{aligned}
</script> </div> <p>将不等式代回前面，得到</p> <div> <div class=MathJax_Preview> \begin{aligned} v_{\pi'}(s)&amp;\geq\frac{\varepsilon}{|\mathcal A(s)|}\sum_aq_\pi(s,a)+(1-\varepsilon)\sum_a\frac{\pi(a|s)-\frac{\varepsilon}{|\mathcal A(s)|}}{1-\varepsilon}q_\pi(s,a)\\ &amp;=\frac{\varepsilon}{|\mathcal A(s)|}\sum_aq_\pi(s,a)-\frac{\varepsilon}{|\mathcal A(s)|}\sum_aq_\pi(s,a)+\sum_a\pi(a|s)q_\pi(s,a)\\ &amp;=v_\pi(s) \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
v_{\pi'}(s)&\geq\frac{\varepsilon}{|\mathcal A(s)|}\sum_aq_\pi(s,a)+(1-\varepsilon)\sum_a\frac{\pi(a|s)-\frac{\varepsilon}{|\mathcal A(s)|}}{1-\varepsilon}q_\pi(s,a)\\
&=\frac{\varepsilon}{|\mathcal A(s)|}\sum_aq_\pi(s,a)-\frac{\varepsilon}{|\mathcal A(s)|}\sum_aq_\pi(s,a)+\sum_a\pi(a|s)q_\pi(s,a)\\
&=v_\pi(s)
\end{aligned}
</script> </div> <p>所以得出结论，对策略 <span><span class=MathJax_Preview>\pi</span><script type=math/tex>\pi</script></span> 做出题述的改进后得到的 <span><span class=MathJax_Preview>\pi'</span><script type=math/tex>\pi'</script></span> 确实要优于 <span><span class=MathJax_Preview>\pi</span><script type=math/tex>\pi</script></span> 。</p> <h2 id=55-off-policy-prediction-via-importance-sampling>5.5 Off-policy Prediction via Importance Sampling<a class=headerlink href=#55-off-policy-prediction-via-importance-sampling title="Permanent link">&para;</a></h2> <h3 id=_7>目标<a class=headerlink href=#_7 title="Permanent link">&para;</a></h3> <p>在我们的问题中，目标策略经常是<strong>确定性</strong>的贪心策略（与 soft 相对，确定性策略选出的 action 很确定），这种情况下，我们若要使用 On-policy ，不得不又重新考虑 exploration start ，为了避免这个情况，我们要考虑一种 Off-policy 的方法。</p> <p>与 On-policy 不同的是，我们去学习一个最优策略，并不一定要在调整这个策略的同时也跟着它走，这一节采用的 Off-policy 方法则是用一个「行为策略」来生成行动，来对我们的「目标策略」进行优化，这样的一个好处是，我们可以通过辅助性的行为策略来做出探索行动，而我们要学习的目标策略，就不用再因 exploration 而不得不加入一些不那么好的行动。</p> <p>而将两个策略分开最重要的好处，正是我们可以通过 soft 的行为策略去生成探索 action ，而目标策略则保持「确定性」，解决了前面提出的问题，进一步加强了算法的泛用性。</p> <ul> <li> <p>目标策略 (<span><span class=MathJax_Preview>\pi</span><script type=math/tex>\pi</script></span>): 被学习的策略</p> </li> <li> <p>行为策略 (<span><span class=MathJax_Preview>b</span><script type=math/tex>b</script></span>): 用来再学习过程中生成 actions 的策略</p> </li> </ul> <p>行为策略需要是完全已知的，并且需要能被目标策略<strong>覆盖</strong>：<span><span class=MathJax_Preview>\pi(a|s)&gt;0\Rightarrow b(a|s)&gt;0</span><script type=math/tex>\pi(a|s)>0\Rightarrow b(a|s)>0</script></span> ，即目标策略所有可能采取到的行动，在行为策略中其被选取的概率也必须大于 0 。</p> <h3 id=_8>原理<a class=headerlink href=#_8 title="Permanent link">&para;</a></h3> <p><strong>重要性采样</strong>:</p> <p>重要性采样（importance sampling）是统计学中估计某一分布性质时使用的一种方法。该方法从与原分布不同的另一个分布中采样，而对原先分布的性质进行估计。</p> <p>易知，</p> <div> <div class=MathJax_Preview> \begin{aligned} \mathbb E_f[x] &amp;= \int xf(x)\mathrm{d}x\\ &amp;= \int \frac{xf(x)}{g(x)} g(x)\mathrm{d}x\\ &amp;= \mathbb E_g [\frac{f(x)}{g(x)}x] \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\mathbb E_f[x] &= \int xf(x)\mathrm{d}x\\
&= \int \frac{xf(x)}{g(x)} g(x)\mathrm{d}x\\
&= \mathbb E_g [\frac{f(x)}{g(x)}x]
\end{aligned}
</script> </div> <p>我们称 <span><span class=MathJax_Preview>\rho=\frac{f(x)}{g(x)}</span><script type=math/tex>\rho=\frac{f(x)}{g(x)}</script></span> 为重要性采样比例，这样，对于一个未知的分布 <span><span class=MathJax_Preview>f</span><script type=math/tex>f</script></span> ，若已知分布 <span><span class=MathJax_Preview>g</span><script type=math/tex>g</script></span> ，并且能求出比值 <span><span class=MathJax_Preview>\frac{f(x)}{g(x)}</span><script type=math/tex>\frac{f(x)}{g(x)}</script></span> ，便能方便地在 <span><span class=MathJax_Preview>g</span><script type=math/tex>g</script></span> 分布下对 <span><span class=MathJax_Preview>x</span><script type=math/tex>x</script></span> 进行估计。</p> <p>在我们这个问题中，因为有状态条件，所以应该求条件期望，易分析知条件期望同样适用：</p> <div> <div class=MathJax_Preview> \begin{aligned} \mathbb{E}_f[x|S=s]&amp;=\int xf(x|s)\mathrm{d}x\\ &amp;=\int x\frac{f(x|s)}{g(x|s)}g(x|s)\mathrm{d}x\\ &amp;=\mathbb{E}_g[x\frac{f(x|s)}{g(x|s)}|S=s]\\ &amp;=\mathbb{E}_g[\rho x|S=s] \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}_f[x|S=s]&=\int xf(x|s)\mathrm{d}x\\
&=\int x\frac{f(x|s)}{g(x|s)}g(x|s)\mathrm{d}x\\
&=\mathbb{E}_g[x\frac{f(x|s)}{g(x|s)}|S=s]\\
&=\mathbb{E}_g[\rho x|S=s]
\end{aligned}
</script> </div> <p>因此，对于我们具体的问题，我们可以按下面的方法来进行估计：</p> <p>对于一个 episode ，其子序列的概率为</p> <div> <div class=MathJax_Preview> \begin{aligned} &amp;\mathrm{P}_\pi\{A_t,S_{t+1},A_{t+1},\ldots,S_T\mid S_t\}\\ &amp;=\pi(A_t|S_t)p(S_{t+1}|S_t,A_t)\pi(A_{t+1}|S_{t+1})\cdots p(S_T|S_{T-1},A_{T-1})\\ &amp;=\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)\\ &amp;\mathrm{P}_b\{A_t,S_{t+1},A_{t+1},\ldots,S_T\mid S_t\}\\ &amp;=b(A_t|S_t)p(S_{t+1}|S_t,A_t)b(A_{t+1}|S_{t+1})\cdots p(S_T|S_{T-1},A_{T-1})\\ &amp;=\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+1}|S_k,A_k) \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
&\mathrm{P}_\pi\{A_t,S_{t+1},A_{t+1},\ldots,S_T\mid S_t\}\\
&=\pi(A_t|S_t)p(S_{t+1}|S_t,A_t)\pi(A_{t+1}|S_{t+1})\cdots p(S_T|S_{T-1},A_{T-1})\\
&=\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)\\
&\mathrm{P}_b\{A_t,S_{t+1},A_{t+1},\ldots,S_T\mid S_t\}\\
&=b(A_t|S_t)p(S_{t+1}|S_t,A_t)b(A_{t+1}|S_{t+1})\cdots p(S_T|S_{T-1},A_{T-1})\\
&=\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+1}|S_k,A_k)
\end{aligned}
</script> </div> <p>计算出重要性采样比例：</p> <div> <div class=MathJax_Preview> \begin{aligned} \rho_{t:T-1} &amp;\doteq \frac{\mathrm{P}_\pi\{A_t,S_{t+1},A_{t+1},\ldots,S_T\mid S_t\}}{\mathrm{P}_b\{A_t,S_{t+1},A_{t+1},\ldots,S_T\mid S_t\}}\\ &amp;=\frac{\prod_{k=t}^{T-1}\pi(A_{k+1}|S_k)p(S_{k+1}|S_k, A_{k+1})}{\prod_{k=t}^{T-1}b(A_{k+1}|S_k)p(S_{k+1}|S_k, A_{k+1})} \\ &amp;=\prod_{k=t}^{T-1} \frac{\pi(A_{k+1}|S_k)}{b(A_{k+1}|S_k)} \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
      \rho_{t:T-1} &\doteq \frac{\mathrm{P}_\pi\{A_t,S_{t+1},A_{t+1},\ldots,S_T\mid S_t\}}{\mathrm{P}_b\{A_t,S_{t+1},A_{t+1},\ldots,S_T\mid S_t\}}\\
      &=\frac{\prod_{k=t}^{T-1}\pi(A_{k+1}|S_k)p(S_{k+1}|S_k, A_{k+1})}{\prod_{k=t}^{T-1}b(A_{k+1}|S_k)p(S_{k+1}|S_k, A_{k+1})} \\
      &=\prod_{k=t}^{T-1} \frac{\pi(A_{k+1}|S_k)}{b(A_{k+1}|S_k)}
      \end{aligned}
</script> </div> <p>从上面可以看出，我们只需要知道策略是怎样的，而无需去关心环境上的细节（也就是不用知道状态转移概率 <span><span class=MathJax_Preview>p</span><script type=math/tex>p</script></span> ）</p> <p>有了前面的准备，我们可以推出：</p> <div> <div class=MathJax_Preview> \begin{aligned} v_\pi(s)&amp;=\mathbb{E}_\pi[G_t|S_t=s]\\ &amp;=\mathbb{E}_b[\rho_{t:T-1}G_t|S_t=s] \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
      v_\pi(s)&=\mathbb{E}_\pi[G_t|S_t=s]\\
      &=\mathbb{E}_b[\rho_{t:T-1}G_t|S_t=s]
      \end{aligned}
</script> </div> <h3 id=_9>两种估计方法<a class=headerlink href=#_9 title="Permanent link">&para;</a></h3> <h4 id=ordinary-importance-sampling>简单平均（Ordinary Importance Sampling）<a class=headerlink href=#ordinary-importance-sampling title="Permanent link">&para;</a></h4> <ul> <li> <p><span><span class=MathJax_Preview>\displaystyle V(s)\doteq\frac{\sum_{t\in\mathcal T(s)}\rho_{t:T(t)-1}G_t}{|\mathcal T(s)|}</span><script type=math/tex>\displaystyle V(s)\doteq\frac{\sum_{t\in\mathcal T(s)}\rho_{t:T(t)-1}G_t}{|\mathcal T(s)|}</script></span></p> </li> <li> <p><span><span class=MathJax_Preview>\mathcal T(s)</span><script type=math/tex>\mathcal T(s)</script></span>: 访问到状态 s 的时间点集合</p> </li> <li> <p><span><span class=MathJax_Preview>T(t)</span><script type=math/tex>T(t)</script></span>: 以时间点 t 开始的 episode 的终止时间点</p> </li> <li> <p>优点：无偏估计</p> </li> <li> <p>缺点：方差较大，不稳定</p> </li> </ul> <h4 id=weighted-importance-sampling>加权平均（Weighted Importance Sampling）<a class=headerlink href=#weighted-importance-sampling title="Permanent link">&para;</a></h4> <ul> <li> <p><span><span class=MathJax_Preview>\displaystyle V(s)\doteq\frac{\sum_{t\in\mathcal T(s)}\rho_{t:T(t)-1}G_t}{\sum_{t\in\mathcal T(s)}\rho_{t:T(t)-1}}</span><script type=math/tex>\displaystyle V(s)\doteq\frac{\sum_{t\in\mathcal T(s)}\rho_{t:T(t)-1}G_t}{\sum_{t\in\mathcal T(s)}\rho_{t:T(t)-1}}</script></span></p> </li> <li> <p>优点：方差较小</p> </li> <li> <p>缺点：有偏估计（但是渐进无偏）</p> </li> </ul> <h4 id=_10>举例：简单平均有可能导致无穷大方差<a class=headerlink href=#_10 title="Permanent link">&para;</a></h4> <p><img alt src=../imgs/RLAI_5/infvar.png></p> <p>首先，对于加权平均，我们易分析得：</p> <ul> <li> <p>如果以行动 <strong><em>left</em></strong> 收尾，显然会返回 <span><span class=MathJax_Preview>G_t = 1</span><script type=math/tex>G_t = 1</script></span> ，此时 <span><span class=MathJax_Preview>\rho =\frac{1}{0.5}=2</span><script type=math/tex>\rho =\frac{1}{0.5}=2</script></span>，那么必然有 <span><span class=MathJax_Preview>V(s) = 1</span><script type=math/tex>V(s) = 1</script></span></p> </li> <li> <p>如果以行动 <strong><em>right</em></strong> 收尾，显然会返回 <span><span class=MathJax_Preview>G_t = 0</span><script type=math/tex>G_t = 0</script></span> ，此时 <span><span class=MathJax_Preview>\rho = 0</span><script type=math/tex>\rho = 0</script></span> ，易分析知 <span><span class=MathJax_Preview>V(s)=0</span><script type=math/tex>V(s)=0</script></span></p> </li> </ul> <p>可以看出，加权平均下的估计是稳定的，方差很小。</p> <p>而对于简单平均，由于</p> <div> <div class=MathJax_Preview> \mathrm{Var}[V]=\mathbb{E}[V-\bar{V}]^2=\mathbb{E}[V^2-2V\bar{V}+\bar{V}^2]=\mathbb{E}[V^2]-\bar{V}^2 </div> <script type="math/tex; mode=display">
\mathrm{Var}[V]=\mathbb{E}[V-\bar{V}]^2=\mathbb{E}[V^2-2V\bar{V}+\bar{V}^2]=\mathbb{E}[V^2]-\bar{V}^2
</script> </div> <p>因为 <span><span class=MathJax_Preview>\bar{V}</span><script type=math/tex>\bar{V}</script></span> 有限，我们只需讨论 <span><span class=MathJax_Preview>\mathbb{E}[V^2]</span><script type=math/tex>\mathbb{E}[V^2]</script></span>:</p> <div> <div class=MathJax_Preview> \begin{aligned} \mathbb{E}[V^2]=&amp;\mathbb{E}_b\left[\left(\prod_{t=0}^{T-1}\frac{\pi(A_t|S_t)}{b(A_t|S_t)}G_0\right)^2\right]\\ =&amp;\frac{1}{2}\cdot0.1\left(\frac{1}{0.5}\right)^2\\ &amp;+\frac{1}{2}\cdot0.9\cdot\frac{1}{2}\cdot0.1\left(\frac{1}{0.5}\frac{1}{0.5}\right)^2\\ &amp;+\frac{1}{2}\cdot0.9\cdot\frac{1}{2}\cdot0.9\cdot\frac{1}{2}\cdot0.1\left(\frac{1}{0.5}\frac{1}{0.5}\frac{1}{0.5}\right)^2\\ &amp;+\cdots\\ =&amp;0.1\sum_{k=0}^\infty0.9^k\cdot2^k\cdot2=0.2\sum_{k=0}^{\infty}1.8^k=\infty \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}[V^2]=&\mathbb{E}_b\left[\left(\prod_{t=0}^{T-1}\frac{\pi(A_t|S_t)}{b(A_t|S_t)}G_0\right)^2\right]\\
=&\frac{1}{2}\cdot0.1\left(\frac{1}{0.5}\right)^2\\
&+\frac{1}{2}\cdot0.9\cdot\frac{1}{2}\cdot0.1\left(\frac{1}{0.5}\frac{1}{0.5}\right)^2\\
&+\frac{1}{2}\cdot0.9\cdot\frac{1}{2}\cdot0.9\cdot\frac{1}{2}\cdot0.1\left(\frac{1}{0.5}\frac{1}{0.5}\frac{1}{0.5}\right)^2\\
&+\cdots\\
=&0.1\sum_{k=0}^\infty0.9^k\cdot2^k\cdot2=0.2\sum_{k=0}^{\infty}1.8^k=\infty
\end{aligned}
</script> </div> <p>所以，从这个例子可以可以看出，简单平均是相当不稳定的。</p> <h2 id=56-incremental-implementation>5.6 Incremental Implementation<a class=headerlink href=#56-incremental-implementation title="Permanent link">&para;</a></h2> <p>第二章讲过增量执行式，可以通过增量计算来节省内存并且提高计算速度，这个思路我们同样能用在这一章的算法里。</p> <p>将 <span><span class=MathJax_Preview>\rho_{t:T(t)-1}</span><script type=math/tex>\rho_{t:T(t)-1}</script></span> 简记作 <span><span class=MathJax_Preview>W_k</span><script type=math/tex>W_k</script></span> ，即有 <span><span class=MathJax_Preview>\displaystyle V_n = \frac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k}</span><script type=math/tex>\displaystyle V_n = \frac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k}</script></span> ，记 <span><span class=MathJax_Preview>C_n = \sum_{k=1}^nW_k</span><script type=math/tex>C_n = \sum_{k=1}^nW_k</script></span> ，那么</p> <div> <div class=MathJax_Preview> \begin{aligned} V_{n+1}&amp;=\frac{\sum_{k=1}^{n-1}W_kG_k+W_nG_n}{\sum_{k=1}^nW_k}\\ &amp;=\frac{\sum_{k=1}^{n-1}W_k\frac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k}+W_nG_n}{\sum_{k=1}^nW_k}\\ &amp;=\frac{C_{n-1}V_n+W_nG_n}{C_n}\\ &amp;=\frac{(C_n-W_n)V_n+W_nG_n}{C_n}\\ &amp;=V_n+\frac{W_n}{C_n}[G_n-V_n] \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
V_{n+1}&=\frac{\sum_{k=1}^{n-1}W_kG_k+W_nG_n}{\sum_{k=1}^nW_k}\\
&=\frac{\sum_{k=1}^{n-1}W_k\frac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k}+W_nG_n}{\sum_{k=1}^nW_k}\\
&=\frac{C_{n-1}V_n+W_nG_n}{C_n}\\
&=\frac{(C_n-W_n)V_n+W_nG_n}{C_n}\\
&=V_n+\frac{W_n}{C_n}[G_n-V_n]
\end{aligned}
</script> </div> <p>于是便能得到下面的增量执行式：</p> <div> <div class=MathJax_Preview> \begin{aligned} V_{n+1}&amp;=V_n+\frac{W_n}{C_n}[G_n-V_n]\\ C_{n+1}&amp;=C_n+W_{n+1}, (C_0=0) \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
V_{n+1}&=V_n+\frac{W_n}{C_n}[G_n-V_n]\\
C_{n+1}&=C_n+W_{n+1}, (C_0=0)
\end{aligned}
</script> </div> <p>下面是结合增量执行的 Off-policy MC 算法</p> <p><img alt src=../imgs/RLAI_5/opmcp.png></p> <p>这个算法的终止条件是 <span><span class=MathJax_Preview>W=0\Rightarrow \pi(A_t|S_t)=0</span><script type=math/tex>W=0\Rightarrow \pi(A_t|S_t)=0</script></span> ，这意味着行为策略 <span><span class=MathJax_Preview>b</span><script type=math/tex>b</script></span> 生成了一个目标策略 <span><span class=MathJax_Preview>\pi</span><script type=math/tex>\pi</script></span> 中没有的 action ，这样就没有继续学习下去的意义，所以需要终止此段 episode ，开始下一段 episode 的学习。</p> <h2 id=57-off-policy-monte-carlo-control>5.7 Off-policy Monte Carlo Control<a class=headerlink href=#57-off-policy-monte-carlo-control title="Permanent link">&para;</a></h2> <p>有了前面的准备，便能最终得到估计最优策略的 Off-policy MC 算法：</p> <p><img alt src=../imgs/RLAI_5/opmcc.png></p> <ul> <li>这个算法里，我们给的 <span><span class=MathJax_Preview>\pi</span><script type=math/tex>\pi</script></span> 是确定性而非 soft 的，但是将 <span><span class=MathJax_Preview>b</span><script type=math/tex>b</script></span> 设为 soft 的，以确保 action 的 exploration ，并且同时维持 <span><span class=MathJax_Preview>\pi</span><script type=math/tex>\pi</script></span> 的确定性</li> <li>注意到，<span><span class=MathJax_Preview>W</span><script type=math/tex>W</script></span> 的更新式理应写作 <span><span class=MathJax_Preview>W\leftarrow W\frac{\pi(A_t|S_t)}{b(A_t|S_t)}</span><script type=math/tex>W\leftarrow W\frac{\pi(A_t|S_t)}{b(A_t|S_t)}</script></span> ，但这里写作 <span><span class=MathJax_Preview>W\leftarrow W\frac{1}{b(A_t|S_t)}</span><script type=math/tex>W\leftarrow W\frac{1}{b(A_t|S_t)}</script></span> ，这是因为本例中的目标策略是<strong>确定性</strong>的，所以每个状态下，采取的行动是确定的，因而 <span><span class=MathJax_Preview>\pi(A_t|S_t)=1</span><script type=math/tex>\pi(A_t|S_t)=1</script></span></li> <li>终止条件 <span><span class=MathJax_Preview>A_t\neq \pi(S_t)</span><script type=math/tex>A_t\neq \pi(S_t)</script></span> 的原理和前面相同，即此时没有了继续学习下去的意义，应当停止当且 episode 并进入下一片段</li> </ul> <h2 id=others>Others<a class=headerlink href=#others title="Permanent link">&para;</a></h2> <p>本书后面几小节仅简要介绍了一些更特殊的估计 <span><span class=MathJax_Preview>V(s)</span><script type=math/tex>V(s)</script></span> 的方法，但未作进一步论证，此处略去。</p> <hr> <div class=md-source-date> <small> Last update: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">August 1, 2019</span> </small> </div> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid" aria-label=Footer> <a href=../RLAI_4/ title="Chapter 4" class="md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Previous </span> Chapter 4 </div> </div> </a> <a href=../RLAI_6/ title="Chapter 6" class="md-footer-nav__link md-footer-nav__link--next" rel=next> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Next </span> Chapter 6 </div> </div> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2016-2020 ZHANGWP </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-footer-social> <a href=https://github.com/zawnpn target=_blank rel=noopener title=github.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://twitter.com/zawnpn target=_blank rel=noopener title=twitter.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> </a> <a href=https://psnprofiles.com/zawnpn target=_blank rel=noopener title=psnprofiles.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><path d="M570.9 372.3c-11.3 14.2-38.8 24.3-38.8 24.3L327 470.2v-54.3l150.9-53.8c17.1-6.1 19.8-14.8 5.8-19.4-13.9-4.6-39.1-3.3-56.2 2.9L327 381.1v-56.4c23.2-7.8 47.1-13.6 75.7-16.8 40.9-4.5 90.9.6 130.2 15.5 44.2 14 49.2 34.7 38 48.9zm-224.4-92.5v-139c0-16.3-3-31.3-18.3-35.6-11.7-3.8-19 7.1-19 23.4v347.9l-93.8-29.8V32c39.9 7.4 98 24.9 129.2 35.4C424.1 94.7 451 128.7 451 205.2c0 74.5-46 102.8-104.5 74.6zM43.2 410.2c-45.4-12.8-53-39.5-32.3-54.8 19.1-14.2 51.7-24.9 51.7-24.9l134.5-47.8v54.5l-96.8 34.6c-17.1 6.1-19.7 14.8-5.8 19.4 13.9 4.6 39.1 3.3 56.2-2.9l46.4-16.9v48.8c-51.6 9.3-101.4 7.3-153.9-10z"/></svg> </a> <a href=https://steamcommunity.com/id/zawnpn/ target=_blank rel=noopener title=steamcommunity.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M496 256c0 137-111.2 248-248.4 248-113.8 0-209.6-76.3-239-180.4l95.2 39.3c6.4 32.1 34.9 56.4 68.9 56.4 39.2 0 71.9-32.4 70.2-73.5l84.5-60.2c52.1 1.3 95.8-40.9 95.8-93.5 0-51.6-42-93.5-93.7-93.5s-93.7 42-93.7 93.5v1.2L176.6 279c-15.5-.9-30.7 3.4-43.5 12.1L0 236.1C10.2 108.4 117.1 8 247.6 8 384.8 8 496 119 496 256zM155.7 384.3l-30.5-12.6a52.79 52.79 0 0027.2 25.8c26.9 11.2 57.8-1.6 69-28.4 5.4-13 5.5-27.3.1-40.3-5.4-13-15.5-23.2-28.5-28.6-12.9-5.4-26.7-5.2-38.9-.6l31.5 13c19.8 8.2 29.2 30.9 20.9 50.7-8.3 19.9-31 29.2-50.8 21zm173.8-129.9c-34.4 0-62.4-28-62.4-62.3s28-62.3 62.4-62.3 62.4 28 62.4 62.3-27.9 62.3-62.4 62.3zm.1-15.6c25.9 0 46.9-21 46.9-46.8 0-25.9-21-46.8-46.9-46.8s-46.9 21-46.9 46.8c.1 25.8 21.1 46.8 46.9 46.8z"/></svg> </a> <a href=https://www.zhihu.com/people/zhangwanpeng target=_blank rel=noopener title=www.zhihu.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13H170.54zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82v170.31zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62v.01zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2l19.23 14.43zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78l.03-.01z"/></svg> </a> </div> </div> </div> </footer> </div> <script src=../../../../assets/javascripts/vendor.d710d30a.min.js></script> <script src=../../../../assets/javascripts/bundle.b39636ac.min.js></script><script id=__lang type=application/json>{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script> <script>
        app = initialize({
          base: "../../../..",
          features: ["tabs"],
          search: Object.assign({
            worker: "../../../../assets/javascripts/worker/search.a68abb33.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script> <script src="//cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-MML-AM_SVG"></script> </body> </html>