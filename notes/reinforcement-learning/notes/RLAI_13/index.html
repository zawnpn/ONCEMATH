



<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="Welcome to zhangwp's blog.">
      
      
        <link rel="canonical" href="https://www.zhangwp.com/notes/reinforcement-learning/notes/RLAI_13/">
      
      
        <meta name="author" content="zawnpn">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="jp">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.0.4">
    
    
      
        <title>Chapter 13 - ZHANGWP</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/application.451f80e5.css">
      
        <link rel="stylesheet" href="../../../../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
      <script src="../../../../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      
    
    <link rel="stylesheet" href="../../../../assets/fonts/material-icons.css">
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="white" data-md-color-accent="light-blue">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="../../../../#-" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://www.zhangwp.com" title="ZHANGWP" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                ZHANGWP
              </span>
              <span class="md-header-nav__topic">
                Chapter 13
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  


  <a href="https://github.com/zawnpn/ZHANGWP/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      GitHub
    </div>
  </a>

          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../.." title="Home" class="md-tabs__link">
          Home
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../" title="Notes" class="md-tabs__link md-tabs__link--active">
          Notes
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../../tips/" title="Tips" class="md-tabs__link">
          Tips
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../../share/" title="Share" class="md-tabs__link">
          Share
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../../../../statements/" title="Statements" class="md-tabs__link">
          Statements
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://www.zhangwp.com" title="ZHANGWP" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    ZHANGWP
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/zawnpn/ZHANGWP/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#__github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      GitHub
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-1" type="checkbox" id="nav-1">
    
    <label class="md-nav__link" for="nav-1">
      Home
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-1">
        Home
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../.." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../links/" title="Links" class="md-nav__link">
      Links
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../donates/" title="Donate" class="md-nav__link">
      Donate
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      Notes
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Notes
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../" title="Index" class="md-nav__link">
      Index
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-2" type="checkbox" id="nav-2-2" checked>
    
    <label class="md-nav__link" for="nav-2-2">
      Reinforcement
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-2-2">
        Reinforcement
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-2-1" type="checkbox" id="nav-2-2-1" checked>
    
    <label class="md-nav__link" for="nav-2-2-1">
      Reinforcement Learning An Introduction
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-2-2-1">
        Reinforcement Learning An Introduction
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_2/" title="Chapter 2" class="md-nav__link">
      Chapter 2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_3/" title="Chapter 3" class="md-nav__link">
      Chapter 3
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_4/" title="Chapter 4" class="md-nav__link">
      Chapter 4
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_5/" title="Chapter 5" class="md-nav__link">
      Chapter 5
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_6/" title="Chapter 6" class="md-nav__link">
      Chapter 6
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_7/" title="Chapter 7" class="md-nav__link">
      Chapter 7
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_8/" title="Chapter 8" class="md-nav__link">
      Chapter 8
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_9/" title="Chapter 9" class="md-nav__link">
      Chapter 9
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_10/" title="Chapter 10" class="md-nav__link">
      Chapter 10
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_11/" title="Chapter 11" class="md-nav__link">
      Chapter 11
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../RLAI_12/" title="Chapter 12" class="md-nav__link">
      Chapter 12
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Chapter 13
      </label>
    
    <a href="./" title="Chapter 13" class="md-nav__link md-nav__link--active">
      Chapter 13
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#131-policy-approximation-and-its-advantages" title="13.1 Policy Approximation and its Advantages" class="md-nav__link">
    13.1 Policy Approximation and its Advantages
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#132-the-policy-gradient-theorem" title="13.2 The Policy Gradient Theorem" class="md-nav__link">
    13.2 The Policy Gradient Theorem
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#133-reinforce-monte-carlo-policy-gradient" title="13.3 REINFORCE: Monte Carlo Policy Gradient" class="md-nav__link">
    13.3 REINFORCE: Monte Carlo Policy Gradient
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#134-reinforce-with-baseline" title="13.4 REINFORCE with Baseline" class="md-nav__link">
    13.4 REINFORCE with Baseline
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#135-actorcritic-methods" title="13.5 Actor–Critic Methods" class="md-nav__link">
    13.5 Actor–Critic Methods
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#136-policy-gradient-for-continuing-problems" title="13.6 Policy Gradient for Continuing Problems" class="md-nav__link">
    13.6 Policy Gradient for Continuing Problems
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#137-policy-parameterization-for-continuous-actions" title="13.7 Policy Parameterization for Continuous Actions" class="md-nav__link">
    13.7 Policy Parameterization for Continuous Actions
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-2-2" type="checkbox" id="nav-2-2-2">
    
    <label class="md-nav__link" for="nav-2-2-2">
      Some Introduction
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-2-2-2">
        Some Introduction
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../MCTS_introduction/" title="MCTS" class="md-nav__link">
      MCTS
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Tips
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Tips
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../tips/" title="Tips" class="md-nav__link">
      Tips
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../tips/to-do/" title="To Do" class="md-nav__link">
      To Do
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../tips/python/" title="Python" class="md-nav__link">
      Python
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../tips/data-processing/" title="Data Processing" class="md-nav__link">
      Data Processing
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../tips/git/" title="Git" class="md-nav__link">
      Git
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../tips/linux/" title="Linux" class="md-nav__link">
      Linux
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../tips/win/" title="Windows" class="md-nav__link">
      Windows
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      Share
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        Share
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/" title="Index" class="md-nav__link">
      Index
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/blog-history/" title="博客历史" class="md-nav__link">
      博客历史
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/game-log/" title="Game-Log" class="md-nav__link">
      Game-Log
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-4" type="checkbox" id="nav-4-4">
    
    <label class="md-nav__link" for="nav-4-4">
      NKU-Toolkit
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-4-4">
        NKU-Toolkit
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/nku-eamis/" title="NKU-EAMIS工具" class="md-nav__link">
      NKU-EAMIS工具
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/nku-sms-rss/" title="NKU-SMS-RSS" class="md-nav__link">
      NKU-SMS-RSS
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/eamis-miniapp/" title="NKU-EAMIS_MiniApp(南开大学教务助手小程序)" class="md-nav__link">
      NKU-EAMIS_MiniApp(南开大学教务助手小程序)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/eamis-workflow/" title="NKU-EAMIS for iOS(Workflow)" class="md-nav__link">
      NKU-EAMIS for iOS(Workflow)
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-5" type="checkbox" id="nav-4-5">
    
    <label class="md-nav__link" for="nav-4-5">
      Steam-Toolkit
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-4-5">
        Steam-Toolkit
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/steam-market-price-bot/" title="Steam市场比价爬虫" class="md-nav__link">
      Steam市场比价爬虫
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-6" type="checkbox" id="nav-4-6">
    
    <label class="md-nav__link" for="nav-4-6">
      数学建模
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-4-6">
        数学建模
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/2017-mcm-icm/" title="2017美赛参赛整理(Problem D)" class="md-nav__link">
      2017美赛参赛整理(Problem D)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/2016-guosai/" title="2016数学建模国赛" class="md-nav__link">
      2016数学建模国赛
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/math-model-szb/" title="数学建模之2016深圳杯——初次尝试" class="md-nav__link">
      数学建模之2016深圳杯——初次尝试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/polygon-to-ellipse/" title="随机多边形转化为椭圆的过程研究" class="md-nav__link">
      随机多边形转化为椭圆的过程研究
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/FFT-GPU-Accel/" title="FFT-GPU-Accel" class="md-nav__link">
      FFT-GPU-Accel
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-7" type="checkbox" id="nav-4-7">
    
    <label class="md-nav__link" for="nav-4-7">
      NKU 数院试题整理
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-4-7">
        NKU 数院试题整理
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/nku-sms-exams/" title="汇总" class="md-nav__link">
      汇总
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-7-2" type="checkbox" id="nav-4-7-2">
    
    <label class="md-nav__link" for="nav-4-7-2">
      分析
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-4-7-2">
        分析
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/functional-analysis-final/" title="2017-2018第一学期泛函分析期末考试" class="md-nav__link">
      2017-2018第一学期泛函分析期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/real-variable-function/" title="2016-2017第二学期实变函数期末考试" class="md-nav__link">
      2016-2017第二学期实变函数期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/mathematical-analysis-3-3-final/" title="2016-2017第一学期数学分析3-3期末考试" class="md-nav__link">
      2016-2017第一学期数学分析3-3期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/complex-analysis-final/" title="2016-2017第一学期复变函数期末考试" class="md-nav__link">
      2016-2017第一学期复变函数期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/mathematical-analysis-3-3-middle/" title="2016-2017第一学期数学分析3-3期中考试" class="md-nav__link">
      2016-2017第一学期数学分析3-3期中考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/mathematical-analysis-3-2-final/" title="2015-2016第二学期数学分析3-2期末考试（含解答）" class="md-nav__link">
      2015-2016第二学期数学分析3-2期末考试（含解答）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/mathematical-analysis-3-2-middle/" title="2015-2016第二学期数学分析3-2期中考试" class="md-nav__link">
      2015-2016第二学期数学分析3-2期中考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/mathematical-analysis-3-1-final/" title="2015-2016第一学期数学分析3-1期末考试" class="md-nav__link">
      2015-2016第一学期数学分析3-1期末考试
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-7-3" type="checkbox" id="nav-4-7-3">
    
    <label class="md-nav__link" for="nav-4-7-3">
      代数
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-4-7-3">
        代数
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/abstract-algebra-final/" title="2016-2017第一学期抽象代数期末考试" class="md-nav__link">
      2016-2017第一学期抽象代数期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/abstract-algebra-middle/" title="2016-2017第一学期抽象代数期中考试" class="md-nav__link">
      2016-2017第一学期抽象代数期中考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/advanced-algebra-2-2-final/" title="2015-2016第二学期高等代数2-2期末考试" class="md-nav__link">
      2015-2016第二学期高等代数2-2期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/advanced-algebra-2-2-middle/" title="2015-2016第二学期高等代数2-2期中考试" class="md-nav__link">
      2015-2016第二学期高等代数2-2期中考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/advanced-algebra-2-1-final/" title="2015-2016第一学期高等代数2-1期末考试" class="md-nav__link">
      2015-2016第一学期高等代数2-1期末考试
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-7-4" type="checkbox" id="nav-4-7-4">
    
    <label class="md-nav__link" for="nav-4-7-4">
      概率统计
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-4-7-4">
        概率统计
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/probability-final/" title="2016-2017第二学期概率论期末考试" class="md-nav__link">
      2016-2017第二学期概率论期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/probability-middle/" title="2016-2017第二学期概率论期中考试" class="md-nav__link">
      2016-2017第二学期概率论期中考试
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-7-5" type="checkbox" id="nav-4-7-5">
    
    <label class="md-nav__link" for="nav-4-7-5">
      微分方程
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-4-7-5">
        微分方程
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/PDE-final/" title="2017-2018第一学期数理方程期末考试" class="md-nav__link">
      2017-2018第一学期数理方程期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/ODE-final/" title="2016-2017第一学期常微分方程期末考试" class="md-nav__link">
      2016-2017第一学期常微分方程期末考试
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/exam/ODE-middle/" title="2016-2017第一学期常微分方程期中考试" class="md-nav__link">
      2016-2017第一学期常微分方程期中考试
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/my-postgraduate-share/" title="保研推免经验分享 - 数学系跨保 CS" class="md-nav__link">
      保研推免经验分享 - 数学系跨保 CS
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/roc-fly/" title="鹏程万里" class="md-nav__link">
      鹏程万里
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-10" type="checkbox" id="nav-4-10">
    
    <label class="md-nav__link" for="nav-4-10">
      Other
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-4-10">
        Other
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../share/github-student-pack/" title="Student Developer Pack - GitHub Education" class="md-nav__link">
      Student Developer Pack - GitHub Education
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      Statements
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        Statements
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../../../statements/" title="Statements" class="md-nav__link">
      Statements
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#131-policy-approximation-and-its-advantages" title="13.1 Policy Approximation and its Advantages" class="md-nav__link">
    13.1 Policy Approximation and its Advantages
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#132-the-policy-gradient-theorem" title="13.2 The Policy Gradient Theorem" class="md-nav__link">
    13.2 The Policy Gradient Theorem
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#133-reinforce-monte-carlo-policy-gradient" title="13.3 REINFORCE: Monte Carlo Policy Gradient" class="md-nav__link">
    13.3 REINFORCE: Monte Carlo Policy Gradient
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#134-reinforce-with-baseline" title="13.4 REINFORCE with Baseline" class="md-nav__link">
    13.4 REINFORCE with Baseline
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#135-actorcritic-methods" title="13.5 Actor–Critic Methods" class="md-nav__link">
    13.5 Actor–Critic Methods
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#136-policy-gradient-for-continuing-problems" title="13.6 Policy Gradient for Continuing Problems" class="md-nav__link">
    13.6 Policy Gradient for Continuing Problems
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#137-policy-parameterization-for-continuous-actions" title="13.7 Policy Parameterization for Continuous Actions" class="md-nav__link">
    13.7 Policy Parameterization for Continuous Actions
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/zawnpn/ZHANGWP/edit/master/docs/notes/reinforcement-learning/notes/RLAI_13.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="-">强化学习导论（十三）- 策略梯度法<a class="headerlink" href="#-" title="Permanent link">&para;</a></h1>
<p>之前一直在讲 action-value 方法，它们都依赖于对 action-value 的估计，而本章的方法将考虑直接去学习『参数化策略』，这样就能不通过 value function 来选择 action 。</p>
<div>
<div class="MathJax_Preview">
\pi ( a | s , \boldsymbol { \theta } ) = \operatorname { Pr } \left\{ A _ { t } = a | S _ { t } = s , \boldsymbol { \theta } _ { t } = \boldsymbol { \theta } \right\}
</div>
<script type="math/tex; mode=display">
\pi ( a | s , \boldsymbol { \theta } ) = \operatorname { Pr } \left\{ A _ { t } = a | S _ { t } = s , \boldsymbol { \theta } _ { t } = \boldsymbol { \theta } \right\}
</script>
</div>
<p>本章主要考虑对度量函数 <span><span class="MathJax_Preview">J ( \boldsymbol { \theta } )</span><script type="math/tex">J ( \boldsymbol { \theta } )</script></span> 的梯度（关于策略参数 <span><span class="MathJax_Preview">\boldsymbol { \theta }</span><script type="math/tex">\boldsymbol { \theta }</script></span> ）来学习，来最大化 performance ，因此参数更新式即为对 <span><span class="MathJax_Preview">J</span><script type="math/tex">J</script></span> 的梯度上升：</p>
<div>
<div class="MathJax_Preview">
\boldsymbol { \theta } _ { t + 1 } = \boldsymbol { \theta } _ { t } + \alpha \widehat { \nabla J \left( \boldsymbol { \theta } _ { t } \right) }
</div>
<script type="math/tex; mode=display">
\boldsymbol { \theta } _ { t + 1 } = \boldsymbol { \theta } _ { t } + \alpha \widehat { \nabla J \left( \boldsymbol { \theta } _ { t } \right) }
</script>
</div>
<p>其中，<span><span class="MathJax_Preview">\widehat { \nabla J \left( \boldsymbol { \theta } _ { t } \right) } \in \mathbb { R } ^ { d ^ { \prime } }</span><script type="math/tex">\widehat { \nabla J \left( \boldsymbol { \theta } _ { t } \right) } \in \mathbb { R } ^ { d ^ { \prime } }</script></span> 是对梯度的一个估计，其期望值为该梯度。所有满足该通式的方法，均称为『policy gradient methods』。</p>
<p>有些方法，既学习了 policy ，也同时学了 value function ，称这样的方法为『actor-critic methods』，其中『actor』指所学的 policy，『critic』指所学的 value function 。</p>
<p>下面具体介绍 Policy Gradient Methods 。</p>
<h2 id="131-policy-approximation-and-its-advantages"><strong>13.1 Policy Approximation and its Advantages</strong><a class="headerlink" href="#131-policy-approximation-and-its-advantages" title="Permanent link">&para;</a></h2>
<p>对于不太大的离散 action 空间，若要将 policy 参数化，一个很自然的方法是对每个 state-action 来构造一个参数化的数值偏好 <span><span class="MathJax_Preview">h(s, a, \boldsymbol{\theta}) \in \mathbb{R}</span><script type="math/tex">h(s, a, \boldsymbol{\theta}) \in \mathbb{R}</script></span> ，进而通过指数 soft-max 函数来得到分布</p>
<div>
<div class="MathJax_Preview">
\pi(a | s, \boldsymbol{\theta}) \doteq \frac{e^{h(s, a, \boldsymbol{\theta})}}{\sum_{b} e^{h(s, b, \boldsymbol{\theta})}}
</div>
<script type="math/tex; mode=display">
\pi(a | s, \boldsymbol{\theta}) \doteq \frac{e^{h(s, a, \boldsymbol{\theta})}}{\sum_{b} e^{h(s, b, \boldsymbol{\theta})}}
</script>
</div>
<p>称这样参数化得到的 policy 为『soft-max in action preferences』。</p>
<p>action preferences 可以被任意参数化。既可以用神经网络来计算（ <span><span class="MathJax_Preview">\boldsymbol{\theta}</span><script type="math/tex">\boldsymbol{\theta}</script></span> 作为网络的权重），也可以简单地使用线性模型 <span><span class="MathJax_Preview">h(s, a, \boldsymbol{\theta})=\boldsymbol{\theta}^{\top} \mathbf{x}(s, a)</span><script type="math/tex">h(s, a, \boldsymbol{\theta})=\boldsymbol{\theta}^{\top} \mathbf{x}(s, a)</script></span> 。</p>
<p>Policy Approximation 有几个优点：</p>
<ul>
<li>第一个优势是，即使对于 deterministic policy（确定性策略，明确选择某个具体 action 的策略），参数化策略也能足够逼近（比如将某个 a 对应的 <span><span class="MathJax_Preview">h(s,a,\boldsymbol{\theta})</span><script type="math/tex">h(s,a,\boldsymbol{\theta})</script></span> 设为无穷大即可），而传统的 <span><span class="MathJax_Preview">\varepsilon</span><script type="math/tex">\varepsilon</script></span>-greedy 策略则不能做到，因为它必须对非最优策略分配 <span><span class="MathJax_Preview">\varepsilon</span><script type="math/tex">\varepsilon</script></span> 的概率。</li>
<li>第二个优势是能灵活地任意分配 action 的概率，对于一些特殊情况，比如不完全信息下的卡牌游戏，最佳 policy 对应的选择随机性很强，能够对两种差异很大的 action 来分配概率进而做出选择，比如在 Poker 中进行 bluffing 时（bluff 指在自己手牌较弱时加注以试图吓退对方），这对于 action-value 方法而言很难做到，从书中 example 13.1 中可以简单明了的看出两类方法的效果差异。</li>
<li>第三个优势是，参数化的 policy 是一个相对更易于近似的函数(Simsek, Algorta, and Kothiyal, 2016)。</li>
<li>最后一个优势是，参数化的 policy 能较好地将先验知识引入强化学习系统，这通常也是选择 policy-based learning method 的重要原因。</li>
</ul>
<h2 id="132-the-policy-gradient-theorem"><strong>13.2 The Policy Gradient Theorem</strong><a class="headerlink" href="#132-the-policy-gradient-theorem" title="Permanent link">&para;</a></h2>
<p>参数化 policy 除了上一节提到的几点实用价值外，还有一个重要的理论优势。对于连续的参数化 policy ，action 的概率可以连续性地改变，而 <span><span class="MathJax_Preview">\varepsilon</span><script type="math/tex">\varepsilon</script></span>-greedy 方法中选择 action 的概率则有可能因小变动而发生突变，主要是由于 policy-gradient methods 有着更强的收敛性。</p>
<p>这一节先只考虑 episodic 形式的问题，不失一般性，先假定每一段 episode 都起始于指定状态 <span><span class="MathJax_Preview">s_0</span><script type="math/tex">s_0</script></span> ，定义 performance：</p>
<div>
<div class="MathJax_Preview">
J(\boldsymbol{\theta}) \doteq v_{\pi_{\boldsymbol{\theta}}}\left(s_{0}\right)
</div>
<script type="math/tex; mode=display">
J(\boldsymbol{\theta}) \doteq v_{\pi_{\boldsymbol{\theta}}}\left(s_{0}\right)
</script>
</div>
<p>其中 <span><span class="MathJax_Preview">v_{\pi_{\boldsymbol{\theta}}}\left(s_{0}\right)</span><script type="math/tex">v_{\pi_{\boldsymbol{\theta}}}\left(s_{0}\right)</script></span> 是 <span><span class="MathJax_Preview">\pi_\theta</span><script type="math/tex">\pi_\theta</script></span> 的 <strong>true</strong> value function ，其中策略由 <span><span class="MathJax_Preview">\boldsymbol{\theta}</span><script type="math/tex">\boldsymbol{\theta}</script></span> 决定。为简化证明，后面的推导中假定 <span><span class="MathJax_Preview">\gamma=1</span><script type="math/tex">\gamma=1</script></span> （但描述算法时则写回一般形式）。</p>
<p>『<strong>Policy Gradient Theorem</strong>』：</p>
<div>
<div class="MathJax_Preview">
\nabla J(\boldsymbol{\theta}) \propto \sum_{s} \mu(s) \sum_{a} q_{\pi}(s, a) \nabla \pi(a | s, \boldsymbol{\theta})
</div>
<script type="math/tex; mode=display">
\nabla J(\boldsymbol{\theta}) \propto \sum_{s} \mu(s) \sum_{a} q_{\pi}(s, a) \nabla \pi(a | s, \boldsymbol{\theta})
</script>
</div>
<p>推导过程如下 (episodic case)</p>
<div>
<div class="MathJax_Preview">
\begin{aligned} \nabla v_{\pi}(s) &amp;=\nabla\left[\sum_{a} \pi(a | s) q_{\pi}(s, a)\right], \quad \text { for all } s \in \mathcal{S} \\ &amp;=\sum_{a}\left[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \nabla q_{\pi}(s, a)\right] \\ &amp;=\sum_{a}\left[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \nabla \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left(r+v_{\pi}\left(s^{\prime}\right)\right)\right]
\\&amp;=\sum_{a}\left[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla v_{\pi}\left(s^{\prime}\right)\right] \\&amp;= \sum_{a}\Bigg[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right)  \\ &amp;\times \sum_{a^{\prime}}\left[\nabla \pi\left(a^{\prime} | s^{\prime}\right) q_{\pi}\left(s^{\prime}, a^{\prime}\right)+\pi\left(a^{\prime} | s^{\prime}\right) \sum_{s^{\prime \prime}} p\left(s^{\prime \prime} | s^{\prime}, a^{\prime}\right) \nabla v_{\pi}\left(s^{\prime \prime}\right)\right] \Bigg]\\
&amp;=\sum_{x \in \mathcal S} \sum_{k=0}^{\infty} \operatorname{Pr}(s \rightarrow x, k, \pi) \sum_{a} \nabla \pi(a | x) q_{\pi}(x, a)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} \nabla v_{\pi}(s) &=\nabla\left[\sum_{a} \pi(a | s) q_{\pi}(s, a)\right], \quad \text { for all } s \in \mathcal{S} \\ &=\sum_{a}\left[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \nabla q_{\pi}(s, a)\right] \\ &=\sum_{a}\left[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \nabla \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left(r+v_{\pi}\left(s^{\prime}\right)\right)\right]
\\&=\sum_{a}\left[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla v_{\pi}\left(s^{\prime}\right)\right] \\&= \sum_{a}\Bigg[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right)  \\ &\times \sum_{a^{\prime}}\left[\nabla \pi\left(a^{\prime} | s^{\prime}\right) q_{\pi}\left(s^{\prime}, a^{\prime}\right)+\pi\left(a^{\prime} | s^{\prime}\right) \sum_{s^{\prime \prime}} p\left(s^{\prime \prime} | s^{\prime}, a^{\prime}\right) \nabla v_{\pi}\left(s^{\prime \prime}\right)\right] \Bigg]\\
&=\sum_{x \in \mathcal S} \sum_{k=0}^{\infty} \operatorname{Pr}(s \rightarrow x, k, \pi) \sum_{a} \nabla \pi(a | x) q_{\pi}(x, a)
\end{aligned}
</script>
</div>
<p>上一步是将前式反复展开得来，其中 <span><span class="MathJax_Preview">\operatorname{Pr}(s \rightarrow x, k, \pi)</span><script type="math/tex">\operatorname{Pr}(s \rightarrow x, k, \pi)</script></span> 表示在策略 <span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span> 下，从状态 s 经过 k 步达到状态 x 的转移概率，于是可得</p>
<div>
<div class="MathJax_Preview">
\begin{aligned} \nabla J(\boldsymbol{\theta}) &amp;=\nabla v_{\pi}\left(s_{0}\right) \\ &amp;=\sum_{s}\left(\sum_{k=0}^{\infty} \operatorname{Pr}\left(s_{0} \rightarrow s, k, \pi\right)\right) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \\ &amp;=\sum_{s} \eta(s) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \\ &amp;=\sum_{s^{\prime}} \eta\left(s^{\prime}\right) \sum_{s} \frac{\eta(s)}{\sum_{s^{\prime}} \eta\left(s^{\prime}\right)} \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \\ &amp;=\sum_{s^{\prime}} \eta\left(s^{\prime}\right) \sum_{s} \mu(s)\sum_a\nabla\pi(a|s) q_{\pi}(s, a) \\ &amp; \propto \sum_{s} \mu(s) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} \nabla J(\boldsymbol{\theta}) &=\nabla v_{\pi}\left(s_{0}\right) \\ &=\sum_{s}\left(\sum_{k=0}^{\infty} \operatorname{Pr}\left(s_{0} \rightarrow s, k, \pi\right)\right) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \\ &=\sum_{s} \eta(s) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \\ &=\sum_{s^{\prime}} \eta\left(s^{\prime}\right) \sum_{s} \frac{\eta(s)}{\sum_{s^{\prime}} \eta\left(s^{\prime}\right)} \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \\ &=\sum_{s^{\prime}} \eta\left(s^{\prime}\right) \sum_{s} \mu(s)\sum_a\nabla\pi(a|s) q_{\pi}(s, a) \\ & \propto \sum_{s} \mu(s) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \end{aligned}
</script>
</div>
<p>证毕。</p>
<p>关于 <span><span class="MathJax_Preview">\eta(s)</span><script type="math/tex">\eta(s)</script></span> 和 <span><span class="MathJax_Preview">\mu(s)</span><script type="math/tex">\mu(s)</script></span> ，之前已在第 9、10 章有过相关定义：
$$
\begin{aligned}
\eta(s)&amp;=h(s)+\sum_{\overline{s}} \eta(\overline{s}) \sum_{a} \pi(a | \overline{s}) p(s | \overline{s}, a), \text { for all } s \in \mathcal{S}\
\mu(s)&amp;=\frac{\eta(s)}{\sum_{s^{\prime}} \eta\left(s^{\prime}\right)}, \text { for all } s \in \mathcal{S}
\end{aligned}
$$</p>
<h2 id="133-reinforce-monte-carlo-policy-gradient"><strong>13.3 REINFORCE: Monte Carlo Policy Gradient</strong><a class="headerlink" href="#133-reinforce-monte-carlo-policy-gradient" title="Permanent link">&para;</a></h2>
<p>下面开始介绍 policy-gradient 的学习算法。回想一开始的目标是要得到随机梯度上升的形式，且希望样本梯度的期望恰好为度量函数的梯度，而 policy gradient theorem 给出的公式恰好满足，注意到公式右侧是一个关于 <span><span class="MathJax_Preview">\mu(s)</span><script type="math/tex">\mu(s)</script></span> （其含义是在服从策略 <span><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span> 时，各状态 s 发生的概率）的加权和，因此有</p>
<div>
<div class="MathJax_Preview">
\begin{aligned} \nabla J(\boldsymbol{\theta}) &amp; \propto \sum_{s} \mu(s) \sum_{a} q_{\pi}(s, a) \nabla \pi(a | s, \boldsymbol{\theta}) \\ &amp;=\mathbb{E}_{\pi}\left[\sum_{a} q_{\pi}\left(S_{t}, a\right) \nabla \pi\left(a | S_{t}, \boldsymbol{\theta}\right)\right] \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} \nabla J(\boldsymbol{\theta}) & \propto \sum_{s} \mu(s) \sum_{a} q_{\pi}(s, a) \nabla \pi(a | s, \boldsymbol{\theta}) \\ &=\mathbb{E}_{\pi}\left[\sum_{a} q_{\pi}\left(S_{t}, a\right) \nabla \pi\left(a | S_{t}, \boldsymbol{\theta}\right)\right] \end{aligned}
</script>
</div>
<p>于是我们的随机梯度上升算法可以写作：</p>
<div>
<div class="MathJax_Preview">
\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alpha \sum_{a} \hat{q}\left(S_{t}, a, \mathbf{w}\right) \nabla \pi\left(a | S_{t}, \boldsymbol{\theta}\right)
</div>
<script type="math/tex; mode=display">
\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alpha \sum_{a} \hat{q}\left(S_{t}, a, \mathbf{w}\right) \nabla \pi\left(a | S_{t}, \boldsymbol{\theta}\right)
</script>
</div>
<p>其中 <span><span class="MathJax_Preview">\hat{q}</span><script type="math/tex">\hat{q}</script></span> 是对 <span><span class="MathJax_Preview">q_\pi</span><script type="math/tex">q_\pi</script></span> 学习出来的逼近，称该算法 all-actions 方法。因为它的更新过程包含了全部 action ，有着不错的前景，但这里将重点放在传统的 REINFORCE algorithm (Willams, 1992) ，它在 t 时刻的更新只涉及到该时刻实际采取行动的 action <span><span class="MathJax_Preview">A_t</span><script type="math/tex">A_t</script></span> ，做法是将随机变量的加权和替换为一个期望，然后来对这个期望做采样：</p>
<div>
<div class="MathJax_Preview">
\begin{aligned} \nabla J(\boldsymbol{\theta}) &amp;=\mathbb{E}_{\pi}\left[\sum_{a} \pi\left(a | S_{t}, \boldsymbol{\theta}\right) q_{\pi}\left(S_{t}, a\right) \frac{\nabla \pi\left(a | S_{t}, \boldsymbol{\theta}\right)}{\pi\left(a | S_{t}, \boldsymbol{\theta}\right)}\right] \\ &amp;=\mathbb{E}_{\pi}\left[q_{\pi}\left(S_{t}, A_{t}\right) \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}\right] \\ &amp;=\mathbb{E}_{\pi}\left[G_{t} \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}\right] \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} \nabla J(\boldsymbol{\theta}) &=\mathbb{E}_{\pi}\left[\sum_{a} \pi\left(a | S_{t}, \boldsymbol{\theta}\right) q_{\pi}\left(S_{t}, a\right) \frac{\nabla \pi\left(a | S_{t}, \boldsymbol{\theta}\right)}{\pi\left(a | S_{t}, \boldsymbol{\theta}\right)}\right] \\ &=\mathbb{E}_{\pi}\left[q_{\pi}\left(S_{t}, A_{t}\right) \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}\right] \\ &=\mathbb{E}_{\pi}\left[G_{t} \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}\right] \end{aligned}
</script>
</div>
<p>上面第一步既是将随机变量 a 替换为样本 <span><span class="MathJax_Preview">A_t</span><script type="math/tex">A_t</script></span> ，第二步是因为 <span><span class="MathJax_Preview">\mathbb{E}_{\pi}\left[G_{t} | S_{t}, A_{t}\right]=q_{\pi}\left(S_{t}, A_{t}\right)</span><script type="math/tex">\mathbb{E}_{\pi}\left[G_{t} | S_{t}, A_{t}\right]=q_{\pi}\left(S_{t}, A_{t}\right)</script></span> 。这样就得到了 REINFORCE update：</p>
<div>
<div class="MathJax_Preview">
\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alpha G_{t} \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}
</div>
<script type="math/tex; mode=display">
\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alpha G_{t} \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}
</script>
</div>
<p>这样就能通过简单的采样来代替梯度。直观上也很好理解，每次的增量正比于 <span><span class="MathJax_Preview">G_t</span><script type="math/tex">G_t</script></span> 乘上一个向量，这个向量即为参数空间中，那些能够增加 <span><span class="MathJax_Preview">A_t</span><script type="math/tex">A_t</script></span> 在 <span><span class="MathJax_Preview">S_t</span><script type="math/tex">S_t</script></span> 下被选中概率的参数的<strong>方向</strong>。于是如果 reward 较好，<span><span class="MathJax_Preview">A_t</span><script type="math/tex">A_t</script></span> 对应的参数方向上就会得到较大幅度的更新，促进未来再被选中的概率，反之如果 reward 较差，更新的幅度就会变小，相对不如其他 action ，未来被选中的概率就会降低。</p>
<p>由于 REINFORCE 算法使用了完整的返回值 <span><span class="MathJax_Preview">G_t</span><script type="math/tex">G_t</script></span> ，因此属于蒙特卡罗算法，只适用于 episodic 形式。</p>
<p><img alt="" src="../imgs/RLAI_13/REINFORCE_algorighm.png" /></p>
<p>注意最后一行用 <span><span class="MathJax_Preview">\nabla \ln \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)</span><script type="math/tex">\nabla \ln \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)</script></span> 来代替 <span><span class="MathJax_Preview">\frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}</span><script type="math/tex">\frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}</script></span> ，这是利用了 <span><span class="MathJax_Preview">\nabla \ln x=\frac{\nabla x}{x}</span><script type="math/tex">\nabla \ln x=\frac{\nabla x}{x}</script></span> 的简单性质。</p>
<h2 id="134-reinforce-with-baseline"><strong>13.4 REINFORCE with Baseline</strong><a class="headerlink" href="#134-reinforce-with-baseline" title="Permanent link">&para;</a></h2>
<p>Policy gradient theorem 原本的形式为：</p>
<div>
<div class="MathJax_Preview">
\nabla J(\boldsymbol{\theta}) \propto \sum_{s} \mu(s) \sum_{a} q_{\pi}(s, a) \nabla \pi(a | s, \boldsymbol{\theta})
</div>
<script type="math/tex; mode=display">
\nabla J(\boldsymbol{\theta}) \propto \sum_{s} \mu(s) \sum_{a} q_{\pi}(s, a) \nabla \pi(a | s, \boldsymbol{\theta})
</script>
</div>
<p>考虑加入一个任意的 baseline <span><span class="MathJax_Preview">b(s)</span><script type="math/tex">b(s)</script></span> ：</p>
<div>
<div class="MathJax_Preview">
\nabla J(\boldsymbol{\theta}) \propto \sum_{s} \mu(s) \sum_{a}\left(q_{\pi}(s, a)-b(s)\right) \nabla \pi(a | s, \boldsymbol{\theta})
</div>
<script type="math/tex; mode=display">
\nabla J(\boldsymbol{\theta}) \propto \sum_{s} \mu(s) \sum_{a}\left(q_{\pi}(s, a)-b(s)\right) \nabla \pi(a | s, \boldsymbol{\theta})
</script>
</div>
<p><span><span class="MathJax_Preview">b(s)</span><script type="math/tex">b(s)</script></span> 可以是任意函数或随机变量，只要不和 a 相关即可。这样的改动显然是合理的，因为</p>
<div>
<div class="MathJax_Preview">
\sum_{a} b(s) \nabla \pi(a | s, \boldsymbol{\theta})=b(s) \nabla \sum_{a} \pi(a | s, \boldsymbol{\theta})=b(s) \nabla 1=0
</div>
<script type="math/tex; mode=display">
\sum_{a} b(s) \nabla \pi(a | s, \boldsymbol{\theta})=b(s) \nabla \sum_{a} \pi(a | s, \boldsymbol{\theta})=b(s) \nabla 1=0
</script>
</div>
<p>这样便能同理得到带 baseline 的 REINFORCE 更新式：</p>
<div>
<div class="MathJax_Preview">
\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alpha\left(G_{t}-b\left(S_{t}\right)\right) \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}
</div>
<script type="math/tex; mode=display">
\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alpha\left(G_{t}-b\left(S_{t}\right)\right) \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}
</script>
</div>
<p>加入 baseline 不会对期望有任何影响，但能够减小方差。常用的选择是将 <span><span class="MathJax_Preview">\hat{v}\left(S_{t}, \mathbf{w}\right)</span><script type="math/tex">\hat{v}\left(S_{t}, \mathbf{w}\right)</script></span> 作为 baseline ，好处是无需做别的计算，利用现有的量就能同时更新 <span><span class="MathJax_Preview">\boldsymbol \theta</span><script type="math/tex">\boldsymbol \theta</script></span> 和 <span><span class="MathJax_Preview">\boldsymbol w</span><script type="math/tex">\boldsymbol w</script></span> 。具体算法如下：</p>
<p><img alt="" src="../imgs/RLAI_13/REINFORCE_algorighm_baseline.png" /></p>
<h2 id="135-actorcritic-methods"><strong>13.5 Actor–Critic Methods</strong><a class="headerlink" href="#135-actorcritic-methods" title="Permanent link">&para;</a></h2>
<p>最开头提到过，同时学习 policy 和 value function 的算法是『actor-critic 算法』，尽管上一节的 REINFORCE-with-baseline 算法同时学习了 policy 和 value function ，但并不认为它是 actor-critic 算法，因为这个 value function 仅仅是用作 baseline ，而没起到 critic 的用处，具体而言，即是它没有用来做 bootstrapping（指通过状态估计值序列来更新状态估计值。例如用 <span><span class="MathJax_Preview">R_{t+1}+\gamma \hat{v}\left(S_{t+1}, \mathbf{w}\right)-\hat{v}\left(S_{t}, \mathbf{w}\right)</span><script type="math/tex">R_{t+1}+\gamma \hat{v}\left(S_{t+1}, \mathbf{w}\right)-\hat{v}\left(S_{t}, \mathbf{w}\right)</script></span> 更新就是 bootstrapping，而用 <span><span class="MathJax_Preview">G_t -\hat{v}\left(S_{t}, \mathbf{w}\right)</span><script type="math/tex">G_t -\hat{v}\left(S_{t}, \mathbf{w}\right)</script></span> 更新则不是）。</p>
<p>这样区分是有意义的，因为只有通过 bootstrapping 才能引入 bias ，以及对函数近似效果的渐进依赖。前面章节有提到过，通过 bootstrapping 引入的 bias 以及对状态表示的依赖都是有益的，因为它能够减小方差，并且加速学习。而上一节的 REINFORCE-with-baseline 算法由于是 unbiased 的，容易渐进收敛到局部最优解，同时由于是 MC 方法，方差较大，学习速度较慢，且不太适合处理在线学习/连续型问题。之前章节介绍的 TD 方法恰好能够解决上面所有的缺点，所以引出了下面的 actor–critic methods with a bootstrapping critic 。</p>
<p>首先考虑 one-step actor-critic methods ，将 full return 替换为 one-step return:</p>
<div>
<div class="MathJax_Preview">
\begin{aligned} \boldsymbol{\theta}_{t+1} &amp; \doteq \boldsymbol{\theta}_{t}+\alpha\left(G_{t : t+1}-\hat{v}\left(S_{t}, \mathbf{w}\right)\right) \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)} \\ &amp;=\boldsymbol{\theta}_{t}+\alpha\left(R_{t+1}+\gamma \hat{v}\left(S_{t+1}, \mathbf{w}\right)-\hat{v}\left(S_{t}, \mathbf{w}\right)\right) \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)} \\ &amp;=\boldsymbol{\theta}_{t}+\alpha \delta_{t} \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)} \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} \boldsymbol{\theta}_{t+1} & \doteq \boldsymbol{\theta}_{t}+\alpha\left(G_{t : t+1}-\hat{v}\left(S_{t}, \mathbf{w}\right)\right) \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)} \\ &=\boldsymbol{\theta}_{t}+\alpha\left(R_{t+1}+\gamma \hat{v}\left(S_{t+1}, \mathbf{w}\right)-\hat{v}\left(S_{t}, \mathbf{w}\right)\right) \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)} \\ &=\boldsymbol{\theta}_{t}+\alpha \delta_{t} \frac{\nabla \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)} \end{aligned}
</script>
</div>
<p>下面是算法伪码：</p>
<p><img alt="" src="../imgs/RLAI_13/one-step-actor-critic.png" /></p>
<h2 id="136-policy-gradient-for-continuing-problems"><strong>13.6 Policy Gradient for Continuing Problems</strong><a class="headerlink" href="#136-policy-gradient-for-continuing-problems" title="Permanent link">&para;</a></h2>
<p>在第十章中，对于没有 episode 界限的连续型问题，定义了平均回报率：</p>
<div>
<div class="MathJax_Preview">
\begin{aligned} J(\boldsymbol{\theta}) \doteq r(\pi) &amp; \doteq \lim _{h \rightarrow \infty} \frac{1}{h} \sum_{t=1}^{h} \mathbb{E}\left[R_{t} | S_{0}, A_{0 : t-1} \sim \pi\right] \\ &amp;=\lim _{t \rightarrow \infty} \mathbb{E}\left[R_{t} | S_{0}, A_{0 : t-1} \sim \pi\right] \\ &amp;=\sum_{s} \mu(s) \sum_{a} \pi(a | s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right) r \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} J(\boldsymbol{\theta}) \doteq r(\pi) & \doteq \lim _{h \rightarrow \infty} \frac{1}{h} \sum_{t=1}^{h} \mathbb{E}\left[R_{t} | S_{0}, A_{0 : t-1} \sim \pi\right] \\ &=\lim _{t \rightarrow \infty} \mathbb{E}\left[R_{t} | S_{0}, A_{0 : t-1} \sim \pi\right] \\ &=\sum_{s} \mu(s) \sum_{a} \pi(a | s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right) r \end{aligned}
</script>
</div>
<p>其中 <span><span class="MathJax_Preview">\mu(s) \doteq \lim _{t \rightarrow \infty} \operatorname{Pr}\left\{S_{t}=s | A_{0 : t} \sim \pi\right\}</span><script type="math/tex">\mu(s) \doteq \lim _{t \rightarrow \infty} \operatorname{Pr}\left\{S_{t}=s | A_{0 : t} \sim \pi\right\}</script></span> 。</p>
<p>此时</p>
<div>
<div class="MathJax_Preview">
G_{t} \doteq R_{t+1}-r(\pi)+R_{t+2}-r(\pi)+R_{t+3}-r(\pi)+\cdots
</div>
<script type="math/tex; mode=display">
G_{t} \doteq R_{t+1}-r(\pi)+R_{t+2}-r(\pi)+R_{t+3}-r(\pi)+\cdots
</script>
</div>
<p>使用上面在连续型问题下定义的 return 值，原先 episodic 下的 Policy Gradient Theorem 便可拓展到连续型问题下了。</p>
<p>下面给出连续型问题下 Policy Gradient Theorem 的证明</p>
<div>
<div class="MathJax_Preview">
\begin{aligned} \nabla v_{\pi}(s) &amp;=\nabla\left[\sum_{a} \pi(a | s) q_{\pi}(s, a)\right], \quad \text { for all } s \in \mathcal{S} \\ &amp;=\sum_{a}\left[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \nabla q_{\pi}(s, a)\right]\\ &amp;=\sum_{a}\Bigg[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \nabla \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left(r-r(\boldsymbol{\theta})+v_{\pi}\left(s^{\prime}\right)\right)\Bigg] \\ &amp;=\sum_{a}\Bigg[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s)\left[-\nabla r(\boldsymbol{\theta})+\sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla v_{\pi}\left(s^{\prime}\right)\right]\Bigg] \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} \nabla v_{\pi}(s) &=\nabla\left[\sum_{a} \pi(a | s) q_{\pi}(s, a)\right], \quad \text { for all } s \in \mathcal{S} \\ &=\sum_{a}\left[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \nabla q_{\pi}(s, a)\right]\\ &=\sum_{a}\Bigg[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \nabla \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left(r-r(\boldsymbol{\theta})+v_{\pi}\left(s^{\prime}\right)\right)\Bigg] \\ &=\sum_{a}\Bigg[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s)\left[-\nabla r(\boldsymbol{\theta})+\sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla v_{\pi}\left(s^{\prime}\right)\right]\Bigg] \end{aligned}
</script>
</div>
<p>整理可得</p>
<div>
<div class="MathJax_Preview">
\nabla r(\boldsymbol{\theta})=\sum_{a}\left[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla v_{\pi}\left(s^{\prime}\right)\right]-\nabla v_{\pi}(s)
</div>
<script type="math/tex; mode=display">
\nabla r(\boldsymbol{\theta})=\sum_{a}\left[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla v_{\pi}\left(s^{\prime}\right)\right]-\nabla v_{\pi}(s)
</script>
</div>
<p>注意到等式坐标即为 <span><span class="MathJax_Preview">\nabla J(\boldsymbol{\theta})</span><script type="math/tex">\nabla J(\boldsymbol{\theta})</script></span> ，与 s 无关，故等式右边也与 s 无关，且由于 <span><span class="MathJax_Preview">\sum_{s} \mu(s)=1</span><script type="math/tex">\sum_{s} \mu(s)=1</script></span> ，可得</p>
<div>
<div class="MathJax_Preview">
\begin{aligned} \nabla J(\boldsymbol{\theta})=&amp; \sum_{s} \mu(s)\Bigg(\sum_{a}\left[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla v_{\pi}\left(s^{\prime}\right)\right]-\nabla v_{\pi}(s)\Bigg) \\=&amp; \sum_{s} \mu(s) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \\ &amp;+\sum_{s} \mu(s) \sum_{a} \pi(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla v_{\pi}\left(s^{\prime}\right)-\sum_{s} \mu(s) \nabla v_{\pi}(s) \\
=&amp; \sum_{s} \mu(s) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \\ &amp;+\sum_{s^{\prime}} \underbrace{\sum_{s} \mu(s) \sum_{a} \pi(a | s) p\left(s^{\prime} | s, a\right)}_{\mu(s^{\prime})} \nabla v_{\pi}\left(s^{\prime}\right)-\sum_{s} \mu(s) \nabla v_{\pi}(s)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} \nabla J(\boldsymbol{\theta})=& \sum_{s} \mu(s)\Bigg(\sum_{a}\left[\nabla \pi(a | s) q_{\pi}(s, a)+\pi(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla v_{\pi}\left(s^{\prime}\right)\right]-\nabla v_{\pi}(s)\Bigg) \\=& \sum_{s} \mu(s) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \\ &+\sum_{s} \mu(s) \sum_{a} \pi(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla v_{\pi}\left(s^{\prime}\right)-\sum_{s} \mu(s) \nabla v_{\pi}(s) \\
=& \sum_{s} \mu(s) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a) \\ &+\sum_{s^{\prime}} \underbrace{\sum_{s} \mu(s) \sum_{a} \pi(a | s) p\left(s^{\prime} | s, a\right)}_{\mu(s^{\prime})} \nabla v_{\pi}\left(s^{\prime}\right)-\sum_{s} \mu(s) \nabla v_{\pi}(s)
\end{aligned}
</script>
</div>
<p>整理即得</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
\nabla J(\boldsymbol{\theta})=&amp; \sum_{s} \mu(s) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a)+\sum_{s^{\prime}} \mu\left(s^{\prime}\right) \nabla v_{\pi}\left(s^{\prime}\right)-\sum_{s} \mu(s) \nabla v_{\pi}(s) \\=&amp; \sum_{s} \mu(s) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla J(\boldsymbol{\theta})=& \sum_{s} \mu(s) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a)+\sum_{s^{\prime}} \mu\left(s^{\prime}\right) \nabla v_{\pi}\left(s^{\prime}\right)-\sum_{s} \mu(s) \nabla v_{\pi}(s) \\=& \sum_{s} \mu(s) \sum_{a} \nabla \pi(a | s) q_{\pi}(s, a)
\end{aligned}
</script>
</div>
<h2 id="137-policy-parameterization-for-continuous-actions"><strong>13.7 Policy Parameterization for Continuous Actions</strong><a class="headerlink" href="#137-policy-parameterization-for-continuous-actions" title="Permanent link">&para;</a></h2>
<p>Policy-based methods 为较大 action 空间的问题提供了实用的处理方法，甚至对于连续型问题这种有着无穷种 action 的情况也没问题，它并不去计算某个具体 action 的概率值，而是直接去学习概率分布。例如，假设 action 集合是一些实数，并且来自一个高斯分布，其概率分布便可写作</p>
<div>
<div class="MathJax_Preview">
\pi(a | s, \boldsymbol{\theta}) \doteq \frac{1}{\sigma(s, \boldsymbol{\theta}) \sqrt{2 \pi}} \exp \left(-\frac{(a-\mu(s, \boldsymbol{\theta}))^{2}}{2 \sigma(s, \boldsymbol{\theta})^{2}}\right)
</div>
<script type="math/tex; mode=display">
\pi(a | s, \boldsymbol{\theta}) \doteq \frac{1}{\sigma(s, \boldsymbol{\theta}) \sqrt{2 \pi}} \exp \left(-\frac{(a-\mu(s, \boldsymbol{\theta}))^{2}}{2 \sigma(s, \boldsymbol{\theta})^{2}}\right)
</script>
</div>
<p>其中 <span><span class="MathJax_Preview">\mu : \mathcal{S} \times \mathbb{R}^{d^{\prime}} \rightarrow \mathbb{R}, \sigma : \mathcal{S} \times \mathbb{R}^{d^{\prime}} \rightarrow \mathbb{R}^{+}</span><script type="math/tex">\mu : \mathcal{S} \times \mathbb{R}^{d^{\prime}} \rightarrow \mathbb{R}, \sigma : \mathcal{S} \times \mathbb{R}^{d^{\prime}} \rightarrow \mathbb{R}^{+}</script></span> 是参数化的近似函数。策略参数 <span><span class="MathJax_Preview">\boldsymbol{\theta}</span><script type="math/tex">\boldsymbol{\theta}</script></span> 由两部分组成：<span><span class="MathJax_Preview">\boldsymbol{\theta}=\left[\boldsymbol{\theta}_{\mu}, \boldsymbol{\theta}_{\sigma}\right]^{\top}</span><script type="math/tex">\boldsymbol{\theta}=\left[\boldsymbol{\theta}_{\mu}, \boldsymbol{\theta}_{\sigma}\right]^{\top}</script></span> ，第一部分用于均值的近似，第二部分用于标准差的近似：</p>
<div>
<div class="MathJax_Preview">
\mu(s, \boldsymbol{\theta}) \doteq \boldsymbol{\theta}_{\mu}^{\top} \mathbf{x}_{\mu}(s) \quad \text { and } \quad \sigma(s, \boldsymbol{\theta}) \doteq \exp \left(\boldsymbol{\theta}_{\sigma}^{\top} \mathbf{x}_{\sigma}(s)\right)
</div>
<script type="math/tex; mode=display">
\mu(s, \boldsymbol{\theta}) \doteq \boldsymbol{\theta}_{\mu}^{\top} \mathbf{x}_{\mu}(s) \quad \text { and } \quad \sigma(s, \boldsymbol{\theta}) \doteq \exp \left(\boldsymbol{\theta}_{\sigma}^{\top} \mathbf{x}_{\sigma}(s)\right)
</script>
</div>
<p>这样，便组成了完整的连续型问题下 Policy 参数化算法。</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../RLAI_12/" title="Chapter 12" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Chapter 12
              </span>
            </div>
          </a>
        
        
          <a href="../MCTS_introduction/" title="MCTS" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                MCTS
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2016-2020 ZHANGWP
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
  <div class="md-footer-social">
    <link rel="stylesheet" href="../../../../assets/fonts/font-awesome.css">
    
      <a href="https://github.com/zawnpn" class="md-footer-social__link fa fa-github"></a>
    
      <a href="https://twitter.com/zawnpn" class="md-footer-social__link fa fa-twitter"></a>
    
      <a href="https://psnprofiles.com/zawnpn" class="md-footer-social__link fa fa-trophy"></a>
    
      <a href="https://steamcommunity.com/id/zawnpn/" class="md-footer-social__link fa fa-steam"></a>
    
      <a href="https://www.zhihu.com/people/zhangwanpeng" class="md-footer-social__link fa fa-globe"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../../assets/javascripts/application.583bbe55.js"></script>
      
        
        
          
          <script src="../../../../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
                <script src="../../../../assets/javascripts/lunr/tinyseg.js"></script>
              
              
                <script src="../../../../assets/javascripts/lunr/lunr.jp.js"></script>
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:"../../../.."}})</script>
      
        <script src="../../../../assets/extra.js"></script>
      
        <script src="//cdn.bootcss.com/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_SVG"></script>
      
    
    
      
    
  </body>
</html>