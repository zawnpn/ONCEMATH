<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Welcome to zhangwp's blog."><link href=https://www.zhangwp.com/notes/reinforcement-learning/notes/RLAI_2/ rel=canonical><meta name=author content=zawnpn><link rel="shortcut icon" href=../../../../assets/images/favicon.svg><meta name=generator content="mkdocs-1.1.2, mkdocs-material-5.4.0"><title>Chapter 2 - ZHANGWP</title><link rel=stylesheet href=../../../../assets/stylesheets/main.fe0cca5b.min.css><link rel=stylesheet href=../../../../assets/stylesheets/palette.a46bcfb3.min.css><meta name=theme-color content=#546e7a></head> <body dir=ltr data-md-color-scheme data-md-color-primary=blue-grey data-md-color-accent> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#- class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid" aria-label=Header> <a href=https://www.zhangwp.com title=ZHANGWP class="md-header-nav__button md-logo" aria-label=ZHANGWP> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><path d="M257.981 272.971L63.638 467.314c-9.373 9.373-24.569 9.373-33.941 0L7.029 444.647c-9.357-9.357-9.375-24.522-.04-33.901L161.011 256 6.99 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L257.981 239.03c9.373 9.372 9.373 24.568 0 33.941zM640 456v-32c0-13.255-10.745-24-24-24H312c-13.255 0-24 10.745-24 24v32c0 13.255 10.745 24 24 24h304c13.255 0 24-10.745 24-24z"/></svg> </a> <label class="md-header-nav__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header-nav__title data-md-component=header-title> <div class=md-header-nav__ellipsis> <span class="md-header-nav__topic md-ellipsis"> ZHANGWP </span> <span class="md-header-nav__topic md-ellipsis"> Chapter 2 </span> </div> </div> <label class="md-header-nav__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query data-md-state=active> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <button type=reset class="md-search__icon md-icon" aria-label=Clear data-md-component=search-reset tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header-nav__source> <a href=https://github.com/zawnpn/ZHANGWP/ title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class="md-tabs md-tabs--active" aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../ class="md-tabs__link md-tabs__link--active"> Notes </a> </li> <li class=md-tabs__item> <a href=../../../../tips/ class=md-tabs__link> Tips </a> </li> <li class=md-tabs__item> <a href=../../../../share/ class=md-tabs__link> Share </a> </li> <li class=md-tabs__item> <a href=../../../../statements/ class=md-tabs__link> Statements </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=https://www.zhangwp.com title=ZHANGWP class="md-nav__button md-logo" aria-label=ZHANGWP> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><path d="M257.981 272.971L63.638 467.314c-9.373 9.373-24.569 9.373-33.941 0L7.029 444.647c-9.357-9.357-9.375-24.522-.04-33.901L161.011 256 6.99 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L257.981 239.03c9.373 9.372 9.373 24.568 0 33.941zM640 456v-32c0-13.255-10.745-24-24-24H312c-13.255 0-24 10.745-24 24v32c0 13.255 10.745 24 24 24h304c13.255 0 24-10.745 24-24z"/></svg> </a> ZHANGWP </label> <div class=md-nav__source> <a href=https://github.com/zawnpn/ZHANGWP/ title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1 type=checkbox id=nav-1> <label class=md-nav__link for=nav-1> Home <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Home data-md-level=1> <label class=md-nav__title for=nav-1> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../.. title=Home class=md-nav__link> Home </a> </li> <li class=md-nav__item> <a href=../../../../links/ title=Links class=md-nav__link> Links </a> </li> <li class=md-nav__item> <a href=../../../../donates/ title=Donate class=md-nav__link> Donate </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2 type=checkbox id=nav-2 checked> <label class=md-nav__link for=nav-2> Notes <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Notes data-md-level=1> <label class=md-nav__title for=nav-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Notes </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../ title=Index class=md-nav__link> Index </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2-2 type=checkbox id=nav-2-2 checked> <label class=md-nav__link for=nav-2-2> Reinforcement <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Reinforcement data-md-level=2> <label class=md-nav__title for=nav-2-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Reinforcement </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2-2-1 type=checkbox id=nav-2-2-1 checked> <label class=md-nav__link for=nav-2-2-1> Reinforcement Learning An Introduction <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Reinforcement Learning An Introduction" data-md-level=3> <label class=md-nav__title for=nav-2-2-1> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Reinforcement Learning An Introduction </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Chapter 2 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 9h14V7H3v2m0 4h14v-2H3v2m0 4h14v-2H3v2m16 0h2v-2h-2v2m0-10v2h2V7h-2m0 6h2v-2h-2v2z"/></svg> </span> </label> <a href=./ title="Chapter 2" class="md-nav__link md-nav__link--active"> Chapter 2 </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#21-a-k-armed-bandit-problem class=md-nav__link> 2.1 A k-armed Bandit Problem </a> <nav class=md-nav aria-label="2.1 A k-armed Bandit Problem"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#background class=md-nav__link> Background </a> </li> <li class=md-nav__item> <a href=#representation-of-the-problem class=md-nav__link> Representation of the problem </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#22-action-value-methods class=md-nav__link> 2.2 Action-value Methods </a> <nav class=md-nav aria-label="2.2 Action-value Methods"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#sample-average class=md-nav__link> Sample-Average </a> </li> <li class=md-nav__item> <a href=#greedy-action class=md-nav__link> greedy action </a> </li> <li class=md-nav__item> <a href=#-greedy-action class=md-nav__link> ε-greedy action </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#23-the-10-armed-testbed class=md-nav__link> 2.3 The 10-armed Testbed </a> <nav class=md-nav aria-label="2.3 The 10-armed Testbed"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#background_1 class=md-nav__link> Background </a> </li> <li class=md-nav__item> <a href=#conclusion class=md-nav__link> Conclusion </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#24-incremental-implementation class=md-nav__link> 2.4 Incremental Implementation </a> <nav class=md-nav aria-label="2.4 Incremental Implementation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#optimization class=md-nav__link> Optimization </a> </li> <li class=md-nav__item> <a href=#pseuducode class=md-nav__link> Pseuducode </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#25-tracking-a-nonstationary-problem class=md-nav__link> 2.5 Tracking a Nonstationary Problem </a> </li> <li class=md-nav__item> <a href=#26-optimistic-initial-values class=md-nav__link> 2.6 Optimistic Initial Values </a> </li> <li class=md-nav__item> <a href=#27-upper-confidence-bound-action-selection class=md-nav__link> 2.7 Upper-Confidence-Bound Action Selection </a> </li> <li class=md-nav__item> <a href=#28-gradient-bandit-algorithms class=md-nav__link> 2.8 Gradient Bandit Algorithms </a> <nav class=md-nav aria-label="2.8 Gradient Bandit Algorithms"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#introduction class=md-nav__link> Introduction </a> </li> <li class=md-nav__item> <a href=#proof class=md-nav__link> Proof </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#29-associative-search-contextual-bandit class=md-nav__link> 2.9 Associative Search (Contextual Bandit) </a> <nav class=md-nav aria-label="2.9 Associative Search (Contextual Bandit)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#background_2 class=md-nav__link> Background </a> </li> <li class=md-nav__item> <a href=#full-reinforcement-learning-problem class=md-nav__link> Full Reinforcement Learning Problem </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../RLAI_3/ title="Chapter 3" class=md-nav__link> Chapter 3 </a> </li> <li class=md-nav__item> <a href=../RLAI_4/ title="Chapter 4" class=md-nav__link> Chapter 4 </a> </li> <li class=md-nav__item> <a href=../RLAI_5/ title="Chapter 5" class=md-nav__link> Chapter 5 </a> </li> <li class=md-nav__item> <a href=../RLAI_6/ title="Chapter 6" class=md-nav__link> Chapter 6 </a> </li> <li class=md-nav__item> <a href=../RLAI_7/ title="Chapter 7" class=md-nav__link> Chapter 7 </a> </li> <li class=md-nav__item> <a href=../RLAI_8/ title="Chapter 8" class=md-nav__link> Chapter 8 </a> </li> <li class=md-nav__item> <a href=../RLAI_9/ title="Chapter 9" class=md-nav__link> Chapter 9 </a> </li> <li class=md-nav__item> <a href=../RLAI_10/ title="Chapter 10" class=md-nav__link> Chapter 10 </a> </li> <li class=md-nav__item> <a href=../RLAI_11/ title="Chapter 11" class=md-nav__link> Chapter 11 </a> </li> <li class=md-nav__item> <a href=../RLAI_12/ title="Chapter 12" class=md-nav__link> Chapter 12 </a> </li> <li class=md-nav__item> <a href=../RLAI_13/ title="Chapter 13" class=md-nav__link> Chapter 13 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2-2-2 type=checkbox id=nav-2-2-2> <label class=md-nav__link for=nav-2-2-2> Some Introduction <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Some Introduction" data-md-level=3> <label class=md-nav__title for=nav-2-2-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Some Introduction </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../MCTS_introduction/ title=MCTS class=md-nav__link> MCTS </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> Tips <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Tips data-md-level=1> <label class=md-nav__title for=nav-3> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Tips </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../tips/ title=Tips class=md-nav__link> Tips </a> </li> <li class=md-nav__item> <a href=../../../../tips/to-do/ title="To Do" class=md-nav__link> To Do </a> </li> <li class=md-nav__item> <a href=../../../../tips/python/ title=Python class=md-nav__link> Python </a> </li> <li class=md-nav__item> <a href=../../../../tips/data-processing/ title="Data Processing" class=md-nav__link> Data Processing </a> </li> <li class=md-nav__item> <a href=../../../../tips/git/ title=Git class=md-nav__link> Git </a> </li> <li class=md-nav__item> <a href=../../../../tips/linux/ title=Linux class=md-nav__link> Linux </a> </li> <li class=md-nav__item> <a href=../../../../tips/win/ title=Windows class=md-nav__link> Windows </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4 type=checkbox id=nav-4> <label class=md-nav__link for=nav-4> Share <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Share data-md-level=1> <label class=md-nav__title for=nav-4> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Share </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/ title=Index class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../../share/blog-history/ title=博客历史 class=md-nav__link> 博客历史 </a> </li> <li class=md-nav__item> <a href=../../../../share/game-log/ title=Game-Log class=md-nav__link> Game-Log </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-4 type=checkbox id=nav-4-4> <label class=md-nav__link for=nav-4-4> NKU-Toolkit <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=NKU-Toolkit data-md-level=2> <label class=md-nav__title for=nav-4-4> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> NKU-Toolkit </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/nku-eamis/ title=NKU-EAMIS工具 class=md-nav__link> NKU-EAMIS工具 </a> </li> <li class=md-nav__item> <a href=../../../../share/nku-sms-rss/ title=NKU-SMS-RSS class=md-nav__link> NKU-SMS-RSS </a> </li> <li class=md-nav__item> <a href=../../../../share/eamis-miniapp/ title=NKU-EAMIS_MiniApp(南开大学教务助手小程序) class=md-nav__link> NKU-EAMIS_MiniApp(南开大学教务助手小程序) </a> </li> <li class=md-nav__item> <a href=../../../../share/eamis-workflow/ title="NKU-EAMIS for iOS(Workflow)" class=md-nav__link> NKU-EAMIS for iOS(Workflow) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-5 type=checkbox id=nav-4-5> <label class=md-nav__link for=nav-4-5> Steam-Toolkit <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Steam-Toolkit data-md-level=2> <label class=md-nav__title for=nav-4-5> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Steam-Toolkit </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/steam-market-price-bot/ title=Steam市场比价爬虫 class=md-nav__link> Steam市场比价爬虫 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-6 type=checkbox id=nav-4-6> <label class=md-nav__link for=nav-4-6> 数学建模 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=数学建模 data-md-level=2> <label class=md-nav__title for=nav-4-6> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 数学建模 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/2017-mcm-icm/ title="2017美赛参赛整理(Problem D)" class=md-nav__link> 2017美赛参赛整理(Problem D) </a> </li> <li class=md-nav__item> <a href=../../../../share/2016-guosai/ title=2016数学建模国赛 class=md-nav__link> 2016数学建模国赛 </a> </li> <li class=md-nav__item> <a href=../../../../share/math-model-szb/ title=数学建模之2016深圳杯——初次尝试 class=md-nav__link> 数学建模之2016深圳杯——初次尝试 </a> </li> <li class=md-nav__item> <a href=../../../../share/polygon-to-ellipse/ title=随机多边形转化为椭圆的过程研究 class=md-nav__link> 随机多边形转化为椭圆的过程研究 </a> </li> <li class=md-nav__item> <a href=../../../../share/FFT-GPU-Accel/ title=FFT-GPU-Accel class=md-nav__link> FFT-GPU-Accel </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7 type=checkbox id=nav-4-7> <label class=md-nav__link for=nav-4-7> NKU 数院试题整理 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="NKU 数院试题整理" data-md-level=2> <label class=md-nav__title for=nav-4-7> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> NKU 数院试题整理 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/nku-sms-exams/ title=汇总 class=md-nav__link> 汇总 </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7-2 type=checkbox id=nav-4-7-2> <label class=md-nav__link for=nav-4-7-2> 分析 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=分析 data-md-level=3> <label class=md-nav__title for=nav-4-7-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 分析 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/functional-analysis-final/ title=2017-2018第一学期泛函分析期末考试 class=md-nav__link> 2017-2018第一学期泛函分析期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/real-variable-function/ title=2016-2017第二学期实变函数期末考试 class=md-nav__link> 2016-2017第二学期实变函数期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-3-final/ title=2016-2017第一学期数学分析3-3期末考试 class=md-nav__link> 2016-2017第一学期数学分析3-3期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/complex-analysis-final/ title=2016-2017第一学期复变函数期末考试 class=md-nav__link> 2016-2017第一学期复变函数期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-3-middle/ title=2016-2017第一学期数学分析3-3期中考试 class=md-nav__link> 2016-2017第一学期数学分析3-3期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-2-final/ title=2015-2016第二学期数学分析3-2期末考试（含解答） class=md-nav__link> 2015-2016第二学期数学分析3-2期末考试（含解答） </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-2-middle/ title=2015-2016第二学期数学分析3-2期中考试 class=md-nav__link> 2015-2016第二学期数学分析3-2期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-1-final/ title=2015-2016第一学期数学分析3-1期末考试 class=md-nav__link> 2015-2016第一学期数学分析3-1期末考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7-3 type=checkbox id=nav-4-7-3> <label class=md-nav__link for=nav-4-7-3> 代数 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=代数 data-md-level=3> <label class=md-nav__title for=nav-4-7-3> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 代数 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/abstract-algebra-final/ title=2016-2017第一学期抽象代数期末考试 class=md-nav__link> 2016-2017第一学期抽象代数期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/abstract-algebra-middle/ title=2016-2017第一学期抽象代数期中考试 class=md-nav__link> 2016-2017第一学期抽象代数期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/advanced-algebra-2-2-final/ title=2015-2016第二学期高等代数2-2期末考试 class=md-nav__link> 2015-2016第二学期高等代数2-2期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/advanced-algebra-2-2-middle/ title=2015-2016第二学期高等代数2-2期中考试 class=md-nav__link> 2015-2016第二学期高等代数2-2期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/advanced-algebra-2-1-final/ title=2015-2016第一学期高等代数2-1期末考试 class=md-nav__link> 2015-2016第一学期高等代数2-1期末考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7-4 type=checkbox id=nav-4-7-4> <label class=md-nav__link for=nav-4-7-4> 概率统计 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=概率统计 data-md-level=3> <label class=md-nav__title for=nav-4-7-4> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 概率统计 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/probability-final/ title=2016-2017第二学期概率论期末考试 class=md-nav__link> 2016-2017第二学期概率论期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/probability-middle/ title=2016-2017第二学期概率论期中考试 class=md-nav__link> 2016-2017第二学期概率论期中考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7-5 type=checkbox id=nav-4-7-5> <label class=md-nav__link for=nav-4-7-5> 微分方程 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=微分方程 data-md-level=3> <label class=md-nav__title for=nav-4-7-5> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 微分方程 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/PDE-final/ title=2017-2018第一学期数理方程期末考试 class=md-nav__link> 2017-2018第一学期数理方程期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/ODE-final/ title=2016-2017第一学期常微分方程期末考试 class=md-nav__link> 2016-2017第一学期常微分方程期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/ODE-middle/ title=2016-2017第一学期常微分方程期中考试 class=md-nav__link> 2016-2017第一学期常微分方程期中考试 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-8 type=checkbox id=nav-4-8> <label class=md-nav__link for=nav-4-8> Other <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Other data-md-level=2> <label class=md-nav__title for=nav-4-8> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Other </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/github-student-pack/ title="Student Developer Pack - GitHub Education" class=md-nav__link> Student Developer Pack - GitHub Education </a> </li> <li class=md-nav__item> <a href=../../../../share/my-postgraduate-share/ title="保研推免经验分享 - 数学系跨保 CS" class=md-nav__link> 保研推免经验分享 - 数学系跨保 CS </a> </li> <li class=md-nav__item> <a href=../../../../share/roc-fly/ title=鹏程万里 class=md-nav__link> 鹏程万里 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-5 type=checkbox id=nav-5> <label class=md-nav__link for=nav-5> Statements <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Statements data-md-level=1> <label class=md-nav__title for=nav-5> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Statements </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../statements/ title=Statements class=md-nav__link> Statements </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#21-a-k-armed-bandit-problem class=md-nav__link> 2.1 A k-armed Bandit Problem </a> <nav class=md-nav aria-label="2.1 A k-armed Bandit Problem"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#background class=md-nav__link> Background </a> </li> <li class=md-nav__item> <a href=#representation-of-the-problem class=md-nav__link> Representation of the problem </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#22-action-value-methods class=md-nav__link> 2.2 Action-value Methods </a> <nav class=md-nav aria-label="2.2 Action-value Methods"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#sample-average class=md-nav__link> Sample-Average </a> </li> <li class=md-nav__item> <a href=#greedy-action class=md-nav__link> greedy action </a> </li> <li class=md-nav__item> <a href=#-greedy-action class=md-nav__link> ε-greedy action </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#23-the-10-armed-testbed class=md-nav__link> 2.3 The 10-armed Testbed </a> <nav class=md-nav aria-label="2.3 The 10-armed Testbed"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#background_1 class=md-nav__link> Background </a> </li> <li class=md-nav__item> <a href=#conclusion class=md-nav__link> Conclusion </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#24-incremental-implementation class=md-nav__link> 2.4 Incremental Implementation </a> <nav class=md-nav aria-label="2.4 Incremental Implementation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#optimization class=md-nav__link> Optimization </a> </li> <li class=md-nav__item> <a href=#pseuducode class=md-nav__link> Pseuducode </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#25-tracking-a-nonstationary-problem class=md-nav__link> 2.5 Tracking a Nonstationary Problem </a> </li> <li class=md-nav__item> <a href=#26-optimistic-initial-values class=md-nav__link> 2.6 Optimistic Initial Values </a> </li> <li class=md-nav__item> <a href=#27-upper-confidence-bound-action-selection class=md-nav__link> 2.7 Upper-Confidence-Bound Action Selection </a> </li> <li class=md-nav__item> <a href=#28-gradient-bandit-algorithms class=md-nav__link> 2.8 Gradient Bandit Algorithms </a> <nav class=md-nav aria-label="2.8 Gradient Bandit Algorithms"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#introduction class=md-nav__link> Introduction </a> </li> <li class=md-nav__item> <a href=#proof class=md-nav__link> Proof </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#29-associative-search-contextual-bandit class=md-nav__link> 2.9 Associative Search (Contextual Bandit) </a> <nav class=md-nav aria-label="2.9 Associative Search (Contextual Bandit)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#background_2 class=md-nav__link> Background </a> </li> <li class=md-nav__item> <a href=#full-reinforcement-learning-problem class=md-nav__link> Full Reinforcement Learning Problem </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <a href=https://github.com/zawnpn/ZHANGWP/edit/master/docs/notes/reinforcement-learning/notes/RLAI_2.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg> </a> <h1 id=->强化学习导论（二）- 多臂老虎机<a class=headerlink href=#- title="Permanent link">&para;</a></h1> <p>本章主要针对“非关联性（nonassociative）”的简单场景来学习基础的强化学习方法。什么是“非关联性”呢？其实通过最后 2.9 节可以看出，非关联性在本章就是指<strong>无需考虑每一步行动之间的影响，以及环境对行动的影响</strong>。非关联性问题是一种很理想化的问题，研究这种问题对于现实中的实用性意义不大，但对于入门强化学习理论，是一个不错的背景载体。</p> <p>再说到强化学习，他与其他的机器学习方法最大的区别，也就是他自身的特点，在于强化学习重点关注<strong>评价性反馈（Evaluative Feedback）</strong>，而不是<strong>指导性反馈（Instructive Feedback）</strong>。</p> <ul> <li>评价性反馈：知道每一步 action 的好坏程度，但不知道这个 action 是否是最好/最差</li> <li>指导性反馈：直接得知最优 action</li> </ul> <p>指导性反馈多用在监督学习中，需要大量正确的先验知识/信息来给予“指导”，而在一些特殊背景下，无法得到监督性指导，但却有大量实时的评价性反馈，这时候就需要用到<strong>强化学习</strong>。</p> <h2 id=21-a-k-armed-bandit-problem>2.1 A k-armed Bandit Problem<a class=headerlink href=#21-a-k-armed-bandit-problem title="Permanent link">&para;</a></h2> <h3 id=background>Background<a class=headerlink href=#background title="Permanent link">&para;</a></h3> <p>问题的背景就是简化的 k 臂老虎机：</p> <ul> <li>每次在 k 个选项中做出一个选择，称之为一个 action</li> <li>每次根据玩家的 action 反馈一个“奖励值”，每种 action 对应的奖励值服从一个固定的概率分布（这个概率分布是我们从背后分析问题，也就是从上帝视角才能得知的，真正的玩家一开始根本不知道奖励值服从什么规律或者是否有规律，他需要通过“学习”来找到这一规律）</li> <li>玩家的目标在于使收获的奖励的<strong>累积值</strong>最大化</li> </ul> <h3 id=representation-of-the-problem>Representation of the problem<a class=headerlink href=#representation-of-the-problem title="Permanent link">&para;</a></h3> <div> <div class=MathJax_Preview>q_*(a)\doteq\mathbb{E}[R_t|A_t=a]</div> <script type="math/tex; mode=display">q_*(a)\doteq\mathbb{E}[R_t|A_t=a]</script> </div> <ul> <li><span><span class=MathJax_Preview>A_t</span><script type=math/tex>A_t</script></span>: 第 t 步做出的 action</li> <li><span><span class=MathJax_Preview>R_t</span><script type=math/tex>R_t</script></span>: 第 t 步行动后得到的回报值</li> <li><span><span class=MathJax_Preview>a</span><script type=math/tex>a</script></span>: 一个任意的行动</li> <li><span><span class=MathJax_Preview>q_*(a)</span><script type=math/tex>q_*(a)</script></span>: 行动 a 的理论期望值</li> </ul> <p>我们可以很自然地想到，如果玩家真的从上帝视角得知了这台老虎机的回报规律，也即是知道了每个行动 a 真正能得到理论期望回报<span><span class=MathJax_Preview>q_*(a)</span><script type=math/tex>q_*(a)</script></span>，那只需保持选择能够收获最大期望值的 action ，就能确保最大的总收益。所以，这个问题的目标，就是要去学习探索，取得关于 <span><span class=MathJax_Preview>q_*(a), \forall a</span><script type=math/tex>q_*(a), \forall a</script></span> 的信息。</p> <p>但是玩家一开始显然是不知道 <span><span class=MathJax_Preview>q_*(a)</span><script type=math/tex>q_*(a)</script></span> 的情况，所以他要建立一套自己对所有 <span><span class=MathJax_Preview>a</span><script type=math/tex>a</script></span> 的评估体系，即根据他目前拥有的知识，来估计/猜测当前第 t 步 <span><span class=MathJax_Preview>a</span><script type=math/tex>a</script></span> 的回报值 <span><span class=MathJax_Preview>Q_t(a)</span><script type=math/tex>Q_t(a)</script></span>: <span><span class=MathJax_Preview>Q_t(a)\approx q_*(a)</span><script type=math/tex>Q_t(a)\approx q_*(a)</script></span>。如何去估计呢？这个先不急，这正是后面长篇大论的东西，简言之，关键在于要有这么一套合适的评估体系。</p> <p>先假设玩家建立好了一套他认为合适的评估体系，那接下来该如何去根据<strong>评价性反馈</strong>来采取行动呢？这时候先要提到两个概念：</p> <ul> <li>利用（Exploiting）：采取贪心行动，也就是根据目前<strong>已掌握的信息</strong>来做当前最优选择</li> <li>探索（Exploring）：放弃贪心行动，去探索潜在的、有长远价值的信息</li> </ul> <p>Exploitation 对于每一步而言，是能尽量利用上当前已掌握知识的最佳策略，能确保回报玩家认知范围内的最佳奖励值；Exploration 则会去“试错”，去尝试一些信息量少的 action ，这些 action 之所以信息量少，是因为在玩家的评估体系中被认为是低回报 action 而很少被选中，从而收获到的信息少。不过这个低回报，既有可能是真的低回报，也有可能是被低估了，如果这个 action 事实上是一个很有价值的 action ，却因过分低估而被玩家放弃，是一件非常可惜的事情。为了避免这一情况，从长远意义上真正地最大化收益，就需要玩家适当地去探索、去试错。信息越多，做出的选择也越客观。</p> <h2 id=22-action-value-methods>2.2 Action-value Methods<a class=headerlink href=#22-action-value-methods title="Permanent link">&para;</a></h2> <h3 id=sample-average>Sample-Average<a class=headerlink href=#sample-average title="Permanent link">&para;</a></h3> <p>上一节提到，玩家需要建立一套合适的评估体系，这一节就会介绍一种最简单基础的方法。</p> <p>一个很自然的想法便是将过去得到过的奖励值取均值作为这一次对该 action 的评估</p> <div> <div class=MathJax_Preview>Q_{t}(a) \doteq \frac{\sum_{i=1}^{t-1}R_{i} \cdot \textbf{1}_{A_{i} = a}}{\sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a}}</div> <script type="math/tex; mode=display">Q_{t}(a) \doteq \frac{\sum_{i=1}^{t-1}R_{i} \cdot \textbf{1}_{A_{i} = a}}{\sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a}}</script> </div> <p>我们可以看出：</p> <ul> <li>如果 <span><span class=MathJax_Preview>\displaystyle \sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a} = 0</span><script type=math/tex>\displaystyle \sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a} = 0</script></span>, 分母为 0 该式无意义，这时候需要将 <span><span class=MathJax_Preview>Q_t(a)</span><script type=math/tex>Q_t(a)</script></span> 定义为一个默认值，比如 0</li> <li>如果 <span><span class=MathJax_Preview>\displaystyle \sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a}\rightarrow \infty</span><script type=math/tex>\displaystyle \sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a}\rightarrow \infty</script></span>, 根据<strong>大数定律</strong>, <span><span class=MathJax_Preview>Q_t(a) \rightarrow q_*(a)</span><script type=math/tex>Q_t(a) \rightarrow q_*(a)</script></span>，样本统计值收敛于理论值，达到了我们前面提到的情况——只要掌握了真实的<span><span class=MathJax_Preview>q_*(a)</span><script type=math/tex>q_*(a)</script></span>，必将能取得最优解。</li> </ul> <h3 id=greedy-action>greedy action<a class=headerlink href=#greedy-action title="Permanent link">&para;</a></h3> <p>而前面所提到的贪心行动，表述为数学语言即为</p> <div> <div class=MathJax_Preview>A_t \doteq \mathop{\arg\max}\limits_aQ_t(a)</div> <script type="math/tex; mode=display">A_t \doteq \mathop{\arg\max}\limits_aQ_t(a)</script> </div> <p>可以想象，纯贪心行动很有可能陷入局部最优解（最坏情况下，贪心行动甚至可能导致玩家从头到尾都在选择一个固定的非最优的 action），很难实现让每个 action 都能满足 <span><span class=MathJax_Preview>\displaystyle \sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a}\rightarrow \infty</span><script type=math/tex>\displaystyle \sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a}\rightarrow \infty</script></span> 。</p> <p>这时候，就需要去“探索（Exploring）”，牺牲一点眼前的利益，换来能带来长远价值的信息。只需对贪心策略稍作修改，我们就能做到这一点。</p> <h3 id=-greedy-action>ε-greedy action<a class=headerlink href=#-greedy-action title="Permanent link">&para;</a></h3> <blockquote> <p><strong>ε-greedy action:</strong> 以 1-ε 的概率采取贪心行动，ε 概率随机选择一个行动 <span><span class=MathJax_Preview>a</span><script type=math/tex>a</script></span> 。</p> </blockquote> <p>为什么这个 ε-greedy action 就要比单纯的 greedy action 策略好呢？我们来简单分析一下：</p> <ul> <li>显然可知， ε-greedy action 由于有随机探索的过程，必然能保证：当 <span><span class=MathJax_Preview>t \rightarrow \infty</span><script type=math/tex>t \rightarrow \infty</script></span>，就有 <span><span class=MathJax_Preview>\displaystyle \sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a}\rightarrow \infty</span><script type=math/tex>\displaystyle \sum_{i=1}^{t-1}\textbf{1}_{A_{i} = a}\rightarrow \infty</script></span>。这样正如前面已经分析过的，根据大数定律会有 <span><span class=MathJax_Preview>Q_t(a)\rightarrow q_*(a)</span><script type=math/tex>Q_t(a)\rightarrow q_*(a)</script></span>.</li> <li><span><span class=MathJax_Preview>\mathrm{Pr}\{A_t=\mathop{\arg\max}\limits_{a}Q_t(a)\} = 1-\varepsilon</span><script type=math/tex>\mathrm{Pr}\{A_t=\mathop{\arg\max}\limits_{a}Q_t(a)\} = 1-\varepsilon</script></span>，如果 ε 取得太大，就会过于注重探索，而没有充分利用好这些收获到的信息来增加我们的收益，对于我们想要最大化<strong>累积收益</strong>的目标是不利的，但如果取到一个合适的 ε ，便能兼顾信息探索和信息的<strong>充分利用</strong>。</li> </ul> <h2 id=23-the-10-armed-testbed>2.3 The 10-armed Testbed<a class=headerlink href=#23-the-10-armed-testbed title="Permanent link">&para;</a></h2> <p>这一节就是关于上面提到的方法进行 10-armed bandit 实验来测试效果。</p> <h3 id=background_1>Background<a class=headerlink href=#background_1 title="Permanent link">&para;</a></h3> <p>为了确保实验结果的准确性，总共随机生成了 2000 个 k-armed bandit 问题（k=10），然后针对每个问题，在其背景下都要进行 1000 步 action 的选择，最终针对这 2000 个独立的实验的结果来逐步取均值分析。</p> <div align=center><img src=../imgs/RLAI_2/10-armed.png width=450 alt=10-armed></div> <p>这个图需要好好理解一下，也要根据这张图好好再理解一下问题背景。其中，</p> <ul> <li>首先，需要理解的是，玩家每一步得到的奖励值 <span><span class=MathJax_Preview>R_t</span><script type=math/tex>R_t</script></span> ，是一个来自于对应的正态分布的随机值。举个例子，玩家在第 t 步选择 action 3 ，那么这一步老虎机返回给玩家的奖励值 <span><span class=MathJax_Preview>R_t</span><script type=math/tex>R_t</script></span> 就是一个服从正态分布 <span><span class=MathJax_Preview>\mathrm{N}(q_*(3),1)</span><script type=math/tex>\mathrm{N}(q_*(3),1)</script></span> 的随机值，这个值或高或低，但总体的趋势还是大概率为 <span><span class=MathJax_Preview>q_*(3)</span><script type=math/tex>q_*(3)</script></span> 附近的一个值。我们从上帝视角是知道这些值的，但是玩家并不知道这些情况，只能一步一步地收集信息，以此来猜测、学习这些奖励值的规律。</li> <li>然后，这些 <span><span class=MathJax_Preview>q_*(a)</span><script type=math/tex>q_*(a)</script></span> 是多少呢？这些 <span><span class=MathJax_Preview>q_*(a)</span><script type=math/tex>q_*(a)</script></span> 是我们在初始生成 2000 个问题时随机定下来的，我们从标准正态分布 <span><span class=MathJax_Preview>\mathrm{N}(0,1)</span><script type=math/tex>\mathrm{N}(0,1)</script></span> 中选出 2000 组数据，1 组数据对应生成一个老虎机问题，每组数据有 10 个（所以其实是从 <span><span class=MathJax_Preview>\mathrm{N}(0,1)</span><script type=math/tex>\mathrm{N}(0,1)</script></span> 中选出了 <span><span class=MathJax_Preview>2000\times 10=20000</span><script type=math/tex>2000\times 10=20000</script></span> 个随机数），分别表示这个问题下的 <span><span class=MathJax_Preview>q_*(1),\ldots,q_*(10)</span><script type=math/tex>q_*(1),\ldots,q_*(10)</script></span> 。</li> </ul> <h3 id=conclusion>Conclusion<a class=headerlink href=#conclusion title="Permanent link">&para;</a></h3> <div align=center><img src=../imgs/RLAI_2/avg-reward.png width=450 alt=avg-reward></div> <p>从上图易知，</p> <ul> <li>整体上都能一定程度地通过学习找到问题的规律，所以三种策略最后都能有一个正的稳定回报值（如果没有学到任何信息，也即随机选 action ，最后这些 “<strong>average</strong> reward” 显然会趋于0。注意这个 “<strong>average</strong>” ，是指很多不同问题的平均）</li> <li>贪心策略一开始的表现要比其他的略好，但是最终明显不如 ε-贪心策略（猜测是陷入了局部最优解）。</li> </ul> <div align=center><img src=../imgs/RLAI_2/optimal-action.png width=450 alt=optimal-action></div> <p>从上图易知，</p> <ul> <li>贪心策略只有约 1/3 的次数选到了最优 action，而 ε-贪心策略的表现则显然比单纯的贪心策略好很多，进一步验证了我们认为贪心策略陷入局部最优解的猜想。</li> <li>ε = 0.1 要比 ε = 0.01 选中最优解的概率更大，这与其重视 Exploration 离不开关系，但事实上实验结果表示，最终的总 reward 还是 ε = 0.01 要高一些，原因在于其有 99% 的时间处于 Exploitation 阶段，信息的利用率更高，ε = 0.1 时，Exploitation 的时间只有 90% ，即使探索到了足够多的信息，但是利用率不够高，导致最终效果不如前者。</li> </ul> <p>所以，通过实验，我们看出，Exploration 确实很重要，但是也不能过度探索，需要掌握好平衡。</p> <h2 id=24-incremental-implementation>2.4 Incremental Implementation<a class=headerlink href=#24-incremental-implementation title="Permanent link">&para;</a></h2> <p>这一节的简单讲提到如何让计算机来学习 bandit 问题。</p> <h3 id=optimization>Optimization<a class=headerlink href=#optimization title="Permanent link">&para;</a></h3> <p>首先，我们把问题简化一下，只关注某个具体的 action <span><span class=MathJax_Preview>a</span><script type=math/tex>a</script></span> ，其他的类比即可。</p> <p>设 <span><span class=MathJax_Preview>R_i</span><script type=math/tex>R_i</script></span> 表示第 i 次选到 <span><span class=MathJax_Preview>a</span><script type=math/tex>a</script></span> 时系统返回的奖励值，<span><span class=MathJax_Preview>Q_n</span><script type=math/tex>Q_n</script></span> 表示在前 n 次执行 action <span><span class=MathJax_Preview>a</span><script type=math/tex>a</script></span> 的经验基础上，对下一次再选到 <span><span class=MathJax_Preview>a</span><script type=math/tex>a</script></span> 的预测值，那么就有</p> <div> <div class=MathJax_Preview>Q_n\doteq \dfrac{R_1+R_2+\cdots + R_{n-1}}{n-1}</div> <script type="math/tex; mode=display">Q_n\doteq \dfrac{R_1+R_2+\cdots + R_{n-1}}{n-1}</script> </div> <p>不难看出，我们一直需要存储每一个 <span><span class=MathJax_Preview>R_i</span><script type=math/tex>R_i</script></span> ，空间复杂度为 <span><span class=MathJax_Preview>O(n)</span><script type=math/tex>O(n)</script></span> ，这显然程序跑到后面，会有着巨大的内存占用。不过，通过一个小技巧便可解决：</p> <div> <div class=MathJax_Preview> \begin{aligned} Q_{n+1} &amp; = \dfrac{1}{n}\sum_{i=1}^{n}R_i = \dfrac{1}{n}\left(R_n+\sum_{i=1}^{n-1}R_i\right)\\ &amp; = \dfrac{1}{n}\left(R_n+(n-1)\dfrac{1}{n-1}\sum_{i=1}^{n-1}R_i\right)\\ &amp; = \dfrac{1}{n}\left(R_n+(n-1)Q_n\right) = \dfrac{1}{n}\left(R_n+nQ_n-Q_n\right)\\ &amp; = Q_n + \dfrac{1}{n}\left[R_n-Q_n\right], \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
Q_{n+1} & = \dfrac{1}{n}\sum_{i=1}^{n}R_i = \dfrac{1}{n}\left(R_n+\sum_{i=1}^{n-1}R_i\right)\\
& = \dfrac{1}{n}\left(R_n+(n-1)\dfrac{1}{n-1}\sum_{i=1}^{n-1}R_i\right)\\
& = \dfrac{1}{n}\left(R_n+(n-1)Q_n\right) = \dfrac{1}{n}\left(R_n+nQ_n-Q_n\right)\\
& = Q_n + \dfrac{1}{n}\left[R_n-Q_n\right],
\end{aligned}
</script> </div> <p>即 <span><span class=MathJax_Preview>Q_{n+1} = Q_n + \dfrac{1}{n}\left[R_n-Q_n\right]</span><script type=math/tex>Q_{n+1} = Q_n + \dfrac{1}{n}\left[R_n-Q_n\right]</script></span> ，如此一来，我们只需要存储 <span><span class=MathJax_Preview>Q, R, n</span><script type=math/tex>Q, R, n</script></span> ，每次覆写在变量上即可，空间复杂度降为 <span><span class=MathJax_Preview>O(1)</span><script type=math/tex>O(1)</script></span> ，计算量也有所下降。</p> <p>上面式子的更广义的写法是</p> <div> <div class=MathJax_Preview>NewEstimate \leftarrow OldEstimate + StepSize[Target - OldEstimate]</div> <script type="math/tex; mode=display">NewEstimate \leftarrow OldEstimate + StepSize[Target - OldEstimate]</script> </div> <h3 id=pseuducode>Pseuducode<a class=headerlink href=#pseuducode title="Permanent link">&para;</a></h3> <p>在这个基础上，我们整理一下程序的流程，下面是程序的伪代码：</p> <blockquote> <p>Initialize, for a = 1 to k:</p> <p><span><span class=MathJax_Preview>\qquad Q(a)\leftarrow 0</span><script type=math/tex>\qquad Q(a)\leftarrow 0</script></span></p> <p><span><span class=MathJax_Preview>\qquad N(a)\leftarrow 0</span><script type=math/tex>\qquad N(a)\leftarrow 0</script></span></p> <p>Repeat forever:</p> <p><span><span class=MathJax_Preview>\qquad A \leftarrow \begin{cases} \mathop{\arg\max}\limits_{a}Q(a) &amp; \text{with probability} \ 1-\epsilon\\ \text{a random action} &amp; \text{with probability} \ \epsilon \end{cases}</span><script type=math/tex>\qquad A \leftarrow \begin{cases} \mathop{\arg\max}\limits_{a}Q(a) & \text{with probability} \ 1-\epsilon\\ \text{a random action} & \text{with probability} \ \epsilon \end{cases}</script></span></p> <p><span><span class=MathJax_Preview>\qquad R\leftarrow bandit(A)</span><script type=math/tex>\qquad R\leftarrow bandit(A)</script></span></p> <p><span><span class=MathJax_Preview>\qquad N(A)\leftarrow N(A)+1</span><script type=math/tex>\qquad N(A)\leftarrow N(A)+1</script></span></p> <p><span><span class=MathJax_Preview>\qquad Q(A)\leftarrow Q(A) + \dfrac{1}{N(A)}[R-Q(A)]</span><script type=math/tex>\qquad Q(A)\leftarrow Q(A) + \dfrac{1}{N(A)}[R-Q(A)]</script></span></p> </blockquote> <h2 id=25-tracking-a-nonstationary-problem>2.5 Tracking a Nonstationary Problem<a class=headerlink href=#25-tracking-a-nonstationary-problem title="Permanent link">&para;</a></h2> <p>前面的讨论，都是基于<strong>固定奖励值分布</strong>这一条件的，即 <span><span class=MathJax_Preview>R_t\sim \mathrm{N}(q_*(A_t),1)</span><script type=math/tex>R_t\sim \mathrm{N}(q_*(A_t),1)</script></span> 这一事实在问题生成好之后都是一直保持不变的。然而现实中问题肯定不会如此理想，那么如果这个奖励值分布不是固定不变的，我们该如何解决呢？</p> <p>显然，这种情况下玩家需要将学习的重心放在每一步近期的奖励值分布情况上，这是因为，奖励值分布的变动，如果有规律的话，无论是周期性还是连续性等，都只会更多地体现在较近时刻，而很久之前的某个 reward 对于这一步而言已经很难看出其影响意义。</p> <p>一个常见的作法是，将前面提到的增量式中的参数 StepSize 设为一个常量 <span><span class=MathJax_Preview>\alpha \in (0,1]</span><script type=math/tex>\alpha \in (0,1]</script></span> ，则有</p> <div> <div class=MathJax_Preview>Q_{n+1}\doteq Q_n + \alpha[R_n-Q_n] (\alpha \in (0,1])</div> <script type="math/tex; mode=display">Q_{n+1}\doteq Q_n + \alpha[R_n-Q_n] (\alpha \in (0,1])</script> </div> <p>（再强调一遍，这几节的很多式子都是为了简化而针对某一个 action 而言的，可以看作是所有 action 的通式，不要理解错了）</p> <p>我们不断对此式作展开，</p> <div> <div class=MathJax_Preview> \begin{aligned} Q_{n+1} &amp;\doteq Q_{n} + \alpha[R_{n} - Q_{n}] \\ &amp;= \alpha R_{n} + (1 - \alpha)Q_{n} \\ &amp;= \alpha R_{n} + (1 - \alpha)[\alpha R_{n-1} + (1-\alpha)Q_{n-1}] \\ &amp;= \alpha R_{n} + (1 -\alpha)\alpha R_{n-1} + (1 - \alpha)^{2}Q_{n-1} \\ &amp;= \alpha R_{n} + (1 - \alpha)\alpha R_{n-1} + (1-\alpha)^{2}\alpha R_{n-2} +... \\ &amp;+(1-\alpha)^{n-1}\alpha R_{1} + (1-\alpha)^{n}Q_{1} \\ &amp;= (1-\alpha)^{n}Q_{1}+\sum^{n}_{i=1}(1-\alpha)^{n-i}\alpha R_{i} \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
Q_{n+1} &\doteq Q_{n} + \alpha[R_{n} - Q_{n}]
\\ &= \alpha R_{n} + (1 - \alpha)Q_{n}
\\ &= \alpha R_{n} + (1 - \alpha)[\alpha R_{n-1} + (1-\alpha)Q_{n-1}]
\\ &= \alpha R_{n} + (1 -\alpha)\alpha R_{n-1} + (1 - \alpha)^{2}Q_{n-1}
\\ &= \alpha R_{n} + (1 - \alpha)\alpha R_{n-1} + (1-\alpha)^{2}\alpha R_{n-2} +...
\\ &+(1-\alpha)^{n-1}\alpha R_{1} + (1-\alpha)^{n}Q_{1}
\\ &= (1-\alpha)^{n}Q_{1}+\sum^{n}_{i=1}(1-\alpha)^{n-i}\alpha R_{i}
\end{aligned}
</script> </div> <p>最终整理得到</p> <div> <div class=MathJax_Preview>Q_{n+1} = (1-\alpha)^{n}Q_{1}+\sum^{n}_{i=1}(1-\alpha)^{n-i}\alpha R_{i}</div> <script type="math/tex; mode=display">Q_{n+1} = (1-\alpha)^{n}Q_{1}+\sum^{n}_{i=1}(1-\alpha)^{n-i}\alpha R_{i}</script> </div> <p>因为 <span><span class=MathJax_Preview>\displaystyle (1-\alpha)^{n}+\sum^{n}_{i=1}\alpha(1-\alpha)^{n-i}=1​</span><script type=math/tex>\displaystyle (1-\alpha)^{n}+\sum^{n}_{i=1}\alpha(1-\alpha)^{n-i}=1​</script></span> ，因此这是一个加权平均式，作者将此式称为<strong>指数近因加权平均（Exponential Recency-weighted Average）</strong>。</p> <p>可以看出，当 i 很大时，<span><span class=MathJax_Preview>R_i</span><script type=math/tex>R_i</script></span> 在式子中的影响占比才更大，这也符合了我们要将学习重心放在近期 reward 的要求。下面再讲将 <span><span class=MathJax_Preview>\alpha</span><script type=math/tex>\alpha</script></span> 设为常量的另一个重要原因。</p> <p>我们先回到一般情况，对于 <span><span class=MathJax_Preview>Q_{n+1} = Q_{n} + \alpha_n[R_{n} - Q_{n}]</span><script type=math/tex>Q_{n+1} = Q_{n} + \alpha_n[R_{n} - Q_{n}]</script></span> ，其中的 <span><span class=MathJax_Preview>\alpha_n</span><script type=math/tex>\alpha_n</script></span> 是任意的，也即 step-size 是变长的，对于这样一组 <span><span class=MathJax_Preview>\{\alpha_n\}</span><script type=math/tex>\{\alpha_n\}</script></span> 序列，如果满足随机逼近理论中的一个条件</p> <div> <div class=MathJax_Preview>\sum_{n=1}^{\infty}\alpha_n(a)=\infty\quad \text{and}\quad \sum_{n=1}^{\infty}\alpha_n^2(a)&lt;\infty</div> <script type="math/tex; mode=display">\sum_{n=1}^{\infty}\alpha_n(a)=\infty\quad \text{and}\quad \sum_{n=1}^{\infty}\alpha_n^2(a)<\infty</script> </div> <p>那么 <span><span class=MathJax_Preview>Q_n</span><script type=math/tex>Q_n</script></span> 将会以概率 1 收敛。</p> <p>我们只简单定性分析一下这两个条件的意义：</p> <ul> <li> <p><span><span class=MathJax_Preview>\displaystyle \sum_{n=1}^{\infty}\alpha_n(a)=\infty</span><script type=math/tex>\displaystyle \sum_{n=1}^{\infty}\alpha_n(a)=\infty</script></span> 能够确保总的步数足够长，进而摆脱初始条件和随机波动的影响。</p> </li> <li> <p><span><span class=MathJax_Preview>\displaystyle \sum_{n=1}^{\infty}\alpha_n^2(a)&lt;\infty</span><script type=math/tex>\displaystyle \sum_{n=1}^{\infty}\alpha_n^2(a)<\infty</script></span> 确保最终的步长足够小，进而能够收敛。</p> </li> </ul> <p>易见，<span><span class=MathJax_Preview>\alpha_n = \dfrac{1}{n}</span><script type=math/tex>\alpha_n = \dfrac{1}{n}</script></span> 满足条件能够让其收敛，而 <span><span class=MathJax_Preview>\alpha_n \equiv \alpha</span><script type=math/tex>\alpha_n \equiv \alpha</script></span>(<span><span class=MathJax_Preview>\alpha</span><script type=math/tex>\alpha</script></span> 为常量) 则由于不满足条件中的第二项，使 <span><span class=MathJax_Preview>Q_n</span><script type=math/tex>Q_n</script></span> 不能收敛。看似这是一个坏结果，其实这一结果反而能被利用在<strong>非稳定（nonstationary）问题</strong>中，这是因为，一个不收敛的波动的 <span><span class=MathJax_Preview>Q_n</span><script type=math/tex>Q_n</script></span> 其实更适合用来描述非稳定问题下的奖励值，而前面收敛的 <span><span class=MathJax_Preview>Q_n</span><script type=math/tex>Q_n</script></span> 反而可能失去了非稳定环境下的一些关键波动信息。</p> <p>另一个关键之处在于，满足两个条件的 <span><span class=MathJax_Preview>\{\alpha_n\}</span><script type=math/tex>\{\alpha_n\}</script></span> 往往收敛缓慢，非常不实用，一般也只用在理论研究中。</p> <h2 id=26-optimistic-initial-values>2.6 Optimistic Initial Values<a class=headerlink href=#26-optimistic-initial-values title="Permanent link">&para;</a></h2> <p>这一节简单研究了一下初始预估值对模型学习效果的影响。</p> <p>我们再次拿出前面的指数近因加权平均：</p> <div> <div class=MathJax_Preview>Q_{n+1} = (1-\alpha)^{n}Q_{1}+\sum^{n}_{i=1}(1-\alpha)^{n-i}\alpha R_{i}</div> <script type="math/tex; mode=display">Q_{n+1} = (1-\alpha)^{n}Q_{1}+\sum^{n}_{i=1}(1-\alpha)^{n-i}\alpha R_{i}</script> </div> <p>不难看出，前面讨论的所有方法，对每个 action 而言，评估体系显然都会一定程度上受到初始值 <span><span class=MathJax_Preview>Q_1</span><script type=math/tex>Q_1</script></span> 的影响。在统计学中，这叫做被初值<em>偏置</em>了。</p> <p>初始预估值可以用来根据先验信息提供奖励的期望标准。此外，如果将初始值调高，还有着鼓励模型在早期更多地进行探索的作用。以贪心策略为例，一个很高的初始预期值（称为<strong>乐观初值</strong>），会诱使模型去选择这个 action ，然而事实上 reward 要比估计值差很多，误差值 <span><span class=MathJax_Preview>[R_n - Q_n]</span><script type=math/tex>[R_n - Q_n]</script></span> 会是一个较大的负数，导致模型对这个 action “失望”，评价降低，下一次，模型就会去主动尝试其他 action 。通过这一方法，达到了鼓励模型在早期多做探索的作用。</p> <p>下面是一个具体的实验，“高初始值的 greedy 策略” vs “正常初始值的 ε-greedy 策略”。</p> <div align=center><img src=../imgs/RLAI_2/optimistic-init.png width=450 alt=optimistic-init></div> <p>从图片可以看出，即使是纯贪心行动，由于一开始给定较高初始值，模型便如我们分析的一样，在早期进行了大量探索，收获了大量有用的信息，从而也能摆脱局部最优，达到全局最优解，而且其后期几乎 100% 利用率的优势，使其比该实验中 ε-greedy 方法的效果还要优秀。</p> <p>但是乐观初值法的适用面很窄，它仅适用于固定分布的问题。我们知道模型只会在早期多做探索，后期基本上仍是以 Exploiting 为主，对于非稳定的情况，必然需要时刻探索收集信息，此时乐观初值法就不再适用。</p> <h2 id=27-upper-confidence-bound-action-selection>2.7 Upper-Confidence-Bound Action Selection<a class=headerlink href=#27-upper-confidence-bound-action-selection title="Permanent link">&para;</a></h2> <p>这一节讲到一个考虑得更全面的评估算法：<strong>Upper-Confidence-Bound(UCB)</strong> 算法。简单讲，就是我们之前的 ε-greedy 方法虽然能保证最终能探索到足够的信息，但是效率不高，因为他只是简单的随机探索，探索时每个 action 都是等概率被选择的。</p> <p>我们可以想一想人是怎样探索学习的。人在探索过程中，通过探索学到的知识，肯定会建立一套标准来判定好坏，如果重复执行某个 action ，一直返回一个低回报，那么必须要动态调整探索策略，适当调低再探索这个 action 的概率，而要尽可能多去探索“潜力”更高的 action 。UCB 算法做的就是这么一件事。</p> <p>那么 UCB 算法具体是什么呢？</p> <div> <div class=MathJax_Preview>A_{t} \doteq \mathop{\arg\max}_{a}\left[Q_{t}(a) + c\sqrt{\frac{\ln{t}}{N_{t}(a)}}\right]</div> <script type="math/tex; mode=display">A_{t} \doteq \mathop{\arg\max}_{a}\left[Q_{t}(a) + c\sqrt{\frac{\ln{t}}{N_{t}(a)}}\right]</script> </div> <p>UCB 算法就是采取满足上式的 action <span><span class=MathJax_Preview>A_t</span><script type=math/tex>A_t</script></span> ，算法的核心就在于我们新加入的 <span><span class=MathJax_Preview>c\sqrt{\dfrac{\ln t}{N_t(a)}}</span><script type=math/tex>c\sqrt{\dfrac{\ln t}{N_t(a)}}</script></span> 。</p> <ul> <li><span><span class=MathJax_Preview>c\sqrt{\dfrac{\ln t}{N_t(a)}}</span><script type=math/tex>c\sqrt{\dfrac{\ln t}{N_t(a)}}</script></span> ：对于估值的不确定性。更广义地讲，其意义为方差。</li> <li><span><span class=MathJax_Preview>c</span><script type=math/tex>c</script></span> ：控制了探索的程度，决定了置信度。</li> <li><span><span class=MathJax_Preview>N_t(a)</span><script type=math/tex>N_t(a)</script></span> ：第 t 步之前 action <span><span class=MathJax_Preview>a</span><script type=math/tex>a</script></span> 被选中的次数。<span><span class=MathJax_Preview>N_t(a)</span><script type=math/tex>N_t(a)</script></span> 如果增加，会给公式中的此项带来降低的影响效果。</li> <li><span><span class=MathJax_Preview>\ln t</span><script type=math/tex>\ln t</script></span> ：时间必然会增加，故此项也是一直在保持增加的。但他的影响效果与 action <span><span class=MathJax_Preview>a</span><script type=math/tex>a</script></span> 的选择状态密切相关。如果 <span><span class=MathJax_Preview>a</span><script type=math/tex>a</script></span> 不被选择， <span><span class=MathJax_Preview>\ln t</span><script type=math/tex>\ln t</script></span> 增加但 <span><span class=MathJax_Preview>N_t(a)</span><script type=math/tex>N_t(a)</script></span> 不变，故此项变大，不确定性增加；反之，<span><span class=MathJax_Preview>N_{t+1}(a) = N_t(a) + 1</span><script type=math/tex>N_{t+1}(a) = N_t(a) + 1</script></span> ，虽然 <span><span class=MathJax_Preview>\ln t</span><script type=math/tex>\ln t</script></span> 增加但其增速不如 <span><span class=MathJax_Preview>N_t(a)</span><script type=math/tex>N_t(a)</script></span> ，所以此项整体变小，不确定性降低。</li> </ul> <p>所以，通过取 <span><span class=MathJax_Preview>\mathop{\arg\max}</span><script type=math/tex>\mathop{\arg\max}</script></span> 便能动态调整探索策略，适当地提高更加不确定、有潜力的 action 被探索的概率。</p> <div align=center><img src=../imgs/RLAI_2/ucb.png width=450 alt=ucb></div> <p>通过对比实验发现，UCB 算法确实表现要优于普通的 ε-greedy 算法。但是作者也提到，对于更一般的强化学习问题，UCB 算法会遇到一些难点，不再那么适用，比如非稳定问题、大状态空间问题等。</p> <h2 id=28-gradient-bandit-algorithms>2.8 Gradient Bandit Algorithms<a class=headerlink href=#28-gradient-bandit-algorithms title="Permanent link">&para;</a></h2> <h3 id=introduction>Introduction<a class=headerlink href=#introduction title="Permanent link">&para;</a></h3> <p>前面几种算法，都是在围绕着 <span><span class=MathJax_Preview>Q_t</span><script type=math/tex>Q_t</script></span> 进行取 <span><span class=MathJax_Preview>\mathop{\arg\max}</span><script type=math/tex>\mathop{\arg\max}</script></span> 然后直接执行 action 的策略，显得有点偏激，一个看上去更合理的做法是，每个 action 对其评分后，确定一个概率分布，然后以这种分布下的<strong>趋势</strong>去做选择，而非凭借数值的绝对大小去做选择。这样显得更加“平滑”，同时根据这些趋势，也能达到动态探索的效果。这便是 Gradient Bandit Algorithms 。</p> <div> <div class=MathJax_Preview>\mathrm{Pr}\{A_{t} = a\} \doteq \frac{e^{H_{t}(a)}}{\sum^{k}_{b=1}e^{H_{t}(b)}}\doteq \pi_{t}(a)</div> <script type="math/tex; mode=display">\mathrm{Pr}\{A_{t} = a\} \doteq \frac{e^{H_{t}(a)}}{\sum^{k}_{b=1}e^{H_{t}(b)}}\doteq \pi_{t}(a)</script> </div> <p>其中，<span><span class=MathJax_Preview>H_t(a)</span><script type=math/tex>H_t(a)</script></span> 是 action <span><span class=MathJax_Preview>a</span><script type=math/tex>a</script></span> 的偏好值，也就是前面提到的对每个 action 的评分，然后根据 <em>soft-max</em> 函数来给出选择每个 action 的概率分布。</p> <p>而在取得每步的反馈后，我们则需要利用随机梯度上升法来更新偏好值</p> <div> <div class=MathJax_Preview> \begin{aligned} H_{t+1}(A_{t})&amp;\doteq H_{t}(A_{t}) + \alpha(R_{t}-\overline R_{t})(1-\pi_{t}(A_{t})) \\H_{t+1}(a) &amp;\doteq H_{t}(a) - \alpha(R_{t} - \overline R_{t})\pi_{t}(a),\ \forall a \neq A_{t} \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
H_{t+1}(A_{t})&\doteq H_{t}(A_{t}) + \alpha(R_{t}-\overline R_{t})(1-\pi_{t}(A_{t}))
\\H_{t+1}(a) &\doteq H_{t}(a) - \alpha(R_{t} - \overline R_{t})\pi_{t}(a),\ \forall a \neq A_{t}
\end{aligned}
</script> </div> <p>更一般地，我们可以用指示函数来写成一个通式</p> <div> <div class=MathJax_Preview>H_{t+1}(a) \doteq H_{t}(a) + \alpha(R_{t}-\overline R_{t})(\textbf{1}_{a = A_t}-\pi_{t}(A_{t}))</div> <script type="math/tex; mode=display">H_{t+1}(a) \doteq H_{t}(a) + \alpha(R_{t}-\overline R_{t})(\textbf{1}_{a = A_t}-\pi_{t}(A_{t}))</script> </div> <h3 id=proof>Proof<a class=headerlink href=#proof title="Permanent link">&para;</a></h3> <p>我们知道，随机梯度上升确实能确保收敛到最优值，那么问题就在于，这个形式是否就是“随机梯度上升”的形式呢？</p> <p>结论先摆出来，上面的方法确实满足随机梯度上升的条件。证明如下：</p> <p>只需证明</p> <div> <div class=MathJax_Preview> H_{t+1}(a) \doteq H_{t}(a) + \alpha \frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)} </div> <script type="math/tex; mode=display">
H_{t+1}(a) \doteq H_{t}(a) + \alpha \frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)}
</script> </div> <p>其中</p> <div> <div class=MathJax_Preview> \mathbb{E}[R_t] \doteq \sum_{b} \pi_t(b)q_*(b) </div> <script type="math/tex; mode=display">
\mathbb{E}[R_t] \doteq \sum_{b} \pi_t(b)q_*(b)
</script> </div> <p>而</p> <div> <div class=MathJax_Preview> \begin{aligned} \frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)}&amp;=\frac{\partial}{\partial H_t(a)}\left[\sum_{b}\pi_t(b)q_*(b)\right] \\&amp;=\sum_b q_*(b)\frac{\partial \pi_t(b)}{\partial H_t(a)} \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)}&=\frac{\partial}{\partial H_t(a)}\left[\sum_{b}\pi_t(b)q_*(b)\right]
\\&=\sum_b q_*(b)\frac{\partial \pi_t(b)}{\partial H_t(a)}
\end{aligned}
</script> </div> <p>任设一个标量 <span><span class=MathJax_Preview>X_t</span><script type=math/tex>X_t</script></span> 与 <span><span class=MathJax_Preview>b</span><script type=math/tex>b</script></span> 独立，而显然又有 <span><span class=MathJax_Preview>\displaystyle \sum_b \frac{\partial \pi_t(b)}{\partial H_t(a)} = 0</span><script type=math/tex>\displaystyle \sum_b \frac{\partial \pi_t(b)}{\partial H_t(a)} = 0</script></span> ，因此 <span><span class=MathJax_Preview>\displaystyle X_t\sum_b \frac{\partial \pi_t(b)}{\partial H_t(a)} = 0</span><script type=math/tex>\displaystyle X_t\sum_b \frac{\partial \pi_t(b)}{\partial H_t(a)} = 0</script></span> 。接上式，</p> <div> <div class=MathJax_Preview> \begin{aligned} \\&amp;=\sum_b(q_*(b) - X_t)\frac{\partial \pi_t(b)}{\partial H_t(a)} \\&amp;=\sum_b \pi_t(b)(q_*(b) - X_t)\frac{\partial \pi_t(b)}{\partial H_t(a)}/\pi_t(b) \\&amp;=\mathbb{E}[(q_*(A_t) - X_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)] \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\\&=\sum_b(q_*(b) - X_t)\frac{\partial \pi_t(b)}{\partial H_t(a)}
\\&=\sum_b \pi_t(b)(q_*(b) - X_t)\frac{\partial \pi_t(b)}{\partial H_t(a)}/\pi_t(b)
\\&=\mathbb{E}[(q_*(A_t) - X_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)]
\end{aligned}
</script> </div> <p>在给定 <span><span class=MathJax_Preview>A_t</span><script type=math/tex>A_t</script></span> 的条件下， <span><span class=MathJax_Preview>\mathbb{E}[R_t|A_t]=q_*(A_t)</span><script type=math/tex>\mathbb{E}[R_t|A_t]=q_*(A_t)</script></span> ， <span><span class=MathJax_Preview>R_t</span><script type=math/tex>R_t</script></span> 又与其他项不相关，故对于上式，期望意义下可以把 <span><span class=MathJax_Preview>q_*(A_t)</span><script type=math/tex>q_*(A_t)</script></span> 替换为 <span><span class=MathJax_Preview>R_t</span><script type=math/tex>R_t</script></span> ，此时再将任设的 <span><span class=MathJax_Preview>X_t</span><script type=math/tex>X_t</script></span> 定为 <span><span class=MathJax_Preview>R_t</span><script type=math/tex>R_t</script></span> 的均值 <span><span class=MathJax_Preview>\overline R_t</span><script type=math/tex>\overline R_t</script></span> ，则有</p> <div> <div class=MathJax_Preview> \frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)}=\mathbb{E}[(R_t - \overline R_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)] </div> <script type="math/tex; mode=display">
\frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)}=\mathbb{E}[(R_t - \overline R_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)]
</script> </div> <p>由于</p> <div> <div class=MathJax_Preview> \begin{aligned} \frac{\partial \pi_t(b)}{\partial H_t(a)} &amp;= \frac{\partial}{\partial H_t(a)}\pi_t(b) \\&amp;= \frac{\partial}{\partial H_t(a)}[\frac{e^{H_{t}(b)}}{\sum^{k}_{c=1}e^{H_{t}(c)}}] \\&amp;= \frac{\frac{\partial e^{H_t(b)}}{\partial H_t(a)}\sum_{c=1}^k e^{H_t(c)}-e^{H_t(b)} \frac{\partial \sum_{c=1}^k e^{H_t(c)}}{\partial H_t(a)}}{(\sum_{c=1}^k e^{H_t(c)})^2} \\&amp;=\frac{\textbf{1}_{a = b}e^{H_t(a)}\sum_{c=1}^k e^H_t(c) - e^{H_t(b)}e^{H_t(a)}}{(\sum_{c=1}^k e^{H_t(c)})^2} \\&amp;=\frac{\textbf{1}_{a = b}e^{H_t(b)}}{\sum_{c=1}^k e^{H_t(c)}} - \frac{e^{H_t(b)}e^{H_t(a)}}{(\sum_{c=1}^k e^{H_t(c)})^2} \\&amp;=\textbf{1}_{a=b}\pi_t(b)-\pi_t(b)\pi_t(a) \\&amp;=\pi_t(b)(\textbf{1}_{a=b}-\pi_t(a)) \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial \pi_t(b)}{\partial H_t(a)} &= \frac{\partial}{\partial H_t(a)}\pi_t(b)
\\&= \frac{\partial}{\partial H_t(a)}[\frac{e^{H_{t}(b)}}{\sum^{k}_{c=1}e^{H_{t}(c)}}]
\\&= \frac{\frac{\partial e^{H_t(b)}}{\partial H_t(a)}\sum_{c=1}^k e^{H_t(c)}-e^{H_t(b)} \frac{\partial \sum_{c=1}^k e^{H_t(c)}}{\partial H_t(a)}}{(\sum_{c=1}^k e^{H_t(c)})^2}
\\&=\frac{\textbf{1}_{a = b}e^{H_t(a)}\sum_{c=1}^k e^H_t(c) - e^{H_t(b)}e^{H_t(a)}}{(\sum_{c=1}^k e^{H_t(c)})^2}
\\&=\frac{\textbf{1}_{a = b}e^{H_t(b)}}{\sum_{c=1}^k e^{H_t(c)}} - \frac{e^{H_t(b)}e^{H_t(a)}}{(\sum_{c=1}^k e^{H_t(c)})^2}
\\&=\textbf{1}_{a=b}\pi_t(b)-\pi_t(b)\pi_t(a)
\\&=\pi_t(b)(\textbf{1}_{a=b}-\pi_t(a))
\end{aligned}
</script> </div> <p>代回前面的式子得到</p> <div> <div class=MathJax_Preview> \begin{aligned} \frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)}&amp;=\mathbb{E}[(R_t - \overline R_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)] \\&amp;=\mathbb{E}[(R_t - \overline R_t)\pi_t(A_t)(\textbf{1}_{a = A_t} - \pi_t(a))/\pi_t(A_t)] \\&amp;=\mathbb{E}[(R_t - \overline R_t)(\textbf{1}_{a = A_t} - \pi_t(a))] \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)}&=\mathbb{E}[(R_t - \overline R_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)]
\\&=\mathbb{E}[(R_t - \overline R_t)\pi_t(A_t)(\textbf{1}_{a = A_t} - \pi_t(a))/\pi_t(A_t)]
\\&=\mathbb{E}[(R_t - \overline R_t)(\textbf{1}_{a = A_t} - \pi_t(a))]
\end{aligned}
</script> </div> <p>结合上面的结果，此时再来对比我们的两个目标式</p> <div> <div class=MathJax_Preview> \begin{aligned} H_{t+1}(a) &amp;= H_{t}(a) + \alpha \frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)} \\H_{t+1}(a) &amp;= H_{t}(a) + \alpha(R_{t}-\overline R_{t})(\textbf{1}_{a = A_t}-\pi_{t}(A_{t})) \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
H_{t+1}(a) &= H_{t}(a) + \alpha \frac{\partial \mathbb{E}[R_{t}]}{\partial H_{t}(a)}
\\H_{t+1}(a) &= H_{t}(a) + \alpha(R_{t}-\overline R_{t})(\textbf{1}_{a = A_t}-\pi_{t}(A_{t}))
\end{aligned}
</script> </div> <p>发现确实是梯度上升的形式，证明完毕。</p> <div align=center><img src=../imgs/RLAI_2/sga.png width=450 alt=sga></div> <p>关于 <span><span class=MathJax_Preview>H_t(a)</span><script type=math/tex>H_t(a)</script></span> 的更新式中 <span><span class=MathJax_Preview>\overline R_{t}</span><script type=math/tex>\overline R_{t}</script></span> 这一项，他起到一个对比基准线的作用，事实上这个基准线不一定设为均值，他的取值并不影响更新式的方差。作者表明，其实设为均值并不一定能达到最佳效果，但总体而言是一个简单方便且效果较好的一个选择。上图中的实验简单对比了 baseline 为均值和 baseline 为 0 时的不同效果。</p> <h2 id=29-associative-search-contextual-bandit>2.9 Associative Search (Contextual Bandit)<a class=headerlink href=#29-associative-search-contextual-bandit title="Permanent link">&para;</a></h2> <p>本文的一开头，我们提到本章主要针对“非关联性（nonassociative）”的简单场景来学习基础的强化学习方法。而非关联性在本章就是指<strong>无需考虑每一步行动之间的影响，以及环境对行动的影响</strong>。非关联性问题是一种很理想化的问题，现实中很多东西都是有所联系的，包括 action 与 action 之间的关联， action 与环境之间的关联等等。这一小节，就是关于关联性问题做了一个最基本的简单介绍。</p> <h3 id=background_2>Background<a class=headerlink href=#background_2 title="Permanent link">&para;</a></h3> <ul> <li>考虑有 m 个独立的 <span><span class=MathJax_Preview>k_i</span><script type=math/tex>k_i</script></span>-armed bandit 任务（<span><span class=MathJax_Preview>i=1,\ldots,m</span><script type=math/tex>i=1,\ldots,m</script></span>），每个都有独特的特征能被区分开。</li> <li>每一步会让你面对一个 <span><span class=MathJax_Preview>k_i</span><script type=math/tex>k_i</script></span>-armed bandit 任务来做选择。</li> <li>目标是学习出能将这 m 个独立任务关联起来的最优方案。</li> </ul> <h3 id=full-reinforcement-learning-problem>Full Reinforcement Learning Problem<a class=headerlink href=#full-reinforcement-learning-problem title="Permanent link">&para;</a></h3> <p>简单而言，之前一直讨论的 nonassociative 问题可以看作现在这个问题下 m=1 的特例。在这个新任务中，我们不但要像之前一样通过探索和利用来学习每个问题的情况，还要把问题之间的关联性也学出来，也就是把环境因素也考虑进来。</p> <p>这种复杂的问题，叫做 full reinforcement learning problem ，会在书的后面章节讲到。</p> <hr> <div class=md-source-date> <small> Last update: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">August 1, 2019</span> </small> </div> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid" aria-label=Footer> <a href=../../../ title=Index class="md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Previous </span> Index </div> </div> </a> <a href=../RLAI_3/ title="Chapter 3" class="md-footer-nav__link md-footer-nav__link--next" rel=next> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Next </span> Chapter 3 </div> </div> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2016-2020 ZHANGWP </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-footer-social> <a href=https://github.com/zawnpn target=_blank rel=noopener title=github.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://twitter.com/zawnpn target=_blank rel=noopener title=twitter.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> </a> <a href=https://psnprofiles.com/zawnpn target=_blank rel=noopener title=psnprofiles.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><path d="M570.9 372.3c-11.3 14.2-38.8 24.3-38.8 24.3L327 470.2v-54.3l150.9-53.8c17.1-6.1 19.8-14.8 5.8-19.4-13.9-4.6-39.1-3.3-56.2 2.9L327 381.1v-56.4c23.2-7.8 47.1-13.6 75.7-16.8 40.9-4.5 90.9.6 130.2 15.5 44.2 14 49.2 34.7 38 48.9zm-224.4-92.5v-139c0-16.3-3-31.3-18.3-35.6-11.7-3.8-19 7.1-19 23.4v347.9l-93.8-29.8V32c39.9 7.4 98 24.9 129.2 35.4C424.1 94.7 451 128.7 451 205.2c0 74.5-46 102.8-104.5 74.6zM43.2 410.2c-45.4-12.8-53-39.5-32.3-54.8 19.1-14.2 51.7-24.9 51.7-24.9l134.5-47.8v54.5l-96.8 34.6c-17.1 6.1-19.7 14.8-5.8 19.4 13.9 4.6 39.1 3.3 56.2-2.9l46.4-16.9v48.8c-51.6 9.3-101.4 7.3-153.9-10z"/></svg> </a> <a href=https://steamcommunity.com/id/zawnpn/ target=_blank rel=noopener title=steamcommunity.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M496 256c0 137-111.2 248-248.4 248-113.8 0-209.6-76.3-239-180.4l95.2 39.3c6.4 32.1 34.9 56.4 68.9 56.4 39.2 0 71.9-32.4 70.2-73.5l84.5-60.2c52.1 1.3 95.8-40.9 95.8-93.5 0-51.6-42-93.5-93.7-93.5s-93.7 42-93.7 93.5v1.2L176.6 279c-15.5-.9-30.7 3.4-43.5 12.1L0 236.1C10.2 108.4 117.1 8 247.6 8 384.8 8 496 119 496 256zM155.7 384.3l-30.5-12.6a52.79 52.79 0 0027.2 25.8c26.9 11.2 57.8-1.6 69-28.4 5.4-13 5.5-27.3.1-40.3-5.4-13-15.5-23.2-28.5-28.6-12.9-5.4-26.7-5.2-38.9-.6l31.5 13c19.8 8.2 29.2 30.9 20.9 50.7-8.3 19.9-31 29.2-50.8 21zm173.8-129.9c-34.4 0-62.4-28-62.4-62.3s28-62.3 62.4-62.3 62.4 28 62.4 62.3-27.9 62.3-62.4 62.3zm.1-15.6c25.9 0 46.9-21 46.9-46.8 0-25.9-21-46.8-46.9-46.8s-46.9 21-46.9 46.8c.1 25.8 21.1 46.8 46.9 46.8z"/></svg> </a> <a href=https://www.zhihu.com/people/zhangwanpeng target=_blank rel=noopener title=www.zhihu.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13H170.54zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82v170.31zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62v.01zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2l19.23 14.43zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78l.03-.01z"/></svg> </a> </div> </div> </div> </footer> </div> <script src=../../../../assets/javascripts/vendor.d710d30a.min.js></script> <script src=../../../../assets/javascripts/bundle.b39636ac.min.js></script><script id=__lang type=application/json>{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script> <script>
        app = initialize({
          base: "../../../..",
          features: ["tabs"],
          search: Object.assign({
            worker: "../../../../assets/javascripts/worker/search.a68abb33.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script> <script src="//cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-MML-AM_SVG"></script> </body> </html>