<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Welcome to zhangwp's blog."><link href=https://www.zhangwp.com/notes/reinforcement-learning/notes/RLAI_11/ rel=canonical><meta name=author content=zawnpn><link rel="shortcut icon" href=../../../../assets/images/favicon.svg><meta name=generator content="mkdocs-1.1.2, mkdocs-material-5.4.0"><title>Chapter 11 - ZHANGWP</title><link rel=stylesheet href=../../../../assets/stylesheets/main.fe0cca5b.min.css><link rel=stylesheet href=../../../../assets/stylesheets/palette.a46bcfb3.min.css><meta name=theme-color content=#546e7a></head> <body dir=ltr data-md-color-scheme data-md-color-primary=blue-grey data-md-color-accent> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#-off-policy class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid" aria-label=Header> <a href=https://www.zhangwp.com title=ZHANGWP class="md-header-nav__button md-logo" aria-label=ZHANGWP> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><path d="M257.981 272.971L63.638 467.314c-9.373 9.373-24.569 9.373-33.941 0L7.029 444.647c-9.357-9.357-9.375-24.522-.04-33.901L161.011 256 6.99 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L257.981 239.03c9.373 9.372 9.373 24.568 0 33.941zM640 456v-32c0-13.255-10.745-24-24-24H312c-13.255 0-24 10.745-24 24v32c0 13.255 10.745 24 24 24h304c13.255 0 24-10.745 24-24z"/></svg> </a> <label class="md-header-nav__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header-nav__title data-md-component=header-title> <div class=md-header-nav__ellipsis> <span class="md-header-nav__topic md-ellipsis"> ZHANGWP </span> <span class="md-header-nav__topic md-ellipsis"> Chapter 11 </span> </div> </div> <label class="md-header-nav__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query data-md-state=active> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <button type=reset class="md-search__icon md-icon" aria-label=Clear data-md-component=search-reset tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header-nav__source> <a href=https://github.com/zawnpn/ZHANGWP/ title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class="md-tabs md-tabs--active" aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../ class="md-tabs__link md-tabs__link--active"> Notes </a> </li> <li class=md-tabs__item> <a href=../../../../tips/ class=md-tabs__link> Tips </a> </li> <li class=md-tabs__item> <a href=../../../../share/ class=md-tabs__link> Share </a> </li> <li class=md-tabs__item> <a href=../../../../statements/ class=md-tabs__link> Statements </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=https://www.zhangwp.com title=ZHANGWP class="md-nav__button md-logo" aria-label=ZHANGWP> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><path d="M257.981 272.971L63.638 467.314c-9.373 9.373-24.569 9.373-33.941 0L7.029 444.647c-9.357-9.357-9.375-24.522-.04-33.901L161.011 256 6.99 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L257.981 239.03c9.373 9.372 9.373 24.568 0 33.941zM640 456v-32c0-13.255-10.745-24-24-24H312c-13.255 0-24 10.745-24 24v32c0 13.255 10.745 24 24 24h304c13.255 0 24-10.745 24-24z"/></svg> </a> ZHANGWP </label> <div class=md-nav__source> <a href=https://github.com/zawnpn/ZHANGWP/ title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1 type=checkbox id=nav-1> <label class=md-nav__link for=nav-1> Home <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Home data-md-level=1> <label class=md-nav__title for=nav-1> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../.. title=Home class=md-nav__link> Home </a> </li> <li class=md-nav__item> <a href=../../../../links/ title=Links class=md-nav__link> Links </a> </li> <li class=md-nav__item> <a href=../../../../donates/ title=Donate class=md-nav__link> Donate </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2 type=checkbox id=nav-2 checked> <label class=md-nav__link for=nav-2> Notes <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Notes data-md-level=1> <label class=md-nav__title for=nav-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Notes </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../ title=Index class=md-nav__link> Index </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2-2 type=checkbox id=nav-2-2 checked> <label class=md-nav__link for=nav-2-2> Reinforcement <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Reinforcement data-md-level=2> <label class=md-nav__title for=nav-2-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Reinforcement </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2-2-1 type=checkbox id=nav-2-2-1 checked> <label class=md-nav__link for=nav-2-2-1> Reinforcement Learning An Introduction <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Reinforcement Learning An Introduction" data-md-level=3> <label class=md-nav__title for=nav-2-2-1> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Reinforcement Learning An Introduction </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../RLAI_2/ title="Chapter 2" class=md-nav__link> Chapter 2 </a> </li> <li class=md-nav__item> <a href=../RLAI_3/ title="Chapter 3" class=md-nav__link> Chapter 3 </a> </li> <li class=md-nav__item> <a href=../RLAI_4/ title="Chapter 4" class=md-nav__link> Chapter 4 </a> </li> <li class=md-nav__item> <a href=../RLAI_5/ title="Chapter 5" class=md-nav__link> Chapter 5 </a> </li> <li class=md-nav__item> <a href=../RLAI_6/ title="Chapter 6" class=md-nav__link> Chapter 6 </a> </li> <li class=md-nav__item> <a href=../RLAI_7/ title="Chapter 7" class=md-nav__link> Chapter 7 </a> </li> <li class=md-nav__item> <a href=../RLAI_8/ title="Chapter 8" class=md-nav__link> Chapter 8 </a> </li> <li class=md-nav__item> <a href=../RLAI_9/ title="Chapter 9" class=md-nav__link> Chapter 9 </a> </li> <li class=md-nav__item> <a href=../RLAI_10/ title="Chapter 10" class=md-nav__link> Chapter 10 </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Chapter 11 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 9h14V7H3v2m0 4h14v-2H3v2m0 4h14v-2H3v2m16 0h2v-2h-2v2m0-10v2h2V7h-2m0 6h2v-2h-2v2z"/></svg> </span> </label> <a href=./ title="Chapter 11" class="md-nav__link md-nav__link--active"> Chapter 11 </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#111-semi-gradient-methods class=md-nav__link> 11.1 Semi-gradient Methods </a> <nav class=md-nav aria-label="11.1 Semi-gradient Methods"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#semi-gradient-off-policy-td0 class=md-nav__link> semi-gradient off-policy TD(0) </a> </li> <li class=md-nav__item> <a href=#semi-gradient-expected-sarsa class=md-nav__link> semi-gradient Expected Sarsa </a> </li> <li class=md-nav__item> <a href=#n-step-semi-gradient-expected-sarsa class=md-nav__link> n-step semi-gradient Expected Sarsa </a> </li> <li class=md-nav__item> <a href=#n-step-semi-gradient-tree-backup class=md-nav__link> n-step semi-gradient tree-backup </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#112-examples-of-off-policy-divergence class=md-nav__link> 11.2 Examples of Off-policy Divergence </a> </li> <li class=md-nav__item> <a href=#113-the-deadly-triad class=md-nav__link> 11.3 The Deadly Triad </a> </li> <li class=md-nav__item> <a href=#114-linear-value-function-geometry class=md-nav__link> 11.4 Linear Value-function Geometry </a> </li> <li class=md-nav__item> <a href=#115-gradient-descent-in-the-bellman-error class=md-nav__link> 11.5 Gradient Descent in the Bellman Error </a> </li> <li class=md-nav__item> <a href=#116-the-bellman-error-is-not-learnable class=md-nav__link> 11.6 The Bellman Error is Not Learnable </a> </li> <li class=md-nav__item> <a href=#117-gradient-td-methods class=md-nav__link> 11.7 Gradient-TD Methods </a> </li> <li class=md-nav__item> <a href=#118-emphatic-td-methods class=md-nav__link> 11.8 Emphatic-TD Methods </a> </li> <li class=md-nav__item> <a href=#119-reducing-variance class=md-nav__link> 11.9 Reducing Variance </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../RLAI_12/ title="Chapter 12" class=md-nav__link> Chapter 12 </a> </li> <li class=md-nav__item> <a href=../RLAI_13/ title="Chapter 13" class=md-nav__link> Chapter 13 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2-2-2 type=checkbox id=nav-2-2-2> <label class=md-nav__link for=nav-2-2-2> Some Introduction <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="Some Introduction" data-md-level=3> <label class=md-nav__title for=nav-2-2-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Some Introduction </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../MCTS_introduction/ title=MCTS class=md-nav__link> MCTS </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> Tips <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Tips data-md-level=1> <label class=md-nav__title for=nav-3> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Tips </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../tips/ title=Tips class=md-nav__link> Tips </a> </li> <li class=md-nav__item> <a href=../../../../tips/to-do/ title="To Do" class=md-nav__link> To Do </a> </li> <li class=md-nav__item> <a href=../../../../tips/python/ title=Python class=md-nav__link> Python </a> </li> <li class=md-nav__item> <a href=../../../../tips/data-processing/ title="Data Processing" class=md-nav__link> Data Processing </a> </li> <li class=md-nav__item> <a href=../../../../tips/git/ title=Git class=md-nav__link> Git </a> </li> <li class=md-nav__item> <a href=../../../../tips/linux/ title=Linux class=md-nav__link> Linux </a> </li> <li class=md-nav__item> <a href=../../../../tips/win/ title=Windows class=md-nav__link> Windows </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4 type=checkbox id=nav-4> <label class=md-nav__link for=nav-4> Share <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Share data-md-level=1> <label class=md-nav__title for=nav-4> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Share </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/ title=Index class=md-nav__link> Index </a> </li> <li class=md-nav__item> <a href=../../../../share/blog-history/ title=博客历史 class=md-nav__link> 博客历史 </a> </li> <li class=md-nav__item> <a href=../../../../share/game-log/ title=Game-Log class=md-nav__link> Game-Log </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-4 type=checkbox id=nav-4-4> <label class=md-nav__link for=nav-4-4> NKU-Toolkit <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=NKU-Toolkit data-md-level=2> <label class=md-nav__title for=nav-4-4> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> NKU-Toolkit </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/nku-eamis/ title=NKU-EAMIS工具 class=md-nav__link> NKU-EAMIS工具 </a> </li> <li class=md-nav__item> <a href=../../../../share/nku-sms-rss/ title=NKU-SMS-RSS class=md-nav__link> NKU-SMS-RSS </a> </li> <li class=md-nav__item> <a href=../../../../share/eamis-miniapp/ title=NKU-EAMIS_MiniApp(南开大学教务助手小程序) class=md-nav__link> NKU-EAMIS_MiniApp(南开大学教务助手小程序) </a> </li> <li class=md-nav__item> <a href=../../../../share/eamis-workflow/ title="NKU-EAMIS for iOS(Workflow)" class=md-nav__link> NKU-EAMIS for iOS(Workflow) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-5 type=checkbox id=nav-4-5> <label class=md-nav__link for=nav-4-5> Steam-Toolkit <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Steam-Toolkit data-md-level=2> <label class=md-nav__title for=nav-4-5> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Steam-Toolkit </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/steam-market-price-bot/ title=Steam市场比价爬虫 class=md-nav__link> Steam市场比价爬虫 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-6 type=checkbox id=nav-4-6> <label class=md-nav__link for=nav-4-6> 数学建模 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=数学建模 data-md-level=2> <label class=md-nav__title for=nav-4-6> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 数学建模 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/2017-mcm-icm/ title="2017美赛参赛整理(Problem D)" class=md-nav__link> 2017美赛参赛整理(Problem D) </a> </li> <li class=md-nav__item> <a href=../../../../share/2016-guosai/ title=2016数学建模国赛 class=md-nav__link> 2016数学建模国赛 </a> </li> <li class=md-nav__item> <a href=../../../../share/math-model-szb/ title=数学建模之2016深圳杯——初次尝试 class=md-nav__link> 数学建模之2016深圳杯——初次尝试 </a> </li> <li class=md-nav__item> <a href=../../../../share/polygon-to-ellipse/ title=随机多边形转化为椭圆的过程研究 class=md-nav__link> 随机多边形转化为椭圆的过程研究 </a> </li> <li class=md-nav__item> <a href=../../../../share/FFT-GPU-Accel/ title=FFT-GPU-Accel class=md-nav__link> FFT-GPU-Accel </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7 type=checkbox id=nav-4-7> <label class=md-nav__link for=nav-4-7> NKU 数院试题整理 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label="NKU 数院试题整理" data-md-level=2> <label class=md-nav__title for=nav-4-7> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> NKU 数院试题整理 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/nku-sms-exams/ title=汇总 class=md-nav__link> 汇总 </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7-2 type=checkbox id=nav-4-7-2> <label class=md-nav__link for=nav-4-7-2> 分析 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=分析 data-md-level=3> <label class=md-nav__title for=nav-4-7-2> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 分析 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/functional-analysis-final/ title=2017-2018第一学期泛函分析期末考试 class=md-nav__link> 2017-2018第一学期泛函分析期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/real-variable-function/ title=2016-2017第二学期实变函数期末考试 class=md-nav__link> 2016-2017第二学期实变函数期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-3-final/ title=2016-2017第一学期数学分析3-3期末考试 class=md-nav__link> 2016-2017第一学期数学分析3-3期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/complex-analysis-final/ title=2016-2017第一学期复变函数期末考试 class=md-nav__link> 2016-2017第一学期复变函数期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-3-middle/ title=2016-2017第一学期数学分析3-3期中考试 class=md-nav__link> 2016-2017第一学期数学分析3-3期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-2-final/ title=2015-2016第二学期数学分析3-2期末考试（含解答） class=md-nav__link> 2015-2016第二学期数学分析3-2期末考试（含解答） </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-2-middle/ title=2015-2016第二学期数学分析3-2期中考试 class=md-nav__link> 2015-2016第二学期数学分析3-2期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/mathematical-analysis-3-1-final/ title=2015-2016第一学期数学分析3-1期末考试 class=md-nav__link> 2015-2016第一学期数学分析3-1期末考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7-3 type=checkbox id=nav-4-7-3> <label class=md-nav__link for=nav-4-7-3> 代数 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=代数 data-md-level=3> <label class=md-nav__title for=nav-4-7-3> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 代数 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/abstract-algebra-final/ title=2016-2017第一学期抽象代数期末考试 class=md-nav__link> 2016-2017第一学期抽象代数期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/abstract-algebra-middle/ title=2016-2017第一学期抽象代数期中考试 class=md-nav__link> 2016-2017第一学期抽象代数期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/advanced-algebra-2-2-final/ title=2015-2016第二学期高等代数2-2期末考试 class=md-nav__link> 2015-2016第二学期高等代数2-2期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/advanced-algebra-2-2-middle/ title=2015-2016第二学期高等代数2-2期中考试 class=md-nav__link> 2015-2016第二学期高等代数2-2期中考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/advanced-algebra-2-1-final/ title=2015-2016第一学期高等代数2-1期末考试 class=md-nav__link> 2015-2016第一学期高等代数2-1期末考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7-4 type=checkbox id=nav-4-7-4> <label class=md-nav__link for=nav-4-7-4> 概率统计 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=概率统计 data-md-level=3> <label class=md-nav__title for=nav-4-7-4> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 概率统计 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/probability-final/ title=2016-2017第二学期概率论期末考试 class=md-nav__link> 2016-2017第二学期概率论期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/probability-middle/ title=2016-2017第二学期概率论期中考试 class=md-nav__link> 2016-2017第二学期概率论期中考试 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-7-5 type=checkbox id=nav-4-7-5> <label class=md-nav__link for=nav-4-7-5> 微分方程 <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=微分方程 data-md-level=3> <label class=md-nav__title for=nav-4-7-5> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> 微分方程 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/exam/PDE-final/ title=2017-2018第一学期数理方程期末考试 class=md-nav__link> 2017-2018第一学期数理方程期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/ODE-final/ title=2016-2017第一学期常微分方程期末考试 class=md-nav__link> 2016-2017第一学期常微分方程期末考试 </a> </li> <li class=md-nav__item> <a href=../../../../share/exam/ODE-middle/ title=2016-2017第一学期常微分方程期中考试 class=md-nav__link> 2016-2017第一学期常微分方程期中考试 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-8 type=checkbox id=nav-4-8> <label class=md-nav__link for=nav-4-8> Other <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Other data-md-level=2> <label class=md-nav__title for=nav-4-8> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Other </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../share/github-student-pack/ title="Student Developer Pack - GitHub Education" class=md-nav__link> Student Developer Pack - GitHub Education </a> </li> <li class=md-nav__item> <a href=../../../../share/my-postgraduate-share/ title="保研推免经验分享 - 数学系跨保 CS" class=md-nav__link> 保研推免经验分享 - 数学系跨保 CS </a> </li> <li class=md-nav__item> <a href=../../../../share/roc-fly/ title=鹏程万里 class=md-nav__link> 鹏程万里 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-5 type=checkbox id=nav-5> <label class=md-nav__link for=nav-5> Statements <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg> </span> </label> <nav class=md-nav aria-label=Statements data-md-level=1> <label class=md-nav__title for=nav-5> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Statements </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../statements/ title=Statements class=md-nav__link> Statements </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#111-semi-gradient-methods class=md-nav__link> 11.1 Semi-gradient Methods </a> <nav class=md-nav aria-label="11.1 Semi-gradient Methods"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#semi-gradient-off-policy-td0 class=md-nav__link> semi-gradient off-policy TD(0) </a> </li> <li class=md-nav__item> <a href=#semi-gradient-expected-sarsa class=md-nav__link> semi-gradient Expected Sarsa </a> </li> <li class=md-nav__item> <a href=#n-step-semi-gradient-expected-sarsa class=md-nav__link> n-step semi-gradient Expected Sarsa </a> </li> <li class=md-nav__item> <a href=#n-step-semi-gradient-tree-backup class=md-nav__link> n-step semi-gradient tree-backup </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#112-examples-of-off-policy-divergence class=md-nav__link> 11.2 Examples of Off-policy Divergence </a> </li> <li class=md-nav__item> <a href=#113-the-deadly-triad class=md-nav__link> 11.3 The Deadly Triad </a> </li> <li class=md-nav__item> <a href=#114-linear-value-function-geometry class=md-nav__link> 11.4 Linear Value-function Geometry </a> </li> <li class=md-nav__item> <a href=#115-gradient-descent-in-the-bellman-error class=md-nav__link> 11.5 Gradient Descent in the Bellman Error </a> </li> <li class=md-nav__item> <a href=#116-the-bellman-error-is-not-learnable class=md-nav__link> 11.6 The Bellman Error is Not Learnable </a> </li> <li class=md-nav__item> <a href=#117-gradient-td-methods class=md-nav__link> 11.7 Gradient-TD Methods </a> </li> <li class=md-nav__item> <a href=#118-emphatic-td-methods class=md-nav__link> 11.8 Emphatic-TD Methods </a> </li> <li class=md-nav__item> <a href=#119-reducing-variance class=md-nav__link> 11.9 Reducing Variance </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <a href=https://github.com/zawnpn/ZHANGWP/edit/master/docs/notes/reinforcement-learning/notes/RLAI_11.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg> </a> <h1 id=-off-policy>强化学习导论（十一）- Off-Policy的近似方法<a class=headerlink href=#-off-policy title="Permanent link">&para;</a></h1> <p>前两章（9、10 章）已经讲了on-policy 情形下对于函数近似的拓展，本章继续讲解 off-policy 下对函数近似的拓展，但是这个拓展比on-policy时更难更不同。</p> <p>在第六第七章中讲到的 off-policy 方法可以拓展到函数近似的情况下，但是这些方法在半梯度法下不能像在 on-policy 下一样良好地收敛。</p> <p>Off-policy 在函数逼近时有两大难点：</p> <ol> <li> <p>update target 发生变化。这个问题之前已通过 importance sampling 解决。</p> </li> <li> <p>update distribution 发生变化，已不再是原先的 on-policy distribution。</p> </li> </ol> <p>要解决上述的第二个难点，有两种方法：</p> <ul> <li>通过之前讲的 importance sampling 将 update distribution 转变为 on-policy distribution 。</li> <li>提出一种不依赖任何特定分布的 true gradient 方法。</li> </ul> <h2 id=111-semi-gradient-methods><strong>11.1 Semi-gradient Methods</strong><a class=headerlink href=#111-semi-gradient-methods title="Permanent link">&para;</a></h2> <p>这一节主要目的是将 off-policy 下的查表法改造为梯度 / 半梯度法，主要针对第一个难点（变化的 update target）。大多数情况下，这个方法表现良好，少数情况存在发散的情况。</p> <p>这些算法大多数采用了『<strong>单步重要性比例</strong>』：</p> <div> <div class=MathJax_Preview> \rho_t\doteq\rho_{t:t}=\frac{\pi(A_t|S_t)}{b(A_t|S_t)} </div> <script type="math/tex; mode=display">
\rho_t\doteq\rho_{t:t}=\frac{\pi(A_t|S_t)}{b(A_t|S_t)}
</script> </div> <h3 id=semi-gradient-off-policy-td0><strong>semi-gradient off-policy TD(0)</strong><a class=headerlink href=#semi-gradient-off-policy-td0 title="Permanent link">&para;</a></h3> <div> <div class=MathJax_Preview> \mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\rho_t\delta_t\nabla\hat{v}(S_t,\mathbf{w}_t) </div> <script type="math/tex; mode=display">
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\rho_t\delta_t\nabla\hat{v}(S_t,\mathbf{w}_t)
</script> </div> <p>其中</p> <ul> <li>episodic and discounted problem:</li> </ul> <div> <div class=MathJax_Preview> \delta_t\doteq R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t) </div> <script type="math/tex; mode=display">
\delta_t\doteq R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t)
</script> </div> <ul> <li>continuing and undiscounted problem:</li> </ul> <div> <div class=MathJax_Preview> \delta_t\doteq R_{t+1}-\bar{R}_t+\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t) </div> <script type="math/tex; mode=display">
\delta_t\doteq R_{t+1}-\bar{R}_t+\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t)
</script> </div> <h3 id=semi-gradient-expected-sarsa><strong>semi-gradient Expected Sarsa</strong><a class=headerlink href=#semi-gradient-expected-sarsa title="Permanent link">&para;</a></h3> <div> <div class=MathJax_Preview> \mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t\nabla\hat{q}(S_t,A_t,\mathbf{w}_t) </div> <script type="math/tex; mode=display">
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t\nabla\hat{q}(S_t,A_t,\mathbf{w}_t)
</script> </div> <p>其中</p> <ul> <li>episodic and discounted problem:</li> </ul> <div> <div class=MathJax_Preview> \delta_t\doteq R_{t+1}+\gamma\sum_a\pi(a|S_{t+1})\hat{q}(S_{t+1},a,\mathbf{w}_t)-\hat{q}(S_t,A_t,\mathbf{w}_t) </div> <script type="math/tex; mode=display">
\delta_t\doteq R_{t+1}+\gamma\sum_a\pi(a|S_{t+1})\hat{q}(S_{t+1},a,\mathbf{w}_t)-\hat{q}(S_t,A_t,\mathbf{w}_t)
</script> </div> <ul> <li>continuing and undiscounted problem:</li> </ul> <div> <div class=MathJax_Preview> \delta_t\doteq R_{t+1}-\bar{R}_t+\sum_a\pi(a|S_{t+1})\hat{q}(S_{t+1},a,\mathbf{w}_t)-\hat{q}(S_t,A_t,\mathbf{w}_t) </div> <script type="math/tex; mode=display">
\delta_t\doteq R_{t+1}-\bar{R}_t+\sum_a\pi(a|S_{t+1})\hat{q}(S_{t+1},a,\mathbf{w}_t)-\hat{q}(S_t,A_t,\mathbf{w}_t)
</script> </div> <p>这里梯度更新并未使用 importance sampling ，后面会解释。</p> <p>上面都是针对单步算法，而对于多步算法，无论是 state value 还是 action value ，都需要做 importance sampling 。</p> <h3 id=n-step-semi-gradient-expected-sarsa><strong>n-step semi-gradient Expected Sarsa</strong><a class=headerlink href=#n-step-semi-gradient-expected-sarsa title="Permanent link">&para;</a></h3> <div> <div class=MathJax_Preview> \begin{aligned} \mathbf{w}_{t+n}&amp;\doteq\mathbf{w}_{t+n-1}+\alpha\prod_{k=t+1}^{t+n}\rho_k\delta_{t:t+n}\nabla\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1})\\ \delta_{t:t+n}&amp;\doteq G_{t:t+n}-\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1}) \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\mathbf{w}_{t+n}&\doteq\mathbf{w}_{t+n-1}+\alpha\prod_{k=t+1}^{t+n}\rho_k\delta_{t:t+n}\nabla\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1})\\
\delta_{t:t+n}&\doteq G_{t:t+n}-\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1})
\end{aligned}
</script> </div> <p>其中</p> <ul> <li>episodic and discounted problem:</li> </ul> <div> <div class=MathJax_Preview> G_{t:t+n}\doteq R_{t+1}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{q}(S_{t+n},A_{t+n},\mathbf{w}_{t+n-1}) </div> <script type="math/tex; mode=display">
G_{t:t+n}\doteq R_{t+1}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{q}(S_{t+n},A_{t+n},\mathbf{w}_{t+n-1})
</script> </div> <ul> <li>continuing and undiscounted problem:</li> </ul> <div> <div class=MathJax_Preview> G_{t:t+n}\doteq R_{t+1}-\bar{R}_t+\cdots+R_{t+n}-\bar{R}_{t+n-1}+\hat{q}(S_{t+n},A_{t+n},\mathbf{w}_{t+n-1}) </div> <script type="math/tex; mode=display">
G_{t:t+n}\doteq R_{t+1}-\bar{R}_t+\cdots+R_{t+n}-\bar{R}_{t+n-1}+\hat{q}(S_{t+n},A_{t+n},\mathbf{w}_{t+n-1})
</script> </div> <h3 id=n-step-semi-gradient-tree-backup><strong>n-step semi-gradient tree-backup</strong><a class=headerlink href=#n-step-semi-gradient-tree-backup title="Permanent link">&para;</a></h3> <p>第七章还讲过一种不需要 importance sampling 的算法：tree-backup 算法，其半梯度法如下：</p> <div> <div class=MathJax_Preview> \begin{aligned} \mathbf{w}_{t+n}&amp;\doteq\mathbf{w}_{t+n-1}+\alpha[G_{t:t+n}-\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1})]\nabla\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1})\\ G_{t:t+n}&amp;\doteq\hat{q}(S_t,A_t,\mathbf{w}_{t-1})+\sum_{k=t}^{t+n-1}\delta_k\prod_{i=t+1}^k\gamma\pi(A_i|S_i) \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\mathbf{w}_{t+n}&\doteq\mathbf{w}_{t+n-1}+\alpha[G_{t:t+n}-\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1})]\nabla\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1})\\
G_{t:t+n}&\doteq\hat{q}(S_t,A_t,\mathbf{w}_{t-1})+\sum_{k=t}^{t+n-1}\delta_k\prod_{i=t+1}^k\gamma\pi(A_i|S_i)
\end{aligned}
</script> </div> <h2 id=112-examples-of-off-policy-divergence><strong>11.2 Examples of Off-policy Divergence</strong><a class=headerlink href=#112-examples-of-off-policy-divergence title="Permanent link">&para;</a></h2> <p>从本节开始讨论第二类难点，也就是 update distribution 不再是 on-policy distribution 。本节主要是讲了 off-policy 下使用半梯度法导致不稳定或不收敛的反例。</p> <p>例子的具体情况略过，其结论是，参数 <span><span class=MathJax_Preview>\mathbf{w}</span><script type=math/tex>\mathbf{w}</script></span> 更新的稳定性与步长参数 <span><span class=MathJax_Preview>\alpha</span><script type=math/tex>\alpha</script></span> 无关，只需大于 0 即可，而其值的大小只影响参数 <span><span class=MathJax_Preview>\mathbf{w}</span><script type=math/tex>\mathbf{w}</script></span> 发散的速度，而非是否发散。</p> <p>本例的一个特殊点是，它一直在重复一个状态转移来更新 <span><span class=MathJax_Preview>\mathbf{w}</span><script type=math/tex>\mathbf{w}</script></span> （这也是实际中可能发生的情况），因为 behavior policy 可能会选择 target policy 永远不会选择的那些 action（此时 <span><span class=MathJax_Preview>\rho_t=0</span><script type=math/tex>\rho_t=0</script></span> ，权重得不到更新）。</p> <p>还有一个反例——Baird's counterexmaple ，这个例子主要是在说，bootstrapping 和 semi gradient 在非 on-policy 下结合时，会导致发散。</p> <p>Q-learning 往往是收敛性最好的算法，但仍有使用 Q-learning 也发散的反例，一个解决方案是使 behavior policy 与 target policy 尽量<strong>接近</strong>（比如将 behavior policy 设为 target policy 的 <span><span class=MathJax_Preview>\varepsilon</span><script type=math/tex>\varepsilon</script></span>-greedy policy ）。</p> <h2 id=113-the-deadly-triad><strong>11.3 The Deadly Triad</strong><a class=headerlink href=#113-the-deadly-triad title="Permanent link">&para;</a></h2> <p>上一节对存在不稳定性的情况举了例子，本节再来做一个归纳总结。</p> <p>导致不稳定性有三个主要因素，称其为『<strong>致命三因素</strong>（The Deadly Triad）』：</p> <ol> <li>Function Approximation</li> <li>Bootstrapping</li> <li>Off-policy training</li> </ol> <p>结论：『<strong>当三者同时出现，会导致系统不稳定；只出现两个时则可避免不稳定性。</strong>』</p> <p>关于三者的取舍情况，首先 function approximation <strong>最需要保留</strong>，他能够使我们的算法得到足够的<strong>扩展和延伸</strong>，变得更有<strong>泛化能力</strong>。</p> <p>而 bootstrapping 是可以考虑放弃掉的，但代价是牺牲<strong>计算效率和数据利用率</strong>（bootstrapping 可以利用终止状态之前的数据来进行中途学习，所以效率高）。</p> <p>最后， off-policy 能够将行为从目标函数分隔开，能够带来一定程度上的便利，但并非是必须的。不过若想要『<strong>并行学习</strong>』，则一定要使用 off-policy 。</p> <h2 id=114-linear-value-function-geometry><strong>11.4 Linear Value-function Geometry</strong><a class=headerlink href=#114-linear-value-function-geometry title="Permanent link">&para;</a></h2> <p>为了更好理解 off-policy learning 的一些问题，考虑对函数逼近做一些抽象的分析。</p> <p>设状态空间中的 state-value function 为映射 <span><span class=MathJax_Preview>v:S\to R</span><script type=math/tex>v:S\to R</script></span> （大部分的 v 函数并没有具体意义，即不对应任何具体的 policy ） 。</p> <p>记状态空间为 <span><span class=MathJax_Preview>\mathcal S=\{s_1,s_2,\ldots,s_{|\mathcal S|}\}</span><script type=math/tex>\mathcal S=\{s_1,s_2,\ldots,s_{|\mathcal S|}\}</script></span> ，value function 则为向量 <span><span class=MathJax_Preview>[v(s_1),v(s_2),\ldots,v(s_{|\mathcal S|})]^T</span><script type=math/tex>[v(s_1),v(s_2),\ldots,v(s_{|\mathcal S|})]^T</script></span> 。</p> <p>简化起见，设 <span><span class=MathJax_Preview>\mathcal S=\{s_1,s_2,s_3\}</span><script type=math/tex>\mathcal S=\{s_1,s_2,s_3\}</script></span> ，参数 <span><span class=MathJax_Preview>\mathbf{w}=(w_1,w_2)^T</span><script type=math/tex>\mathbf{w}=(w_1,w_2)^T</script></span> ，此时 value function <span><span class=MathJax_Preview>[v(s_1),v(s_2),v(s_3)]^T</span><script type=math/tex>[v(s_1),v(s_2),v(s_3)]^T</script></span> 可看作一个三维空间中的点。而参数 <span><span class=MathJax_Preview>\mathbf{w}</span><script type=math/tex>\mathbf{w}</script></span> 则能够通过一个二维子空间提供一个替代的坐标系，其线性组合而成的逼近函数 <span><span class=MathJax_Preview>v_{\mathbf{w}}</span><script type=math/tex>v_{\mathbf{w}}</script></span> 显然也在这个子空间内。</p> <p>下图是一个状态空间的示例，一些具体的含义会逐渐通过后续的小节来解释。</p> <p><img alt src=../imgs/RLAI_11/space.png></p> <p>给定一个策略 <span><span class=MathJax_Preview>\pi</span><script type=math/tex>\pi</script></span> ，其对应的 <span><span class=MathJax_Preview>v_\pi</span><script type=math/tex>v_\pi</script></span> 可能较复杂，因此难以被参数的线性组合表示出来，故 <span><span class=MathJax_Preview>v_\pi</span><script type=math/tex>v_\pi</script></span> 可能在参数化的子平面外，而 approximation 要做的事情，就是在这个子平面中找到最接近离真实 <span><span class=MathJax_Preview>v_\pi</span><script type=math/tex>v_\pi</script></span> 最近的逼近函数 <span><span class=MathJax_Preview>v_\mathbf{w}</span><script type=math/tex>v_\mathbf{w}</script></span> 。</p> <p>为了衡量 value function 之间的距离，这里定义一个距离</p> <div> <div class=MathJax_Preview> ||v||_\mu^2\doteq\sum_{s\in\mathcal S}\mu(s)v(s)^2 </div> <script type="math/tex; mode=display">
||v||_\mu^2\doteq\sum_{s\in\mathcal S}\mu(s)v(s)^2
</script> </div> <p>将 value function 投影到子空间最近函数的运算定义为算子 <span><span class=MathJax_Preview>\Pi</span><script type=math/tex>\Pi</script></span> ：</p> <div> <div class=MathJax_Preview> \Pi v\doteq v_{\mathbf{w}}\quad\mathrm{where}\quad\mathbf{w}=\arg\min_\mathbf{w}||v-v_{\mathbf{w}}||_\mu^2 </div> <script type="math/tex; mode=display">
\Pi v\doteq v_{\mathbf{w}}\quad\mathrm{where}\quad\mathbf{w}=\arg\min_\mathbf{w}||v-v_{\mathbf{w}}||_\mu^2
</script> </div> <p>对于一个线性估计而言，投影算子可表示为矩阵（下式之中若不可求逆，则用伪逆）</p> <div> <div class=MathJax_Preview> \Pi \doteq \mathbf{X}(\mathbf{X}^T\mathbf{D}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{D} </div> <script type="math/tex; mode=display">
\Pi \doteq \mathbf{X}(\mathbf{X}^T\mathbf{D}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{D}
</script> </div> <p><span><span class=MathJax_Preview>\mathbf{D}</span><script type=math/tex>\mathbf{D}</script></span> 为分布 <span><span class=MathJax_Preview>\mu(s)</span><script type=math/tex>\mu(s)</script></span> 的对角矩阵形式，<span><span class=MathJax_Preview>\mathbf{X}</span><script type=math/tex>\mathbf{X}</script></span> 则由特征向量组成</p> <div> <div class=MathJax_Preview> \mathbf{D}=\left[\begin{array}{cccc} \mu(s_1)&amp;&amp;&amp;\\ &amp;\mu(s_2)&amp;&amp;\\ &amp;&amp;\ddots&amp;\\ &amp;&amp;&amp;\mu(s_{|\mathcal S|}) \end{array}\right],\quad \mathbf{X}=\left[\begin{array}{c} \mathbf{x}(s_1)^T\\ \mathbf{x}(s_2)^T\\ \vdots\\ \mathbf{x}(s_{|\mathcal S|})^T \end{array}\right] </div> <script type="math/tex; mode=display">
\mathbf{D}=\left[\begin{array}{cccc}
\mu(s_1)&&&\\
&\mu(s_2)&&\\
&&\ddots&\\
&&&\mu(s_{|\mathcal S|})
\end{array}\right],\quad
\mathbf{X}=\left[\begin{array}{c}
\mathbf{x}(s_1)^T\\
\mathbf{x}(s_2)^T\\
\vdots\\
\mathbf{x}(s_{|\mathcal S|})^T
\end{array}\right]
</script> </div> <p>使用这两个矩阵，还可以改写出 <span><span class=MathJax_Preview>||v||_\mu^2=v^T\mathbf{D}v</span><script type=math/tex>||v||_\mu^2=v^T\mathbf{D}v</script></span> ，<span><span class=MathJax_Preview>v_\mathbf{w}=\mathbf{Xw}</span><script type=math/tex>v_\mathbf{w}=\mathbf{Xw}</script></span> 。</p> <p>回想之前求解 Bellman 方程</p> <div> <div class=MathJax_Preview> v_\pi(s)=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')],\forall s\in \mathcal S </div> <script type="math/tex; mode=display">
v_\pi(s)=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')],\forall s\in \mathcal S
</script> </div> <p>若将 <span><span class=MathJax_Preview>v_\mathbf{w}</span><script type=math/tex>v_\mathbf{w}</script></span> 用以替代 <span><span class=MathJax_Preview>v_\pi</span><script type=math/tex>v_\pi</script></span> ，显然等号不再成立，于是可定义 Bellman Error (BE)：</p> <div> <div class=MathJax_Preview> \begin{aligned} \bar{\delta}_\mathbf{w}(s)&amp;=\left(\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_\mathbf{w}(s')]\right)-v_\mathbf{w}(s)\\ &amp;=\mathbb{E}[R_{t+1}+\gamma v_\mathbf{w}(S_{t+1})-v_\mathbf{w}(S_t)|S_t=s,A_t\sim\pi] \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\bar{\delta}_\mathbf{w}(s)&=\left(\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_\mathbf{w}(s')]\right)-v_\mathbf{w}(s)\\
&=\mathbb{E}[R_{t+1}+\gamma v_\mathbf{w}(S_{t+1})-v_\mathbf{w}(S_t)|S_t=s,A_t\sim\pi]
\end{aligned}
</script> </div> <p>易观察知，Bellman error 其实就是 TD error 的期望值。</p> <p>可定义 <strong>Mean Squared Bellman Error</strong>： <span><span class=MathJax_Preview>\mathrm{MSBE}(\mathbf{w})=||\bar{\delta}_\mathbf{w}||_\mu^2</span><script type=math/tex>\mathrm{MSBE}(\mathbf{w})=||\bar{\delta}_\mathbf{w}||_\mu^2</script></span> 。从图 11.3 易知，线性逼近无法使 MSBE 减小至 0（需要 <span><span class=MathJax_Preview>v_\mathbf{w}=v_\pi</span><script type=math/tex>v_\mathbf{w}=v_\pi</script></span> ），后面两节会介绍如何最小化这个 MSBE 。</p> <p>为简化描述，定义 Bellman 算子 <span><span class=MathJax_Preview>B_\pi:\mathbb{R}^{|\mathcal S|}\to \mathbb{R}^{|\mathcal S|}​</span><script type=math/tex>B_\pi:\mathbb{R}^{|\mathcal S|}\to \mathbb{R}^{|\mathcal S|}​</script></span> ，将 Bellman 方程记作算子形式：</p> <div> <div class=MathJax_Preview> (B_\pi v)(s)\doteq \sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v(s')] </div> <script type="math/tex; mode=display">
(B_\pi v)(s)\doteq \sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v(s')]
</script> </div> <p>此时可将 Bellman error 记作 <span><span class=MathJax_Preview>\bar{\delta}_\mathbf{w}=B_\pi v_\mathbf{w}-v_\mathbf{w}</span><script type=math/tex>\bar{\delta}_\mathbf{w}=B_\pi v_\mathbf{w}-v_\mathbf{w}</script></span> 。</p> <p><span><span class=MathJax_Preview>B_\pi</span><script type=math/tex>B_\pi</script></span> 能够产生子空间外新的 value function ，不断作用于 value function ，这有点类似 DP 法，它能够最终收敛到想要的 <span><span class=MathJax_Preview>v_\pi</span><script type=math/tex>v_\pi</script></span> ，如图 11.3 中所示。</p> <p>同样可以将 Bellman error 投影到参数子空间，得到 <span><span class=MathJax_Preview>\Pi \bar{\delta}_{v_\mathbf{w}}</span><script type=math/tex>\Pi \bar{\delta}_{v_\mathbf{w}}</script></span> ，此时可定义 <strong>Mean Square Projected Bellman Error</strong>：</p> <div> <div class=MathJax_Preview> \mathrm{MSPBE}(\mathbf{w})=||\Pi\bar{\delta}_\mathbf{w}||_\mu^2 </div> <script type="math/tex; mode=display">
\mathrm{MSPBE}(\mathbf{w})=||\Pi\bar{\delta}_\mathbf{w}||_\mu^2
</script> </div> <p>对于线性逼近而言，这时显然就可以在子空间内找到使 MSPBE 为 0 的最优点了，这个点恰好就是前面讲过的 TD 不动点。</p> <h2 id=115-gradient-descent-in-the-bellman-error><strong>11.5 Gradient Descent in the Bellman Error</strong><a class=headerlink href=#115-gradient-descent-in-the-bellman-error title="Permanent link">&para;</a></h2> <p>前一节介绍了多种目标函数（MSVE、MSBE、MSPBE 等），这一节回到 off-policy learning 问题。</p> <p>若想使这些目标函数最小化，一般考虑 SGD 来处理，但前面讲过，只有 MC 才是 true SGD ，此时无论是 on-policy 还是 off-policy 其收敛性都很鲁棒，只不过收敛速度较半梯度法稍慢，而半梯度法则在 off-policy 训练中容易发散，且不太适合用于非线性逼近，true SGD 就不存在这种问题。</p> <p>11.5 &amp; 11.6 节以 Bellman error 为目标函数来做优化，不过需要先说明的是，这种算法其实并不好，它的失败之处其实很有意思，能够为我们找到好方法提供思路。</p> <p>首先，在以前的 TD 法中定义过一个 TD error：<span><span class=MathJax_Preview>\delta_t=R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t)</span><script type=math/tex>\delta_t=R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t)</script></span> ，但并未以它为优化目标来研究过，于是定义『<strong>均方 TD error</strong>』：</p> <div> <div class=MathJax_Preview> \begin{aligned} \mathrm{MSTDE}(\mathbf{w})&amp;=\sum_{s\in\mathcal S}\mu(s)\mathbb{E}[\delta_t^2|S_t=s,A_t\sim\pi]\\ &amp;=\sum_{s\in\mathcal S}\mu(s)\mathbb{E}[\rho_t\delta_t^2|S_t=s,A_t\sim b]\\ &amp;=\mathbb{E}_b[\rho_t\delta_t^2] \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\mathrm{MSTDE}(\mathbf{w})&=\sum_{s\in\mathcal S}\mu(s)\mathbb{E}[\delta_t^2|S_t=s,A_t\sim\pi]\\
&=\sum_{s\in\mathcal S}\mu(s)\mathbb{E}[\rho_t\delta_t^2|S_t=s,A_t\sim b]\\
&=\mathbb{E}_b[\rho_t\delta_t^2]
\end{aligned}
</script> </div> <p>此时的 SGD 更新式为</p> <div> <div class=MathJax_Preview> \begin{aligned} \mathbf{w}_{t+1}&amp;=\mathbf{w}_t-\frac{1}{2}\alpha\nabla(\rho_t\delta_t^2)\\ &amp;=\mathbf{w}_t-\alpha\rho_t\delta_t\nabla\delta_t\\ &amp;=\mathbf{w}_t+\alpha\rho_t\delta_t(\nabla\hat{v}(S_t,\mathbf{w}_t)-\gamma\nabla\hat{v}(S_{t+1},\mathbf{w}_t)) \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\mathbf{w}_{t+1}&=\mathbf{w}_t-\frac{1}{2}\alpha\nabla(\rho_t\delta_t^2)\\
&=\mathbf{w}_t-\alpha\rho_t\delta_t\nabla\delta_t\\
&=\mathbf{w}_t+\alpha\rho_t\delta_t(\nabla\hat{v}(S_t,\mathbf{w}_t)-\gamma\nabla\hat{v}(S_{t+1},\mathbf{w}_t))
\end{aligned}
</script> </div> <p>这是一个 true SGD 法，称其为『<strong>naive residual-gradient 算法</strong>』。此方法虽然收敛性很鲁棒，但其实它收敛到的值并不是理想值，下面的一个例子具体地展现了这一点（真实最优解的 MSTDE 反而更大）。</p> <p><img alt src=../imgs/RLAI_11/ex11-2.png></p> <p>另一个更好的想法是优化 Bellman error，也就是 TD error 的期望，称该算法为『<strong>residual gradient 算法</strong>』其更新式为</p> <p><img alt src=../imgs/RLAI_11/BE-SGD.png></p> <p>从中看出，此更新式中有两个含有 <span><span class=MathJax_Preview>S_{t+1}</span><script type=math/tex>S_{t+1}</script></span> 的期望式，为保证无偏性，这两个 <span><span class=MathJax_Preview>S_{t+1}</span><script type=math/tex>S_{t+1}</script></span> 应该是独立的，所以需要<strong>在每一步都采两个样本</strong>，如果环境是<strong>确定性</strong>的，那么采取同一 action 后 <span><span class=MathJax_Preview>S_t\to S_{t+1}</span><script type=math/tex>S_t\to S_{t+1}</script></span> 的过程便是确定的，两处的 <span><span class=MathJax_Preview>S_{t+1}</span><script type=math/tex>S_{t+1}</script></span> 也必然是相同值，故做一次采样即可；但若是<strong>非确定性</strong>的环境，则必须采两次样，这在<strong>真实环境</strong>中无法做到（一旦和环境交互就已确定，不可回退），只能在<strong>模拟环境</strong>中通过回退再次模拟来实现。</p> <p>这个算法是 true SGD，故也有较强的收敛性，但有三个缺点：</p> <ul> <li>比半梯度法慢</li> <li>可能收敛到错误值（如例 11.3 所示）</li> <li>可能不收敛（下一节讲）</li> </ul> <p><img alt src=../imgs/RLAI_11/ex11-3.png></p> <h2 id=116-the-bellman-error-is-not-learnable><strong>11.6 The Bellman Error is Not Learnable</strong><a class=headerlink href=#116-the-bellman-error-is-not-learnable title="Permanent link">&para;</a></h2> <p>本节的『<strong>可学习</strong>（learnable）』与传统机器学习中的 learnable 定义（能够在多项式复杂度下有效地学习）不同，在强化学习中，若一些量<strong>在给定内部结构、知识等信息后可以计算，但通过观测得到的序列却无法计算或估计出来</strong>，则称这些量是<strong>不可学习</strong>的。</p> <p><img alt src=../imgs/RLAI_11/ex-learnable.png></p> <p>上面的例子中，两个不同的问题却有可能产生出相同的观测序列。若设 <span><span class=MathJax_Preview>\gamma = 0</span><script type=math/tex>\gamma = 0</script></span> ，三个 state 的 true value 应为 1、0、2 ，若设 <span><span class=MathJax_Preview>w=1</span><script type=math/tex>w=1</script></span> ，则左图的 MSVE 为 0 ，右图的 MSVE 为 1 。同样的观测序列，对应的理论 MSVE 值却不同，说明如果不给出问题背景，就无法学出正确对应的 MSVE ，因此 MSVE 就是 not learnable 的。</p> <p>MSVE 仍然有一定使用价值，事实上，有着相同分布的 MDP 问题的<strong>最优参数其实是相同的</strong>，利用这个特殊性质，仍然可以采用 MSVE 作为目标函数来进行优化。</p> <p>为更好理解，下面引入一个可学习的 『<strong>Mean Square Return Error</strong>』来探讨，他在 on-policy 下写作</p> <div> <div class=MathJax_Preview> \begin{aligned} \mathrm{MSRE}(\mathbf{w})&amp;=\mathbb{E}[(G_t-\hat{v}(S_t,\mathbf{w}))^2]\\ &amp;=\mathrm{MSVE}(\mathbf{w})+\mathbb{E}[(G_t-v_\pi(S_t))^2] \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\mathrm{MSRE}(\mathbf{w})&=\mathbb{E}[(G_t-\hat{v}(S_t,\mathbf{w}))^2]\\
&=\mathrm{MSVE}(\mathbf{w})+\mathbb{E}[(G_t-v_\pi(S_t))^2]
\end{aligned}
</script> </div> <p>可以看出，MSRE 比 MSVE 多出一项与参数 <span><span class=MathJax_Preview>\mathbf{w}</span><script type=math/tex>\mathbf{w}</script></span> 无关的项，因此两种目标函数对应的最优参数 <span><span class=MathJax_Preview>\mathbf{w}^*</span><script type=math/tex>\mathbf{w}^*</script></span> 是相同的，而 MSRE 又是可学习的，故事实上仍可用 MSVE 来做优化。他们的关系如下图所示。</p> <p><img alt src=../imgs/RLAI_11/MSVE-MSRE.png></p> <p>再来看 MSBE ，他和 MSVE 一样，可由 MDP 问题结构信息计算求得，而无法通过观测 / 经验数据来进行学习。但与 MSVE 不同的是，观测序列的分布相同时，求出的最优解不再相同。下面的例子描述了这一情况。</p> <p><img alt src=../imgs/RLAI_11/ex11-4.png></p> <p>此外，另两种 bootstrapping 目标函数 MSPBE、MSTDE 可由 data 学习（learnable），他们的关系如下图所示。</p> <p><img alt src=../imgs/RLAI_11/PBE-TDE.png></p> <p>MSBE 由于不可学习，故只能用于 model-based learning，residual-gradient 是唯一能最小化 MSBE 的算法，需要对同一 state 做两次采样，对环境信息依赖程度较大，故此方法局限性较高。</p> <h2 id=117-gradient-td-methods><strong>11.7 Gradient-TD Methods</strong><a class=headerlink href=#117-gradient-td-methods title="Permanent link">&para;</a></h2> <p>现考虑最小化 MSPBE 的 SGD 方法，下面介绍推导复杂度为 <span><span class=MathJax_Preview>O(d)</span><script type=math/tex>O(d)</script></span> 的算法。</p> <p>首先，将 MSPBE 写作矩阵形式</p> <div> <div class=MathJax_Preview> \begin{aligned} \mathrm{MSPBE}(\mathbf{w})&amp;=||\Pi\bar{\delta}_\mathbf{w}||_\mu^2\\ &amp;=(\Pi\bar{\delta}_\mathbf{w})^T\mathbf{D}\Pi\bar{\delta}_\mathbf{w}\\ &amp;=\bar{\delta}_\mathbf{w}^T\Pi^T\mathbf{D}\Pi\bar{\delta}_\mathbf{w}\\ \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\mathrm{MSPBE}(\mathbf{w})&=||\Pi\bar{\delta}_\mathbf{w}||_\mu^2\\
&=(\Pi\bar{\delta}_\mathbf{w})^T\mathbf{D}\Pi\bar{\delta}_\mathbf{w}\\
&=\bar{\delta}_\mathbf{w}^T\Pi^T\mathbf{D}\Pi\bar{\delta}_\mathbf{w}\\
\end{aligned}
</script> </div> <p>由于前面已定义 <span><span class=MathJax_Preview>\Pi = \mathbf{X}(\mathbf{X}^T\mathbf{D}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{D}</span><script type=math/tex>\Pi = \mathbf{X}(\mathbf{X}^T\mathbf{D}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{D}</script></span> ，故有（注意 <span><span class=MathJax_Preview>\mathbf{D}</span><script type=math/tex>\mathbf{D}</script></span> 是对称矩阵）</p> <div> <div class=MathJax_Preview> \begin{aligned} \Pi^T\mathbf{D}\Pi&amp;=[\mathbf{X}(\mathbf{X}^T\mathbf{D}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{D}]^T\mathbf{D}[\mathbf{X}(\mathbf{X}^T\mathbf{D}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{D}]\\ &amp;=\mathbf{DX}(\mathbf{X}^T\mathbf{DX})^{-1}\mathbf{X}^T\mathbf{D}\mathbf{X}(\mathbf{X}^T\mathbf{D}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{D}\\ &amp;=\mathbf{DX}(\mathbf{X}^T\mathbf{D}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{D} \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\Pi^T\mathbf{D}\Pi&=[\mathbf{X}(\mathbf{X}^T\mathbf{D}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{D}]^T\mathbf{D}[\mathbf{X}(\mathbf{X}^T\mathbf{D}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{D}]\\
&=\mathbf{DX}(\mathbf{X}^T\mathbf{DX})^{-1}\mathbf{X}^T\mathbf{D}\mathbf{X}(\mathbf{X}^T\mathbf{D}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{D}\\
&=\mathbf{DX}(\mathbf{X}^T\mathbf{D}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{D}
\end{aligned}
</script> </div> <p>回到前式即得</p> <div> <div class=MathJax_Preview> \mathrm{MSPBE}=(\mathbf{X}^T\mathbf{D}\bar{\delta}_\mathbf{w})^{T}(\mathbf{X}^T\mathbf{D}\mathbf{X})^{-1}(\mathbf{X}^T\mathbf{D}\bar{\delta}_\mathbf{w}) </div> <script type="math/tex; mode=display">
\mathrm{MSPBE}=(\mathbf{X}^T\mathbf{D}\bar{\delta}_\mathbf{w})^{T}(\mathbf{X}^T\mathbf{D}\mathbf{X})^{-1}(\mathbf{X}^T\mathbf{D}\bar{\delta}_\mathbf{w})
</script> </div> <p>则其梯度为</p> <div> <div class=MathJax_Preview> \nabla\mathrm{MSPBE}(\mathbf{w})=[2\nabla(\mathbf{X}^T\mathbf{D}\bar{\delta}_\mathbf{w})^{T}](\mathbf{X}^T\mathbf{D}\mathbf{X})^{-1}(\mathbf{X}^T\mathbf{D}\bar{\delta}_\mathbf{w}) </div> <script type="math/tex; mode=display">
\nabla\mathrm{MSPBE}(\mathbf{w})=[2\nabla(\mathbf{X}^T\mathbf{D}\bar{\delta}_\mathbf{w})^{T}](\mathbf{X}^T\mathbf{D}\mathbf{X})^{-1}(\mathbf{X}^T\mathbf{D}\bar{\delta}_\mathbf{w})
</script> </div> <p><span><span class=MathJax_Preview>\mu</span><script type=math/tex>\mu</script></span> 是 behavior policy 下的状态分布，故上式中的各部分都可表示为该分布下的期望：</p> <div> <div class=MathJax_Preview> \begin{aligned} \mathbf{X}^T\mathbf{D}\bar{\delta}_\mathbf{w}&amp;=\sum_s\mu(s)\mathbf{x}(s)\bar{\delta}_\mathbf{w}(s)=\mathbb{E}[\rho_t\delta_t\mathbf{x}_t]\\ \nabla(\mathbf{X}^T\mathbf{D}\bar{\delta}_\mathbf{w})^T&amp;=\mathbb{E}[\rho_t\delta_t\mathbf{x}_t]^T=\mathbb{E}[\rho_t\nabla\delta_t^T\mathbf{x}_t^T]\\ &amp;=\mathbb{E}[\rho_t\nabla(R_{t+1}+\gamma\mathbf{w}^T\mathbf{x}_{t+1}-\mathbf{w}^T\mathbf{x}_{t})^T\mathbf{x}_t^T]\\ &amp;=\mathbb{E}[\rho_t(\gamma\mathbf{x}_{t+1}-\mathbf{x}_t)\mathbf{x}_t^T]\\ \mathbf{X}^T\mathbf{DX}&amp;=\sum_s\mu(s)\mathbf{x}_s\mathbf{x}_s^T=\mathbb{E}[\mathbf{x}_t\mathbf{x}_t^T] \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\mathbf{X}^T\mathbf{D}\bar{\delta}_\mathbf{w}&=\sum_s\mu(s)\mathbf{x}(s)\bar{\delta}_\mathbf{w}(s)=\mathbb{E}[\rho_t\delta_t\mathbf{x}_t]\\
\nabla(\mathbf{X}^T\mathbf{D}\bar{\delta}_\mathbf{w})^T&=\mathbb{E}[\rho_t\delta_t\mathbf{x}_t]^T=\mathbb{E}[\rho_t\nabla\delta_t^T\mathbf{x}_t^T]\\
&=\mathbb{E}[\rho_t\nabla(R_{t+1}+\gamma\mathbf{w}^T\mathbf{x}_{t+1}-\mathbf{w}^T\mathbf{x}_{t})^T\mathbf{x}_t^T]\\
&=\mathbb{E}[\rho_t(\gamma\mathbf{x}_{t+1}-\mathbf{x}_t)\mathbf{x}_t^T]\\
\mathbf{X}^T\mathbf{DX}&=\sum_s\mu(s)\mathbf{x}_s\mathbf{x}_s^T=\mathbb{E}[\mathbf{x}_t\mathbf{x}_t^T]
\end{aligned}
</script> </div> <p>代回前式，即得</p> <div> <div class=MathJax_Preview> \nabla\mathrm{MSPBE}(\mathbf{w})=2\mathbb{E}[\rho_t(\gamma\mathbf{x}_{t+1}-\mathbf{x}_t)\mathbf{x}_t^T]\mathbb{E}[\mathbf{x}_t\mathbf{x}_t^T]^{-1}\mathbb{E}[\rho_t\delta_t\mathbf{x}_t] </div> <script type="math/tex; mode=display">
\nabla\mathrm{MSPBE}(\mathbf{w})=2\mathbb{E}[\rho_t(\gamma\mathbf{x}_{t+1}-\mathbf{x}_t)\mathbf{x}_t^T]\mathbb{E}[\mathbf{x}_t\mathbf{x}_t^T]^{-1}\mathbb{E}[\rho_t\delta_t\mathbf{x}_t]
</script> </div> <p>此时梯度为三个期望的乘积，且一、三项不独立，都依赖下一时刻的 <span><span class=MathJax_Preview>\mathbf{x}_{t+1}</span><script type=math/tex>\mathbf{x}_{t+1}</script></span>（第三项中是含在 <span><span class=MathJax_Preview>\delta_t</span><script type=math/tex>\delta_t</script></span> 内），所以不能对每个值采样然后直接相乘求期望，而应考虑分别分别对期望求估计后再组合得到梯度的无偏估计，但计算资源消耗较大，一个改进措施是，只估计某两项，然后对剩下一项做采样。</p> <p>先估计并存储后两项，得到向量</p> <div> <div class=MathJax_Preview> \mathbf{v}\approx\mathbb{E}[\mathbf{x}_t\mathbf{x}_t^T]^{-1}\mathbb{E}[\rho_t\delta_t\mathbf{x}_t] </div> <script type="math/tex; mode=display">
\mathbf{v}\approx\mathbb{E}[\mathbf{x}_t\mathbf{x}_t^T]^{-1}\mathbb{E}[\rho_t\delta_t\mathbf{x}_t]
</script> </div> <p>观察发现上式和一般『<strong>最小平方问题</strong>』解的形式（<span><span class=MathJax_Preview>\mathbf{w}=(\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\mathbf{\Phi}^T\mathbf{t}</span><script type=math/tex>\mathbf{w}=(\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\mathbf{\Phi}^T\mathbf{t}</script></span>）相似，此问题可看作是对 <span><span class=MathJax_Preview>\rho_t\delta_t</span><script type=math/tex>\rho_t\delta_t</script></span> 求最小平方估计，为了优化上面约等式误差，以 <span><span class=MathJax_Preview>(\mathbf{v}^T\mathbf{x}_t-\rho_t\delta_t)^2</span><script type=math/tex>(\mathbf{v}^T\mathbf{x}_t-\rho_t\delta_t)^2</script></span> 为目标函数，可得 SGD 更新式</p> <div> <div class=MathJax_Preview> \mathbf{v}_{t+1}=\mathbf{v}_t+\beta(\rho_t\delta_t-\mathbf{v}_t^T\mathbf{x}_t)\mathbf{x}_t </div> <script type="math/tex; mode=display">
\mathbf{v}_{t+1}=\mathbf{v}_t+\beta(\rho_t\delta_t-\mathbf{v}_t^T\mathbf{x}_t)\mathbf{x}_t
</script> </div> <p>使用这个方法可以在每一步得到更新的 <span><span class=MathJax_Preview>\mathbf{v}</span><script type=math/tex>\mathbf{v}</script></span> ，在存储了 <span><span class=MathJax_Preview>\mathbf{v}</span><script type=math/tex>\mathbf{v}</script></span> 的情况下，参数 <span><span class=MathJax_Preview>\mathbf{w}</span><script type=math/tex>\mathbf{w}</script></span> 的更新式为</p> <div> <div class=MathJax_Preview> \begin{aligned} \mathbf{w}_{t+1}&amp;=\mathbf{w}_t-\frac{1}{2}\alpha\nabla\mathrm{MSPBE}(\mathbf{w}_t)\\ &amp;=\mathbf{w}_t-\frac{1}{2}\alpha 2\mathbb{E}[\rho_t(\gamma\mathbf{x}_{t+1}-\mathbf{x}_t)\mathbf{x}_t^T]\mathbb{E}[\mathbf{x}_t\mathbf{x}_t^T]^{-1}\mathbb{E}[\rho_t\delta_t\mathbf{x}_t]\\ &amp;=\mathbf{w}_t+\alpha\mathbb{E}[\rho_t(\mathbf{x}_t-\gamma\mathbf{x}_{t+1})\mathbf{x}_t^T]\mathbb{E}[\mathbf{x}_t\mathbf{x}_t^T]^{-1}\mathbb{E}[\rho_t\delta_t\mathbf{x}_t]\\ &amp;\approx\mathbf{w}_t+\alpha\mathbb{E}[\rho_t(\mathbf{x}_t-\gamma\mathbf{x}_{t+1})\mathbf{x}_t^T]\mathbf{v}_t\\ &amp;\approx\mathbf{w}_t+\alpha\rho_t(\mathbf{x}_t-\gamma\mathbf{x}_{t+1})\mathbf{x}_t^T\mathbf{v}_t\qquad(\mathrm{sampling}) \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\mathbf{w}_{t+1}&=\mathbf{w}_t-\frac{1}{2}\alpha\nabla\mathrm{MSPBE}(\mathbf{w}_t)\\
&=\mathbf{w}_t-\frac{1}{2}\alpha 2\mathbb{E}[\rho_t(\gamma\mathbf{x}_{t+1}-\mathbf{x}_t)\mathbf{x}_t^T]\mathbb{E}[\mathbf{x}_t\mathbf{x}_t^T]^{-1}\mathbb{E}[\rho_t\delta_t\mathbf{x}_t]\\
&=\mathbf{w}_t+\alpha\mathbb{E}[\rho_t(\mathbf{x}_t-\gamma\mathbf{x}_{t+1})\mathbf{x}_t^T]\mathbb{E}[\mathbf{x}_t\mathbf{x}_t^T]^{-1}\mathbb{E}[\rho_t\delta_t\mathbf{x}_t]\\
&\approx\mathbf{w}_t+\alpha\mathbb{E}[\rho_t(\mathbf{x}_t-\gamma\mathbf{x}_{t+1})\mathbf{x}_t^T]\mathbf{v}_t\\
&\approx\mathbf{w}_t+\alpha\rho_t(\mathbf{x}_t-\gamma\mathbf{x}_{t+1})\mathbf{x}_t^T\mathbf{v}_t\qquad(\mathrm{sampling})
\end{aligned}
</script> </div> <p>称此算法为『GTD2』，若先计算 <span><span class=MathJax_Preview>(\mathbf{x}_t^T\mathbf{v}_t)</span><script type=math/tex>(\mathbf{x}_t^T\mathbf{v}_t)</script></span> ，则算法复杂度为 <span><span class=MathJax_Preview>O(d)</span><script type=math/tex>O(d)</script></span> 。一个略微的改进是计算 <span><span class=MathJax_Preview>\mathbf{v}_t</span><script type=math/tex>\mathbf{v}_t</script></span> 前先做一点分析调整</p> <div> <div class=MathJax_Preview> \begin{aligned} \mathbf{w}_{t+1}&amp;=\mathbf{w}_t+\alpha\mathbb{E}[\rho_t(\mathbf{x}_t-\gamma\mathbf{x}_{t+1})\mathbf{x}_t^T]\mathbb{E}[\mathbf{x}_t\mathbf{x}_t^T]^{-1}\mathbb{E}[\rho_t\delta_t\mathbf{x}_t]\\ &amp;=\mathbf{w}_t+\alpha(\mathbb{E}[\rho_t\mathbf{x}_t\mathbf{x}_t^T]-\gamma\mathbb{E}[\rho_t\mathbf{x}_{t+1}\mathbf{x}_t^T])\mathbb{E}[\mathbf{x}_t\mathbf{x}_t^T]^{-1}\mathbb{E}[\rho_t\delta_t\mathbf{x}_t]\\ &amp;=\mathbf{w}_t+\alpha(\mathbb{E}[\rho_t\delta_t\mathbf{x}_t]-\gamma\mathbb{E}[\rho_t\mathbf{x}_{t+1}\mathbf{x}_t^T]\mathbb{E}[\mathbf{x}_t\mathbf{x}_t^T]^{-1}\mathbb{E}[\rho_t\delta_t\mathbf{x}_t])\\ &amp;\approx\mathbf{w}_t+\alpha(\mathbb{E}[\rho_t\delta_t\mathbf{x}_t]-\gamma\mathbb{E}[\rho_t\mathbf{x}_{t+1}\mathbf{x}_t^T]\mathbf{v}_t)\\ &amp;\approx\mathbf{w}_t+\alpha\rho_t(\delta_t\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\mathbf{x}_t^T\mathbf{v}_t) \qquad (\mathrm{sampling}) \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\mathbf{w}_{t+1}&=\mathbf{w}_t+\alpha\mathbb{E}[\rho_t(\mathbf{x}_t-\gamma\mathbf{x}_{t+1})\mathbf{x}_t^T]\mathbb{E}[\mathbf{x}_t\mathbf{x}_t^T]^{-1}\mathbb{E}[\rho_t\delta_t\mathbf{x}_t]\\
&=\mathbf{w}_t+\alpha(\mathbb{E}[\rho_t\mathbf{x}_t\mathbf{x}_t^T]-\gamma\mathbb{E}[\rho_t\mathbf{x}_{t+1}\mathbf{x}_t^T])\mathbb{E}[\mathbf{x}_t\mathbf{x}_t^T]^{-1}\mathbb{E}[\rho_t\delta_t\mathbf{x}_t]\\
&=\mathbf{w}_t+\alpha(\mathbb{E}[\rho_t\delta_t\mathbf{x}_t]-\gamma\mathbb{E}[\rho_t\mathbf{x}_{t+1}\mathbf{x}_t^T]\mathbb{E}[\mathbf{x}_t\mathbf{x}_t^T]^{-1}\mathbb{E}[\rho_t\delta_t\mathbf{x}_t])\\
&\approx\mathbf{w}_t+\alpha(\mathbb{E}[\rho_t\delta_t\mathbf{x}_t]-\gamma\mathbb{E}[\rho_t\mathbf{x}_{t+1}\mathbf{x}_t^T]\mathbf{v}_t)\\
&\approx\mathbf{w}_t+\alpha\rho_t(\delta_t\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\mathbf{x}_t^T\mathbf{v}_t) \qquad (\mathrm{sampling})
\end{aligned}
</script> </div> <p>称此算法为『TD(0) with gradient correction (TDC)』或者『GTD(0)』若先计算 <span><span class=MathJax_Preview>(\mathbf{x}_t^T\mathbf{v}_t)</span><script type=math/tex>(\mathbf{x}_t^T\mathbf{v}_t)</script></span> ，则算法复杂度为 <span><span class=MathJax_Preview>O(d)</span><script type=math/tex>O(d)</script></span> 。</p> <p>TDC 算法在 Baird 反例上的实际表现如下图所示</p> <p><img alt src=../imgs/RLAI_11/TDC.png></p> <p>GTD2 及 TDC 都包含了两个学习过程，主过程学习 <span><span class=MathJax_Preview>\mathbf{w}</span><script type=math/tex>\mathbf{w}</script></span> ，次过程学习 <span><span class=MathJax_Preview>\mathbf{v}</span><script type=math/tex>\mathbf{v}</script></span> 。次过程在每一步中需要先于主过程，这种依赖关系称之为『<strong>层叠</strong>（cascade）』，关于学习率，通常需要满足极限</p> <div> <div class=MathJax_Preview> \beta\to 0,\quad\frac{\alpha}{\beta}\to0 </div> <script type="math/tex; mode=display">
\beta\to 0,\quad\frac{\alpha}{\beta}\to0
</script> </div> <p>Gradient TD 方法是最简单易懂的稳定 off-policy 方法，并且有很多的衍生方法。</p> <h2 id=118-emphatic-td-methods><strong>11.8 Emphatic-TD Methods</strong><a class=headerlink href=#118-emphatic-td-methods title="Permanent link">&para;</a></h2> <p>这一节简单介绍了 Emphatic-TD 算法：</p> <div> <div class=MathJax_Preview> \begin{aligned} \delta_t&amp;=R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t)\\ \mathbf{w}_{t+1}&amp;=\mathbf{w}_t+\alpha M_t\rho_t\delta_t\nabla\hat{v}(S_t,\mathbf{w}_t)\\ M_t&amp;=\gamma\rho_{t-1}M_{t-1}+I_t \end{aligned} </div> <script type="math/tex; mode=display">
\begin{aligned}
\delta_t&=R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t)\\
\mathbf{w}_{t+1}&=\mathbf{w}_t+\alpha M_t\rho_t\delta_t\nabla\hat{v}(S_t,\mathbf{w}_t)\\
M_t&=\gamma\rho_{t-1}M_{t-1}+I_t
\end{aligned}
</script> </div> <p>其中 <span><span class=MathJax_Preview>I_t</span><script type=math/tex>I_t</script></span> 表示 interest，为随机值，<span><span class=MathJax_Preview>M_t</span><script type=math/tex>M_t</script></span> 表示 emphasis 。</p> <p>Emphatic-TD 算法在 Baird 反例上的实际表现如下图所示</p> <p><img alt src=../imgs/RLAI_11/emphasis-td.png></p> <p>该算法方差较大，导致其并不实用，所以如何减少算法的方差很值得研究。</p> <h2 id=119-reducing-variance><strong>11.9 Reducing Variance</strong><a class=headerlink href=#119-reducing-variance title="Permanent link">&para;</a></h2> <p>off-policy 算法直观上显然要比 on-policy 算法有着更大的方差，他从行为策略中获得的数据可能和目标策略关系不大，极端情况下甚至可能完全学不到东西，比如一个人不能通过做饭的经验知识来学习如何开车。</p> <p>只有当 behavior policy 与 target policy 相关性较大，即当两个策略经过的 states、actions 很接近，才能在 off-policy training 中取得较好的进展。</p> <p>关于这些<strong>相关但又不一致</strong>的行为策略，主要的问题是如何尽量利用上他们。目前而言这一块已有很多相关的工作，本节末尾列举了很多方法，不作具体介绍。</p> <hr> <div class=md-source-date> <small> Last update: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">June 29, 2020</span> </small> </div> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid" aria-label=Footer> <a href=../RLAI_10/ title="Chapter 10" class="md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Previous </span> Chapter 10 </div> </div> </a> <a href=../RLAI_12/ title="Chapter 12" class="md-footer-nav__link md-footer-nav__link--next" rel=next> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Next </span> Chapter 12 </div> </div> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2016-2020 ZHANGWP </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-footer-social> <a href=https://github.com/zawnpn target=_blank rel=noopener title=github.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://twitter.com/zawnpn target=_blank rel=noopener title=twitter.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> </a> <a href=https://psnprofiles.com/zawnpn target=_blank rel=noopener title=psnprofiles.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><path d="M570.9 372.3c-11.3 14.2-38.8 24.3-38.8 24.3L327 470.2v-54.3l150.9-53.8c17.1-6.1 19.8-14.8 5.8-19.4-13.9-4.6-39.1-3.3-56.2 2.9L327 381.1v-56.4c23.2-7.8 47.1-13.6 75.7-16.8 40.9-4.5 90.9.6 130.2 15.5 44.2 14 49.2 34.7 38 48.9zm-224.4-92.5v-139c0-16.3-3-31.3-18.3-35.6-11.7-3.8-19 7.1-19 23.4v347.9l-93.8-29.8V32c39.9 7.4 98 24.9 129.2 35.4C424.1 94.7 451 128.7 451 205.2c0 74.5-46 102.8-104.5 74.6zM43.2 410.2c-45.4-12.8-53-39.5-32.3-54.8 19.1-14.2 51.7-24.9 51.7-24.9l134.5-47.8v54.5l-96.8 34.6c-17.1 6.1-19.7 14.8-5.8 19.4 13.9 4.6 39.1 3.3 56.2-2.9l46.4-16.9v48.8c-51.6 9.3-101.4 7.3-153.9-10z"/></svg> </a> <a href=https://steamcommunity.com/id/zawnpn/ target=_blank rel=noopener title=steamcommunity.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M496 256c0 137-111.2 248-248.4 248-113.8 0-209.6-76.3-239-180.4l95.2 39.3c6.4 32.1 34.9 56.4 68.9 56.4 39.2 0 71.9-32.4 70.2-73.5l84.5-60.2c52.1 1.3 95.8-40.9 95.8-93.5 0-51.6-42-93.5-93.7-93.5s-93.7 42-93.7 93.5v1.2L176.6 279c-15.5-.9-30.7 3.4-43.5 12.1L0 236.1C10.2 108.4 117.1 8 247.6 8 384.8 8 496 119 496 256zM155.7 384.3l-30.5-12.6a52.79 52.79 0 0027.2 25.8c26.9 11.2 57.8-1.6 69-28.4 5.4-13 5.5-27.3.1-40.3-5.4-13-15.5-23.2-28.5-28.6-12.9-5.4-26.7-5.2-38.9-.6l31.5 13c19.8 8.2 29.2 30.9 20.9 50.7-8.3 19.9-31 29.2-50.8 21zm173.8-129.9c-34.4 0-62.4-28-62.4-62.3s28-62.3 62.4-62.3 62.4 28 62.4 62.3-27.9 62.3-62.4 62.3zm.1-15.6c25.9 0 46.9-21 46.9-46.8 0-25.9-21-46.8-46.9-46.8s-46.9 21-46.9 46.8c.1 25.8 21.1 46.8 46.9 46.8z"/></svg> </a> <a href=https://www.zhihu.com/people/zhangwanpeng target=_blank rel=noopener title=www.zhihu.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13H170.54zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82v170.31zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62v.01zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2l19.23 14.43zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78l.03-.01z"/></svg> </a> </div> </div> </div> </footer> </div> <script src=../../../../assets/javascripts/vendor.d710d30a.min.js></script> <script src=../../../../assets/javascripts/bundle.b39636ac.min.js></script><script id=__lang type=application/json>{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script> <script>
        app = initialize({
          base: "../../../..",
          features: ["tabs"],
          search: Object.assign({
            worker: "../../../../assets/javascripts/worker/search.a68abb33.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script> <script src="//cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-MML-AM_SVG"></script> </body> </html>